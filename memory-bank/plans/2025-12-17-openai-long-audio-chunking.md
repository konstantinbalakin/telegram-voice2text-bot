# –ü–ª–∞–Ω: –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª–∏–Ω–Ω—ã—Ö –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤ —á–µ—Ä–µ–∑ OpenAI —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–º chunking

**–î–∞—Ç–∞:** 2025-12-17
**–°—Ç–∞—Ç—É—Å:** –£—Ç–≤–µ—Ä–∂–¥—ë–Ω, –æ–∂–∏–¥–∞–µ—Ç —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏
**–ê–≤—Ç–æ—Ä:** Claude Code
**–°–≤—è–∑–∞–Ω–Ω—ã–µ issue:** –û—à–∏–±–∫–∞ "audio duration 1885.851812 seconds is longer than 1400 seconds"

---

## –ü—Ä–æ–±–ª–µ–º–∞

–ü—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤ –Ω–∞ –º–æ–¥–µ–ª—è—Ö —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏ OpenAI –æ–±–Ω–∞—Ä—É–∂–∏–ª—Å—è –ª–∏–º–∏—Ç –Ω–∞ **1400 —Å–µ–∫—É–Ω–¥ (~23 –º–∏–Ω—É—Ç—ã)** –¥–ª—è –º–æ–¥–µ–ª–∏ `gpt-4o-transcribe`.

**–ü—Ä–∏–º–µ—Ä –æ—à–∏–±–∫–∏:**
```
2025-12-17 15:59:41,233 - src.transcription.providers.openai_provider - ERROR - OpenAI API client error (400): Client error '400 Bad Request' for url 'https://api.openai.com/v1/audio/transcriptions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400 | Response: {
  "error": {
    "message": "audio duration 1885.851812 seconds is longer than 1400 seconds which is the maximum for this model",
    "type": "invalid_request_error",
    "param": null,
    "code": "invalid_value"
  }
}
```

**–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è OpenAI –º–æ–¥–µ–ª–µ–π:**
- `gpt-4o-transcribe`: max 1400-1500 —Å–µ–∫—É–Ω–¥ + 25MB file size
- `gpt-4o-mini-transcribe`: max 1400-1500 —Å–µ–∫—É–Ω–¥ + 25MB file size
- `whisper-1`: —Ç–æ–ª—å–∫–æ 25MB file size (–±–µ–∑ duration limit)

---

## –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —Å –≥–∏–±–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π:

1. **OPENAI_GPT4O_MAX_DURATION** - –ø–æ—Ä–æ–≥ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1400 —Å–µ–∫)
2. **OPENAI_CHANGE_MODEL** - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–∫–ª—é—á–∞—Ç—å –Ω–∞ whisper-1 –ø—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏
3. **OPENAI_CHUNKING** - —Ä–∞–∑–±–∏–≤–∞—Ç—å —Ñ–∞–π–ª –Ω–∞ —á–∞–Ω–∫–∏ –ø—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏
4. **OPENAI_PARALLEL_CHUNKS** - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —á–∞–Ω–∫–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è

**–õ–æ–≥–∏–∫–∞:**
```
IF audio_duration > OPENAI_GPT4O_MAX_DURATION:
    IF OPENAI_CHUNKING:
        target_model = "whisper-1" IF OPENAI_CHANGE_MODEL ELSE original_model
        –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞—Ç—å —á–∞–Ω–∫–∞–º–∏ —á–µ—Ä–µ–∑ target_model

        IF OPENAI_PARALLEL_CHUNKS:
            –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (–±—ã—Å—Ç—Ä–æ, –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)
        ELSE:
            –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (–º–µ–¥–ª–µ–Ω–Ω–æ, —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º)
    ELSE:
        IF OPENAI_CHANGE_MODEL:
            –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞—Ç—å –≤–µ—Å—å —Ñ–∞–π–ª —á–µ—Ä–µ–∑ whisper-1
        ELSE:
            Raise error: —Ñ–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π
```

---

## –í—ã–±—Ä–∞–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ: Option 2+ (pydub Chunking + Parallel Processing)

### –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ

- ‚úÖ –ü–æ–ª–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è —Ñ–∞–π–ª–æ–≤ –ª—é–±–æ–π –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
- ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ gpt-4o-transcribe (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
- ‚ö° –£—Å–∫–æ—Ä–µ–Ω–∏–µ –≤ 2-3 —Ä–∞–∑–∞ –±–ª–∞–≥–æ–¥–∞—Ä—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç–∏
- üéõÔ∏è –ì–∏–±–∫–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ–¥ —Ä–∞–∑–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏
- üîß –ö–æ–¥ —É–∂–µ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π (httpx.AsyncClient)

### –ù–æ–≤—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏

- **pydub** - –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è –∞—É–¥–∏–æ –Ω–∞ —á–∞–Ω–∫–∏

---

## –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –¥–∏–∑–∞–π–Ω

### 1. –ù–æ–≤—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã

**–§–∞–π–ª:** `src/config.py`

```python
# OpenAI Long Audio Handling
openai_gpt4o_max_duration: int = Field(
    default=1400,
    description="Maximum audio duration in seconds for gpt-4o models before chunking/switching"
)

openai_change_model: bool = Field(
    default=True,
    description="Automatically switch to whisper-1 for audio exceeding max duration"
)

openai_chunking: bool = Field(
    default=False,
    description="Enable audio chunking for long files (splits into segments)"
)

openai_chunk_size_seconds: int = Field(
    default=1200,
    ge=300,
    le=1400,
    description="Size of each audio chunk in seconds (default: 20 minutes)"
)

openai_chunk_overlap_seconds: int = Field(
    default=2,
    ge=0,
    le=10,
    description="Overlap between chunks for better context preservation"
)

openai_parallel_chunks: bool = Field(
    default=True,
    description="Process chunks in parallel for faster transcription (disables context passing)"
)

openai_max_parallel_chunks: int = Field(
    default=3,
    ge=1,
    le=10,
    description="Maximum number of chunks to process simultaneously (rate limiting)"
)
```

### 2. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ OpenAIProvider

**–§–∞–π–ª:** `src/transcription/providers/openai_provider.py`

#### –ù–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã

```python
class OpenAIProvider(TranscriptionProvider):

    async def transcribe(
        self, audio_path: Path, context: TranscriptionContext
    ) -> TranscriptionResult:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π chunking.

        –õ–æ–≥–∏–∫–∞:
        1. –ü—Ä–æ–≤–µ—Ä–∫–∞ file size (—Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è)
        2. –ü—Ä–æ–≤–µ—Ä–∫–∞ duration
        3. –ï—Å–ª–∏ duration > max_duration:
           - Chunking –≤–∫–ª—é—á–µ–Ω? ‚Üí –†–∞–∑–±–∏—Ç—å –∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞—Ç—å
           - Chunking –≤—ã–∫–ª—é—á–µ–Ω + change_model? ‚Üí –ü–µ—Ä–µ–∫–ª—é—á–∏—Ç—å –Ω–∞ whisper-1
           - –ò–Ω–∞—á–µ ‚Üí Raise error
        """
        # Existing validation...

        # NEW: Check duration limit
        if context.duration_seconds > settings.openai_gpt4o_max_duration:
            return await self._handle_long_audio(audio_path, context)

        # Existing transcription logic...

    async def _handle_long_audio(
        self, audio_path: Path, context: TranscriptionContext
    ) -> TranscriptionResult:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª–∏–Ω–Ω—ã—Ö –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤.

        Returns:
            TranscriptionResult
        """
        duration = context.duration_seconds
        max_duration = settings.openai_gpt4o_max_duration

        logger.info(
            f"Audio duration {duration}s exceeds limit {max_duration}s. "
            f"Chunking={settings.openai_chunking}, "
            f"ChangeModel={settings.openai_change_model}"
        )

        if settings.openai_chunking:
            # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è —á–∞–Ω–∫–æ–≤
            target_model = "whisper-1" if settings.openai_change_model else self.model

            logger.info(f"Splitting audio into chunks and transcribing with {target_model}")

            # –†–∞–∑–±–∏—Ç—å –Ω–∞ —á–∞–Ω–∫–∏
            chunk_paths = await self._split_audio_into_chunks(audio_path, context)

            try:
                # –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞—Ç—å —á–∞–Ω–∫–∏
                if settings.openai_parallel_chunks:
                    text = await self._transcribe_chunks_parallel(
                        chunk_paths, context, target_model
                    )
                else:
                    text = await self._transcribe_chunks_sequential(
                        chunk_paths, context, target_model
                    )

                processing_time = time.time() - start_time

                return TranscriptionResult(
                    text=text,
                    language=context.language or "unknown",
                    processing_time=processing_time,
                    audio_duration=context.duration_seconds,
                    provider_used="openai",
                    model_name=f"{target_model} (chunked)",
                )

            finally:
                # Cleanup chunk files
                self._cleanup_chunks(chunk_paths)

        elif settings.openai_change_model:
            # –ü–µ—Ä–µ–∫–ª—é—á–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ whisper-1 –¥–ª—è –≤—Å–µ–≥–æ —Ñ–∞–π–ª–∞
            logger.info(f"Switching model from {self.model} to whisper-1")

            original_model = self.model
            self.model = "whisper-1"

            try:
                result = await self._transcribe_single(audio_path, context)
                result.model_name = f"whisper-1 (switched from {original_model})"
                return result
            finally:
                self.model = original_model

        else:
            raise ValueError(
                f"Audio duration {duration}s exceeds maximum {max_duration}s for {self.model}. "
                f"Enable OPENAI_CHUNKING or OPENAI_CHANGE_MODEL to handle long files."
            )

    async def _split_audio_into_chunks(
        self, audio_path: Path, context: TranscriptionContext
    ) -> list[Path]:
        """
        –†–∞–∑–±–∏—Ç—å –∞—É–¥–∏–æ—Ñ–∞–π–ª –Ω–∞ —á–∞–Ω–∫–∏ –∏—Å–ø–æ–ª—å–∑—É—è pydub.

        Args:
            audio_path: –ü—É—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏

        Returns:
            –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —Ñ–∞–π–ª–∞–º-—á–∞–Ω–∫–∞–º

        Raises:
            RuntimeError: –ï—Å–ª–∏ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å
        """
        from pydub import AudioSegment
        import uuid

        chunk_size_ms = settings.openai_chunk_size_seconds * 1000
        overlap_ms = settings.openai_chunk_overlap_seconds * 1000

        logger.info(
            f"Splitting {audio_path.name} into chunks: "
            f"size={settings.openai_chunk_size_seconds}s, "
            f"overlap={settings.openai_chunk_overlap_seconds}s"
        )

        try:
            # –ó–∞–≥—Ä—É–∑–∏—Ç—å –∞—É–¥–∏–æ
            audio = AudioSegment.from_file(str(audio_path))

            total_duration_ms = len(audio)
            chunk_paths = []

            # –°–æ–∑–¥–∞—Ç—å —á–∞–Ω–∫–∏ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º
            start_ms = 0
            chunk_index = 0

            while start_ms < total_duration_ms:
                end_ms = min(start_ms + chunk_size_ms, total_duration_ms)

                # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —á–∞–Ω–∫
                chunk_audio = audio[start_ms:end_ms]

                # –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
                chunk_filename = f"{audio_path.stem}_chunk_{chunk_index}_{uuid.uuid4().hex[:8]}.mp3"
                chunk_path = audio_path.parent / chunk_filename

                chunk_audio.export(str(chunk_path), format="mp3")
                chunk_paths.append(chunk_path)

                logger.debug(
                    f"Created chunk {chunk_index}: {chunk_path.name}, "
                    f"duration={len(chunk_audio)/1000:.1f}s"
                )

                # –°–ª–µ–¥—É—é—â–∏–π —á–∞–Ω–∫ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å —É—á—ë—Ç–æ–º overlap
                start_ms = end_ms - overlap_ms
                chunk_index += 1

            logger.info(f"Split audio into {len(chunk_paths)} chunks")
            return chunk_paths

        except Exception as e:
            logger.error(f"Failed to split audio into chunks: {e}")
            raise RuntimeError(f"Audio splitting failed: {e}") from e

    async def _transcribe_chunks_parallel(
        self,
        chunk_paths: list[Path],
        context: TranscriptionContext,
        model: str
    ) -> str:
        """
        –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞—Ç—å —á–∞–Ω–∫–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ (–±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏).

        –ë—ã—Å—Ç—Ä–µ–µ, –Ω–æ —Ç–µ—Ä—è–µ—Ç—Å—è –∫–æ–Ω—Ç–µ–∫—Å—Ç –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏.

        Args:
            chunk_paths: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —á–∞–Ω–∫–∞–º
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
            model: –ú–æ–¥–µ–ª—å –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

        Returns:
            –°–∫–ª–µ–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤
        """
        import asyncio

        logger.info(
            f"Starting parallel transcription of {len(chunk_paths)} chunks "
            f"with {model}, max_parallel={settings.openai_max_parallel_chunks}"
        )

        # Semaphore –¥–ª—è rate limiting
        semaphore = asyncio.Semaphore(settings.openai_max_parallel_chunks)

        async def transcribe_one_chunk(chunk_path: Path, chunk_index: int) -> tuple[int, str]:
            """–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞—Ç—å –æ–¥–∏–Ω —á–∞–Ω–∫."""
            async with semaphore:
                try:
                    logger.info(f"Transcribing chunk {chunk_index + 1}/{len(chunk_paths)}")

                    # –°–æ–∑–¥–∞—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —á–∞–Ω–∫–∞
                    chunk_context = TranscriptionContext(
                        user_id=context.user_id,
                        language=context.language,
                        priority=context.priority,
                    )

                    # –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞—Ç—å —á–∞–Ω–∫
                    result = await self._transcribe_single_file(
                        chunk_path, chunk_context, model
                    )

                    logger.info(
                        f"Chunk {chunk_index + 1} complete: {len(result.text)} chars"
                    )

                    return (chunk_index, result.text)

                except Exception as e:
                    logger.error(f"Chunk {chunk_index + 1} failed: {e}")
                    # Retry –ª–æ–≥–∏–∫–∞
                    try:
                        logger.warning(f"Retrying chunk {chunk_index + 1}")
                        result = await self._transcribe_single_file(
                            chunk_path, chunk_context, model
                        )
                        return (chunk_index, result.text)
                    except Exception as retry_error:
                        logger.error(f"Chunk {chunk_index + 1} retry failed: {retry_error}")
                        return (chunk_index, f"[ERROR: Chunk {chunk_index + 1} failed]")

        # –ó–∞–ø—É—Å—Ç–∏—Ç—å –≤—Å–µ —á–∞–Ω–∫–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        tasks = [
            transcribe_one_chunk(chunk_path, i)
            for i, chunk_path in enumerate(chunk_paths)
        ]

        results = await asyncio.gather(*tasks)

        # –û—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ –∏–Ω–¥–µ–∫—Å—É –∏ —Å–∫–ª–µ–∏—Ç—å
        results_sorted = sorted(results, key=lambda x: x[0])
        texts = [text for _, text in results_sorted if not text.startswith("[ERROR")]

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—à–∏–±–∫–∏
        errors = [text for _, text in results_sorted if text.startswith("[ERROR")]
        if errors:
            logger.warning(f"{len(errors)} chunks failed during transcription")

        final_text = " ".join(texts)
        logger.info(f"Parallel transcription complete: {len(final_text)} chars total")

        return final_text

    async def _transcribe_chunks_sequential(
        self,
        chunk_paths: list[Path],
        context: TranscriptionContext,
        model: str
    ) -> str:
        """
        –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞—Ç—å —á–∞–Ω–∫–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ (—Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏).

        –ú–µ–¥–ª–µ–Ω–Ω–µ–µ, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ prompt parameter.

        Args:
            chunk_paths: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —á–∞–Ω–∫–∞–º
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
            model: –ú–æ–¥–µ–ª—å –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

        Returns:
            –°–∫–ª–µ–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤
        """
        logger.info(
            f"Starting sequential transcription of {len(chunk_paths)} chunks with {model}"
        )

        transcriptions = []
        previous_text = ""

        for i, chunk_path in enumerate(chunk_paths):
            try:
                logger.info(f"Transcribing chunk {i + 1}/{len(chunk_paths)}")

                # –°–æ–∑–¥–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å –ø—Ä–æ–º–ø—Ç–æ–º –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —á–∞–Ω–∫–∞
                chunk_context = TranscriptionContext(
                    user_id=context.user_id,
                    language=context.language,
                    priority=context.priority,
                )

                # –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞—Ç—å —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 224 —Ç–æ–∫–µ–Ω–∞)
                prompt = previous_text[-224:] if previous_text else None

                result = await self._transcribe_single_file(
                    chunk_path, chunk_context, model, prompt=prompt
                )

                transcriptions.append(result.text)
                previous_text = result.text

                logger.info(
                    f"Chunk {i + 1} complete: {len(result.text)} chars"
                )

            except Exception as e:
                logger.error(f"Chunk {i + 1} failed: {e}")

                # Retry –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                try:
                    logger.warning(f"Retrying chunk {i + 1} without context")
                    result = await self._transcribe_single_file(
                        chunk_path, chunk_context, model
                    )
                    transcriptions.append(result.text)
                except Exception as retry_error:
                    logger.error(f"Chunk {i + 1} retry failed: {retry_error}")
                    transcriptions.append(f"[ERROR: Chunk {i + 1} failed]")

        final_text = " ".join(transcriptions)
        logger.info(f"Sequential transcription complete: {len(final_text)} chars total")

        return final_text

    async def _transcribe_single_file(
        self,
        audio_path: Path,
        context: TranscriptionContext,
        model: str,
        prompt: Optional[str] = None
    ) -> TranscriptionResult:
        """
        –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞—Ç—å –æ–¥–∏–Ω —Ñ–∞–π–ª (helper –¥–ª—è chunking).

        Args:
            audio_path: –ü—É—Ç—å –∫ –∞—É–¥–∏–æ—Ñ–∞–π–ª—É
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç
            model: –ú–æ–¥–µ–ª—å –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
            prompt: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

        Returns:
            TranscriptionResult
        """
        start_time = time.time()

        try:
            mime_type = mimetypes.guess_type(audio_path)[0] or "audio/mpeg"

            with open(audio_path, "rb") as audio_file:
                files = {"file": (audio_path.name, audio_file, mime_type)}
                data = {"model": model}

                if context.language:
                    data["language"] = context.language

                if prompt:
                    data["prompt"] = prompt

                response = await self._client.post(
                    "/audio/transcriptions",
                    files=files,
                    data=data,
                )

            response.raise_for_status()
            result = response.json()

            processing_time = time.time() - start_time
            text = result.get("text", "")
            language = result.get("language", context.language or "unknown")

            return TranscriptionResult(
                text=text,
                language=language,
                processing_time=processing_time,
                audio_duration=0,  # Unknown for chunks
                provider_used="openai",
                model_name=model,
            )

        except Exception as e:
            logger.error(f"Transcription failed for {audio_path.name}: {e}")
            raise

    def _cleanup_chunks(self, chunk_paths: list[Path]) -> None:
        """
        –£–¥–∞–ª–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã —á–∞–Ω–∫–æ–≤.

        Args:
            chunk_paths: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —á–∞–Ω–∫–∞–º
        """
        for chunk_path in chunk_paths:
            try:
                if chunk_path.exists():
                    chunk_path.unlink()
                    logger.debug(f"Cleaned up chunk: {chunk_path.name}")
            except Exception as e:
                logger.warning(f"Failed to cleanup chunk {chunk_path.name}: {e}")
```

---

## –ò–∑–º–µ–Ω–µ–Ω–∏—è –≤ —Ñ–∞–π–ª–∞—Ö

### 1. `pyproject.toml`

```toml
[tool.poetry.dependencies]
# ... existing dependencies ...
pydub = "^0.25.1"
```

### 2. `requirements.txt`

–ü–æ—Å–ª–µ `poetry lock && poetry export`:
```
pydub==0.25.1
```

### 3. `.env.example`

```bash
# ============================================================================
# OpenAI Long Audio Handling
# ============================================================================

# Maximum audio duration for gpt-4o models before special handling
# gpt-4o-transcribe/mini have a limit of ~1400-1500 seconds
OPENAI_GPT4O_MAX_DURATION=1400

# Automatically switch to whisper-1 for audio exceeding max duration
# whisper-1 has no duration limit (only 25MB file size limit)
OPENAI_CHANGE_MODEL=true

# Enable audio chunking for long files
# Splits audio into segments and transcribes separately
OPENAI_CHUNKING=false

# Size of each audio chunk in seconds (5-23 minutes recommended)
# Must be less than OPENAI_GPT4O_MAX_DURATION
OPENAI_CHUNK_SIZE_SECONDS=1200

# Overlap between chunks in seconds for better context
# Helps preserve context at chunk boundaries
OPENAI_CHUNK_OVERLAP_SECONDS=2

# Process chunks in parallel for faster transcription
# true = faster but loses context between chunks
# false = slower but preserves context via prompt parameter
OPENAI_PARALLEL_CHUNKS=true

# Maximum number of chunks to process simultaneously
# Helps prevent rate limiting (429 errors)
OPENAI_MAX_PARALLEL_CHUNKS=3
```

### 4. `.env.example.short`

```bash
# OpenAI Long Audio
OPENAI_GPT4O_MAX_DURATION=1400
OPENAI_CHANGE_MODEL=true
OPENAI_CHUNKING=false
OPENAI_CHUNK_SIZE_SECONDS=1200
OPENAI_CHUNK_OVERLAP_SECONDS=2
OPENAI_PARALLEL_CHUNKS=true
OPENAI_MAX_PARALLEL_CHUNKS=3
```

### 5. `.github/workflows/deploy.yml`

–î–æ–±–∞–≤–∏—Ç—å –≤ —Å–µ–∫—Ü–∏—é environment variables:
```yaml
OPENAI_GPT4O_MAX_DURATION=1400
OPENAI_CHANGE_MODEL=true
OPENAI_CHUNKING=false
OPENAI_PARALLEL_CHUNKS=true
OPENAI_MAX_PARALLEL_CHUNKS=3
```

---

## –ü–ª–∞–Ω —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è

### Unit Tests

**–§–∞–π–ª:** `tests/test_openai_provider_chunking.py`

```python
import pytest
from pathlib import Path
from unittest.mock import Mock, AsyncMock, patch
from src.transcription.providers.openai_provider import OpenAIProvider
from src.transcription.models import TranscriptionContext

@pytest.fixture
def provider():
    """OpenAI provider with mocked client."""
    provider = OpenAIProvider(api_key="test-key", model="gpt-4o-transcribe")
    provider.initialize()
    return provider

@pytest.fixture
def long_audio_context():
    """Context for long audio (>1400s)."""
    return TranscriptionContext(
        user_id=123,
        duration_seconds=1800,  # 30 minutes
        language="ru"
    )

class TestLongAudioHandling:
    """Tests for long audio handling."""

    @pytest.mark.asyncio
    async def test_duration_check_triggers_chunking(self, provider, long_audio_context):
        """Test that duration > max triggers chunking logic."""
        with patch.object(provider, '_handle_long_audio') as mock_handle:
            mock_handle.return_value = Mock()
            await provider.transcribe(Path("test.mp3"), long_audio_context)
            mock_handle.assert_called_once()

    @pytest.mark.asyncio
    async def test_model_switching_when_chunking_disabled(self, provider, long_audio_context):
        """Test model switches to whisper-1 when chunking disabled."""
        with patch('src.config.settings') as mock_settings:
            mock_settings.openai_chunking = False
            mock_settings.openai_change_model = True
            mock_settings.openai_gpt4o_max_duration = 1400

            # Test implementation...

    @pytest.mark.asyncio
    async def test_split_audio_into_chunks(self, provider):
        """Test audio splitting creates correct number of chunks."""
        # Test with mock audio file
        pass

    @pytest.mark.asyncio
    async def test_parallel_chunk_processing(self, provider):
        """Test parallel processing of chunks."""
        # Mock multiple chunks and verify parallel execution
        pass

    @pytest.mark.asyncio
    async def test_sequential_chunk_processing_with_context(self, provider):
        """Test sequential processing passes context via prompt."""
        # Verify prompt parameter is used
        pass

    @pytest.mark.asyncio
    async def test_chunk_retry_on_failure(self, provider):
        """Test that failed chunks are retried."""
        pass

    @pytest.mark.asyncio
    async def test_cleanup_chunks_after_transcription(self, provider):
        """Test that temporary chunk files are cleaned up."""
        pass

    @pytest.mark.asyncio
    async def test_error_when_chunking_and_switching_disabled(self, provider, long_audio_context):
        """Test error raised when both chunking and model switching disabled."""
        with patch('src.config.settings') as mock_settings:
            mock_settings.openai_chunking = False
            mock_settings.openai_change_model = False

            with pytest.raises(ValueError, match="exceeds maximum"):
                await provider.transcribe(Path("test.mp3"), long_audio_context)
```

### Integration Tests

**–¢–µ—Å—Ç–æ–≤—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏:**

1. ‚úÖ **Short audio (< 1400s)** - –æ–±—ã—á–Ω–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
2. ‚úÖ **Long audio + chunking=false + change_model=true** - –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ whisper-1
3. ‚úÖ **Long audio + chunking=true + parallel=true** - –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–æ–≤
4. ‚úÖ **Long audio + chunking=true + parallel=false** - –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
5. ‚úÖ **Long audio + chunking=false + change_model=false** - –æ—à–∏–±–∫–∞
6. ‚ö†Ô∏è **Chunk failure + retry** - –æ–¥–∏–Ω —á–∞–Ω–∫ –ø–∞–¥–∞–µ—Ç, retry —É—Å–ø–µ—à–µ–Ω
7. ‚ö†Ô∏è **Multiple chunk failures** - –Ω–µ—Å–∫–æ–ª—å–∫–æ —á–∞–Ω–∫–æ–≤ –ø–∞–¥–∞—é—Ç
8. ‚úÖ **Cleanup verification** - –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã —É–¥–∞–ª–µ–Ω—ã

### Manual Testing

**–¢–µ—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã:**
- 10 –º–∏–Ω—É—Ç (600s) - –∫–æ—Ä–æ—Ç–∫–∏–π —Ñ–∞–π–ª
- 25 –º–∏–Ω—É—Ç (1500s) - –¥–ª–∏–Ω–Ω—ã–π —Ñ–∞–π–ª
- 45 –º–∏–Ω—É—Ç (2700s) - –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–π —Ñ–∞–π–ª

**–ü—Ä–æ–≤–µ—Ä–∏—Ç—å:**
- –ö–∞—á–µ—Å—Ç–≤–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –Ω–∞ –≥—Ä–∞–Ω–∏—Ü–∞—Ö —á–∞–Ω–∫–æ–≤
- –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (parallel vs sequential)
- –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ memory leaks
- –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å cleanup

---

## –†–∏—Å–∫–∏ –∏ –º–∏—Ç–∏–≥–∞—Ü–∏—è

| –†–∏—Å–∫ | –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å | –í–æ–∑–¥–µ–π—Å—Ç–≤–∏–µ | –ú–∏—Ç–∏–≥–∞—Ü–∏—è |
|------|-------------|-------------|-----------|
| **Rate limiting (429 errors)** | –°—Ä–µ–¥–Ω—è—è | –í—ã—Å–æ–∫–æ–µ | Semaphore —Å `max_parallel_chunks=3`, exponential backoff retry |
| **–ü–æ—Ç–µ—Ä—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø—Ä–∏ parallel** | –í—ã—Å–æ–∫–∞—è | –°—Ä–µ–¥–Ω–µ–µ | –î–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å, –¥–∞—Ç—å –≤—ã–±–æ—Ä `parallel_chunks=false` |
| **–ö–∞—á–µ—Å—Ç–≤–æ –Ω–∞ –≥—Ä–∞–Ω–∏—Ü–∞—Ö —á–∞–Ω–∫–æ–≤** | –°—Ä–µ–¥–Ω—è—è | –°—Ä–µ–¥–Ω–µ–µ | Overlap 2 —Å–µ–∫—É–Ω–¥—ã –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏ |
| **–£–≤–µ–ª–∏—á–µ–Ω–∏–µ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ API** | –°—Ä–µ–¥–Ω—è—è | –°—Ä–µ–¥–Ω–µ–µ | Chunking –≤—ã–∫–ª—é—á–µ–Ω –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é, —á—ë—Ç–∫–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è |
| **–û—à–∏–±–∫–∏ –ø—Ä–∏ splitting** | –ù–∏–∑–∫–∞—è | –í—ã—Å–æ–∫–æ–µ | Try/except, fallback –Ω–∞ model switching |
| **Memory issues —Å –±–æ–ª—å—à–∏–º–∏ —Ñ–∞–π–ª–∞–º–∏** | –ù–∏–∑–∫–∞—è | –°—Ä–µ–¥–Ω–µ–µ | pydub –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å—Ç—Ä–∏–º–æ–º, cleanup –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ |
| **Timestamps –Ω–µ —Ä–∞–±–æ—Ç–∞—é—Ç** | –í—ã—Å–æ–∫–∞—è | –ù–∏–∑–∫–æ–µ | OpenAI Whisper API –Ω–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç timestamps, –¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ |

---

## –ö—Ä–∏—Ç–µ—Ä–∏–∏ —É—Å–ø–µ—Ö–∞

### –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ

- ‚úÖ –§–∞–π–ª—ã > 1400 —Å–µ–∫ —É—Å–ø–µ—à–Ω–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É—é—Ç—Å—è
- ‚úÖ –í—Å–µ 4 –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –Ω–∞—Å—Ç—Ä–æ–µ–∫ —Ä–∞–±–æ—Ç–∞—é—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
- ‚úÖ –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —É—Å–∫–æ—Ä—è–µ—Ç –≤ 2-3 —Ä–∞–∑–∞
- ‚úÖ –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç
- ‚úÖ –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã —á–∞–Ω–∫–æ–≤ —É–¥–∞–ª—è—é—Ç—Å—è

### –ù–µ—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ

- ‚úÖ Unit tests coverage > 80%
- ‚úÖ –í—Å–µ integration tests –ø—Ä–æ—Ö–æ–¥—è—Ç
- ‚úÖ –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–∞ (README, .env.example)
- ‚úÖ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–µ –Ω–∞ –≤—Å–µ—Ö —ç—Ç–∞–ø–∞—Ö
- ‚úÖ –ù–µ—Ç —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ñ–∞–π–ª–æ–≤ (< 1400s)

### –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

- ‚ö° –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: —É—Å–∫–æ—Ä–µ–Ω–∏–µ ~2-3x
- üìä Overhead –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ñ–∞–π–ª–æ–≤ < 1%
- üíæ Memory usage –Ω–µ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ

---

## –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### –°—Ü–µ–Ω–∞—Ä–∏–π 1: –ë—ã—Å—Ç—Ä–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª–∏–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)

```bash
# .env
OPENAI_GPT4O_MAX_DURATION=1400
OPENAI_CHANGE_MODEL=true
OPENAI_CHUNKING=false
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** –§–∞–π–ª > 1400s –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ whisper-1 (–∫–∞—á–µ—Å—Ç–≤–æ –Ω–∏–∂–µ, –Ω–æ –±—ã—Å—Ç—Ä–æ)

---

### –°—Ü–µ–Ω–∞—Ä–∏–π 2: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç—å—é

```bash
# .env
OPENAI_GPT4O_MAX_DURATION=1400
OPENAI_CHANGE_MODEL=false  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å gpt-4o-transcribe
OPENAI_CHUNKING=true
OPENAI_PARALLEL_CHUNKS=true
OPENAI_MAX_PARALLEL_CHUNKS=3
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
- –§–∞–π–ª —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ 20 –º–∏–Ω—É—Ç
- –ö–∞–∂–¥—ã–π —á–∞–Ω–∫ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è gpt-4o-transcribe
- 3 —á–∞–Ω–∫–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
- –í—Ä–µ–º—è: ~10 –º–∏–Ω—É—Ç –¥–ª—è 30-–º–∏–Ω—É—Ç–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
- –ö–∞—á–µ—Å—Ç–≤–æ: –≤—ã—Å–æ–∫–æ–µ (gpt-4o)

---

### –°—Ü–µ–Ω–∞—Ä–∏–π 3: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º

```bash
# .env
OPENAI_CHUNKING=true
OPENAI_PARALLEL_CHUNKS=false  # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
OPENAI_CHANGE_MODEL=false
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
- –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
- –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–µ—Ä–µ–¥–∞—ë—Ç—Å—è —á–µ—Ä–µ–∑ prompt
- –í—Ä–µ–º—è: ~30 –º–∏–Ω—É—Ç –¥–ª—è 30-–º–∏–Ω—É—Ç–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
- –ö–∞—á–µ—Å—Ç–≤–æ: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ (gpt-4o + –∫–æ–Ω—Ç–µ–∫—Å—Ç)

---

### –°—Ü–µ–Ω–∞—Ä–∏–π 4: –ë–∞–ª–∞–Ω—Å (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)

```bash
# .env
OPENAI_CHUNKING=true
OPENAI_PARALLEL_CHUNKS=true
OPENAI_CHANGE_MODEL=true  # Fallback –Ω–∞ whisper-1 –µ—Å–ª–∏ gpt-4o –ø–∞–¥–∞–µ—Ç
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
- –ë—ã—Å—Ç—Ä–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ gpt-4o
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π fallback –µ—Å–ª–∏ –ø—Ä–æ–±–ª–µ–º—ã
- –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç—å/–∫–∞—á–µ—Å—Ç–≤–æ

---

## –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π

### README.md

–î–æ–±–∞–≤–∏—Ç—å —Å–µ–∫—Ü–∏—é:

```markdown
### Long Audio Files (>23 minutes)

OpenAI's `gpt-4o-transcribe` model has a duration limit of ~1400 seconds (23 minutes).

**Options for handling long files:**

1. **Automatic model switching** (default):
   ```bash
   OPENAI_CHANGE_MODEL=true
   ```
   Files > 1400s are automatically processed with `whisper-1` (no duration limit).

2. **Audio chunking**:
   ```bash
   OPENAI_CHUNKING=true
   OPENAI_PARALLEL_CHUNKS=true
   ```
   - Splits audio into 20-minute segments
   - Processes in parallel for 2-3x speedup
   - Preserves gpt-4o quality

3. **Sequential with context** (best quality):
   ```bash
   OPENAI_CHUNKING=true
   OPENAI_PARALLEL_CHUNKS=false
   ```
   - Passes context between chunks via prompt parameter
   - Slower but better coherence

See `.env.example` for all configuration options.
```

### docs/

–°–æ–∑–¥–∞—Ç—å `docs/features/long-audio-handling.md` —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–µ–π:
- –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏
- –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π
- –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
- Troubleshooting

---

## Timeline

### Phase 1: Core Implementation (4-5 —á–∞—Å–æ–≤)
- [x] –î–æ–±–∞–≤–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ `src/config.py`
- [x] –î–æ–±–∞–≤–∏—Ç—å pydub –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
- [x] –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å `_handle_long_audio()`
- [x] –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å `_split_audio_into_chunks()`
- [x] –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å `_transcribe_single_file()`

### Phase 2: Parallel Processing (2-3 —á–∞—Å–∞)
- [x] –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å `_transcribe_chunks_parallel()`
- [x] –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å `_transcribe_chunks_sequential()`
- [x] –î–æ–±–∞–≤–∏—Ç—å rate limiting —á–µ—Ä–µ–∑ Semaphore
- [x] –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å retry –ª–æ–≥–∏–∫—É

### Phase 3: Testing (2 —á–∞—Å–∞)
- [x] –ù–∞–ø–∏—Å–∞—Ç—å unit tests
- [x] –ù–∞–ø–∏—Å–∞—Ç—å integration tests
- [x] Manual testing —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ —Ñ–∞–π–ª–∞–º–∏

### Phase 4: Documentation (1 —á–∞—Å)
- [x] –û–±–Ω–æ–≤–∏—Ç—å .env.example
- [x] –û–±–Ω–æ–≤–∏—Ç—å README.md
- [x] –°–æ–∑–¥–∞—Ç—å docs/features/long-audio-handling.md

**Total: 8-10 —á–∞—Å–æ–≤**

---

## Next Steps

1. ‚úÖ **–ü–ª–∞–Ω —É—Ç–≤–µ—Ä–∂–¥—ë–Ω** - –¥–æ–∫—É–º–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω
2. ‚è≥ **–†–µ–∞–ª–∏–∑–∞—Ü–∏—è** - –≤—ã–ø–æ–ª–Ω–∏—Ç—å –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º —á–∞—Ç–µ —á–µ—Ä–µ–∑ `/workflow:execute`
3. ‚è≥ **Code Review** - –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é
4. ‚è≥ **Testing** - –∑–∞–ø—É—Å—Ç–∏—Ç—å –≤—Å–µ —Ç–µ—Å—Ç—ã
5. ‚è≥ **Deployment** - —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—å –Ω–∞ VPS
6. ‚è≥ **Monitoring** - –æ—Ç—Å–ª–µ–¥–∏—Ç—å —Ä–∞–±–æ—Ç—É –≤ production

---

## References

**Research Sources:**
- [Building a Long Audio Transcription Tool with OpenAI's Whisper API](https://www.buildwithmatija.com/blog/building-a-long-audio-transcription-tool-with-openai-s-whisper-api)
- [Split and Transcribe Audio Files with OpenAI Whisper](https://ngwaifoong92.medium.com/split-and-transcribe-audio-files-with-openai-whisper-cee0b89a509d)
- [Split large audio file and transcribe it using the Whisper API from OpenAI (GitHub Gist)](https://gist.github.com/patrick-samy/cf8470272d1ff23dff4e2b562b940ef5)
- [GPT4.0-Transcribe‚ÄîMAX 1500 SECONDS? - OpenAI Community](https://community.openai.com/t/gpt4-0-transcribe-max-1500-seconds/1306684)
- [Questions regarding transcribing long audios (>25MB) in Whisper API](https://community.openai.com/t/questions-regarding-transcribing-long-audios-25mb-in-whisper-api/267384)

**Related Files:**
- `src/transcription/providers/openai_provider.py` - –æ—Å–Ω–æ–≤–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
- `src/config.py` - –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
- `src/transcription/models.py` - –º–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö
- `.env.example` - –ø—Ä–∏–º–µ—Ä—ã –Ω–∞—Å—Ç—Ä–æ–µ–∫

---

**Status:** ‚úÖ Ready for Implementation
**Approved by:** User
**Implementation:** Will be done in separate chat session
