# –ü–ª–∞–Ω –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è Memory Bank: API-First Pivot Documentation

**–î–∞—Ç–∞**: 2025-01-09
**–°—Ç–∞—Ç—É—Å**: ‚úÖ Approved
**–í–∞—Ä–∏–∞–Ω—Ç**: Balanced (–í–∞—Ä–∏–∞–Ω—Ç 2)
**–û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏**: 4-6 —á–∞—Å–æ–≤

---

## –ö–æ–Ω—Ç–µ–∫—Å—Ç

### –ü—Ä–æ–±–ª–µ–º–∞

–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è Memory Bank –æ–ø–∏—Å—ã–≤–∞–µ—Ç **—Å—Ç–∞—Ä—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É** (local-first —Å faster-whisper) –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –ø—Ä–æ–¥–∞–∫—à–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç **API-first** –ø–æ–¥—Ö–æ–¥ (OpenAI + DeepSeek).

**–¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ:**
- systemPatterns.md: **2262 —Å—Ç—Ä–æ–∫–∏** (–∫—Ä–∏—Ç–∏—á–Ω–æ oversized)
- 41 –ø–∞—Ç—Ç–µ—Ä–Ω —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏ –æ–ø–∏—Å–∞–Ω–∏—è–º–∏ (–º–Ω–æ–≥–æ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è)
- projectbrief.md: –ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—Ç —Ç–µ–∫—É—â–µ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ ("zero API costs")
- productContext.md: –£—Å—Ç–∞—Ä–µ–≤—à–∏–µ value propositions
- –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Ä–µ—à–µ–Ω–∏—è —Å–º–µ—à–∞–Ω—ã —Å –∞–∫—Ç—É–∞–ª—å–Ω—ã–º–∏

### –§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–æ–¥–∞–∫—à–Ω —Å—Ç–µ–∫ (2025-01-09)

```yaml
–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è:
  Primary: OpenAI API (whisper-1, gpt-4o-transcribe)
  Fallback: faster-whisper local (medium/int8)

–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞:
  Provider: DeepSeek V3
  Features: Structure, Magic, Summary

–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
  Type: API-first with local fallback
  Cost: ~$0.006/min transcription + ~$0.0002 per 60s text
  UX: 5-15 seconds vs 60+ seconds (local)
```

---

## –¶–µ–ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è

1. ‚úÖ –û—Ç—Ä–∞–∑–∏—Ç—å **API-first –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É** –≤ –æ—Å–Ω–æ–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
2. ‚úÖ –°–æ–∫—Ä–∞—Ç–∏—Ç—å systemPatterns.md –Ω–∞ **60%** (2262 ‚Üí ~800 —Å—Ç—Ä–æ–∫)
3. ‚úÖ –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (initial vision ‚Üí pivot)
4. ‚úÖ –£–±—Ä–∞—Ç—å –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è –º–µ–∂–¥—É –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
5. ‚úÖ –£–ª—É—á—à–∏—Ç—å –Ω–∞–≤–∏–≥–∞—Ü–∏—é –∏ —á–∏—Ç–∞–µ–º–æ—Å—Ç—å

---

## –í—ã–±—Ä–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥: Balanced (–í–∞—Ä–∏–∞–Ω—Ç 2)

**–ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è:** –†–∞–∑–¥–µ–ª–∏—Ç—å –Ω–∞ "–¢–µ–∫—É—â–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞" + "–ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Ä–µ—à–µ–Ω–∏—è"

### –°–æ–∑–¥–∞—ë–º—ã–µ —Ñ–∞–π–ª—ã

1. **–ù–æ–≤—ã–π:** `memory-bank/architecture-evolution.md` (~200 —Å—Ç—Ä–æ–∫)
   - Initial Vision (2025-10-12)
   - The Pivot (–ø–æ—á–µ–º—É, –∫–æ–≥–¥–∞, –∫–∞–∫)
   - Current Architecture (2025-01-09)

### –û–±–Ω–æ–≤–ª—è–µ–º—ã–µ —Ñ–∞–π–ª—ã

2. **projectbrief.md** (~188 ‚Üí ~120 —Å—Ç—Ä–æ–∫, -30%)
   - –î–æ–±–∞–≤–∏—Ç—å —Å—Å—ã–ª–∫—É –Ω–∞ architecture-evolution.md
   - –û–±–Ω–æ–≤–∏—Ç—å "Current Status" —Å API-first stack
   - –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Ü–µ–ª–∏ –∫–∞–∫ "Initial Vision (2025)"

3. **productContext.md** (~244 ‚Üí ~180 —Å—Ç—Ä–æ–∫, -25%)
   - –ü–µ—Ä–µ–ø–∏—Å–∞—Ç—å "Value Proposition" ‚Üí –Ω–æ–≤—ã–µ –≤—ã–≥–æ–¥—ã
   - –î–æ–±–∞–≤–∏—Ç—å "Current UX" ‚Üí –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å–µ–π—á–∞—Å
   - –£–±—Ä–∞—Ç—å –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è –æ privacy/API costs

4. **systemPatterns.md** (~2262 ‚Üí ~800 —Å—Ç—Ä–æ–∫, **-65%**)
   - –ù–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ (—Å–º. –Ω–∏–∂–µ)
   - 15-20 –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ (–∫–æ—Ä–æ—Ç–∫–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è)
   - Historical Patterns Reference (–∫–∞—Ç–∞–ª–æ–≥ –≤—Å–µ—Ö 41)

5. **techContext.md** (~835 ‚Üí ~500 —Å—Ç—Ä–æ–∫, -40%)
   - –û–±–Ω–æ–≤–∏—Ç—å "Technology Stack" ‚Üí OpenAI primary
   - –î–æ–±–∞–≤–∏—Ç—å "Cost Structure" ‚Üí —Ä–µ–∞–ª—å–Ω—ã–µ —Ü–µ–Ω—ã
   - –£–±—Ä–∞—Ç—å –¥—É–±–ª–∏ —Å systemPatterns

---

## –î–µ—Ç–∞–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è

### Phase 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ (30 min)

**–¶–µ–ª—å:** –°–æ–∑–¥–∞—Ç—å safety net –ø–µ—Ä–µ–¥ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏

**–®–∞–≥–∏:**

1. –°–æ–∑–¥–∞—Ç—å backup branch:
   ```bash
   git checkout -b docs/update-memory-bank-2025-01-09
   git push -u origin docs/update-memory-bank-2025-01-09
   ```

2. –°–æ–∑–¥–∞—Ç—å –ø–æ–ª–Ω—ã–π backup:
   ```bash
   cp -r memory-bank memory-bank.backup.2025-01-09
   ```

3. –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ç–µ–∫—É—â—É—é –≤–µ—Ä—Å–∏—é systemPatterns.md:
   ```bash
   cp memory-bank/systemPatterns.md memory-bank/systemPatterns-2025-01-09-pre-update.md
   ```

4. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å git status:
   ```bash
   git status
   git log --oneline -5
   ```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ —É—Å–ø–µ—Ö–∞:**
- ‚úÖ Branch —Å–æ–∑–¥–∞–Ω –∏ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –≤ remote
- ‚úÖ Backup –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
- ‚úÖ systemPatterns —Å–æ—Ö—Ä–∞–Ω—ë–Ω —Å –¥–∞—Ç–æ–π

---

### Phase 2: –°–æ–∑–¥–∞—Ç—å architecture-evolution.md (1 hour)

**–¶–µ–ª—å:** –î–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å evolution –æ—Ç local-first –∫ API-first

**–®–∞–≥–∏:**

1. –°–æ–∑–¥–∞—Ç—å —Ñ–∞–π–ª:
   ```bash
   touch memory-bank/architecture-evolution.md
   ```

2. –ù–∞–ø–∏—Å–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É:

   ```markdown
   # Architecture Evolution: Local-First ‚Üí API-First

   ## Initial Vision (2025-10-12)
   ### Goals
   - Local Whisper only (faster-whisper medium)
   - Zero API costs
   - Complete privacy
   - Cost-effective solution

   ### Implementation
   - Model: faster-whisper medium/int8/beam1
   - Performance: RTF ~0.3x (3x faster than audio)
   - Memory: ~2GB RAM peak
   - Quality: Excellent for Russian

   ### Limitations Discovered
   - Slow: 60s audio = 20-36s processing
   - Resource-intensive: 2GB RAM
   - No advanced features (structuring, summary)
   - User experience: "Too slow"

   ---

   ## The Pivot (2025-11-20 ‚Üí 2025-12-25)

   ### Why API-First?

   1. **UX Quality**
      - OpenAI API: 5-10 seconds for 1-minute audio
      - Local: 20-36 seconds for 1-minute audio
      - **4-7x faster** for user

   2. **Transcription Quality**
      - OpenAI whisper-1: Best-in-class for Russian
      - gpt-4o-transcribe: Even better accuracy
      - Local medium: Good but not excellent

   3. **Feature Capabilities**
      - LLM text processing (DeepSeek V3)
      - Structuring, summary, magic modes
      - Impossible with local-only architecture

   4. **Reliability**
      - API: 99.9% uptime, consistent quality
      - Local: VPS crashes, memory issues, variability

   5. **Cost Acceptability**
      - Transcription: $0.006/min (whisper-1)
      - Text processing: ~$0.0002 per 60s (DeepSeek)
      - 60s message total: ~$0.0062
      - **Acceptable for quality provided**

   ### Migration Path

   **Phase 1: Local Only** (2025-10-12 ‚Üí 2025-11-19)
   - faster-whisper medium/int8/beam1
   - Queue-based processing
   - Progress tracking
   - No LLM features

   **Phase 2: Hybrid Experiment** (2025-11-20 ‚Üí 2025-12-04)
   - Short audio: Local quality model
   - Long audio: Local draft ‚Üí LLM refinement
   - First LLM integration (DeepSeek V3)
   - Structured mode feature

   **Phase 3: API Integration** (2025-12-15 ‚Üí 2025-12-25)
   - OpenAI API provider added
   - Provider-aware preprocessing
   - Long audio chunking
   - Document + video support

   **Phase 4: API-First** (2025-12-25 ‚Üí Present)
   - OpenAI API as primary transcription
   - DeepSeek V3 for all text processing
   - faster-whisper as fallback only
   - All interactive features enabled

   ### Decision Dates

   | Date | Decision | Rationale |
   |------|----------|-----------|
   | 2025-11-20 | Add LLM refinement | Improve long audio quality |
   | 2025-12-04 | Add interactive features | User feedback: "Make it beautiful" |
   | 2025-12-15 | Add OpenAI provider | Better quality, reliability |
   | 2025-12-25 | Make OpenAI primary | UX: speed matters most |
   | 2025-01-09 | Document pivot | Clarify architecture evolution |

   ---

   ## Current Architecture (2025-01-09)

   ### Production Stack

   #### Transcription Layer
   **Primary:** OpenAI API
   - Model: whisper-1 (default), gpt-4o-transcribe (optional)
   - Performance: 5-10s for 1-minute audio
   - Quality: Best-in-class for Russian
   - Cost: $0.006/min
   - Fallback: Auto-switch to whisper-1 for >23min audio

   **Fallback:** faster-whisper local
   - Model: medium/int8/beam1
   - Performance: 20-36s for 1-minute audio
   - Used: When OpenAI API unavailable

   #### Text Processing Layer
   **Provider:** DeepSeek V3
   - Features:
     - Structured mode (paragraphs, headings, lists)
     - Magic mode (publication-ready text)
     - Summary mode (key points)
   - Performance: 2-5s processing time
   - Cost: ~$0.0002 per 60s audio
   - Quality: Excellent Russian text transformation

   #### File Support
   - Voice messages (Telegram standard)
   - Audio files (MP3, OGG, WAV, etc.)
   - Documents (.aac, .flac, .wma, .amr, etc.)
   - Videos (.mp4, .mkv, .avi, .mov with audio extraction)
   - Large files: Up to 2GB via Telethon

   #### Interactive Features
   - Inline keyboard with 3 main buttons:
     - üìù –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞—Ç—å (Structure)
     - ü™Ñ –°–¥–µ–ª–∞—Ç—å –∫—Ä–∞—Å–∏–≤–æ (Magic)
     - üìã –û —á–µ–º —ç—Ç–æ—Ç —Ç–µ–∫—Å—Ç (Summary)
   - Variant caching in database
   - File export for long text (>3000 chars)

   ### Architecture Diagram

   ```
   User sends voice/audio/video
     ‚Üì
   Bot handler (type detection)
     ‚Üì
   File download (Bot API or Telethon)
     ‚Üì
   Audio extraction (if video)
     ‚Üì
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Primary: OpenAI API             ‚îÇ
   ‚îÇ - whisper-1 or gpt-4o-transcribe ‚îÇ
   ‚îÇ - 5-10 seconds for 1-minute      ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚Üì (if API fails)
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Fallback: faster-whisper        ‚îÇ
   ‚îÇ - medium/int8/beam1             ‚îÇ
   ‚îÇ - 20-36 seconds for 1-minute    ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚Üì
   Transcription result
     ‚Üì
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ DeepSeek V3 (LLM)               ‚îÇ
   ‚îÇ - Structured / Magic / Summary  ‚îÇ
   ‚îÇ - 2-5 seconds processing        ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚Üì
   Interactive keyboard
   - Original / Structured / Magic / Summary
   - Variant caching
   - File export (if long)
     ‚Üì
   User response
   ```

   ### Configuration

   **Environment Variables (.env):**
   ```bash
   # Transcription providers
   WHISPER_PROVIDERS=["openai", "faster-whisper"]
   PRIMARY_PROVIDER=openai
   FALLBACK_PROVIDER=faster-whisper
   OPENAI_MODEL=whisper-1

   # LLM text processing
   LLM_REFINEMENT_ENABLED=true
   LLM_PROVIDER=deepseek
   LLM_MODEL=deepseek-chat
   LLM_API_KEY=sk-...

   # Interactive features
   INTERACTIVE_MODE_ENABLED=true
   ENABLE_STRUCTURED_MODE=true
   ENABLE_MAGIC_MODE=true
   ENABLE_SUMMARY_MODE=true
   ```

   ### Performance Metrics

   | Operation | API-First | Local-Only | Improvement |
   |-----------|-----------|------------|-------------|
   60s audio transcription | 5-10s | 20-36s | **4-7x faster** |
   Text structuring | 2-5s | N/A | New feature |
   Total end-to-end | 7-15s | 20-36s | **2-5x faster** |
   Memory usage | <500MB | ~2GB | **4x less** |
   CPU usage | Low | High | **Significant** |

   ### Cost Structure

   **Per 60-second message:**
   - OpenAI transcription: $0.006
   - DeepSeek processing: ~$0.0002
   - **Total: ~$0.0062 per message**

   **Monthly estimates:**
   - 100 messages/day √ó 30 days = 3000 messages
   - 3000 √ó 60s = 180,000 seconds = 50 hours
   - Cost: 3000 √ó $0.0062 = **$18.60/month**

   **Comparison:**
   - Free (local): $0 but poor UX
   - API-first: $18.60/month for 100 daily users
   - **Conclusion:** Cost acceptable for UX quality

   ### Benefits Realized

   1. **User Experience**: 4-7x faster = users actually use the bot
   2. **Quality**: OpenAI > local for Russian speech
   3. **Features**: Structuring, magic, summary (impossible locally)
   4. **Reliability**: API with fallback = always works
   5. **Resource usage**: 4x less memory/CPU on VPS
   6. **Scalability**: Can handle 10x more users

   ### Trade-offs Accepted

   1. **Cost**: $18.60/month vs $0 (but worth it for UX)
   2. **Privacy**: Data sent to API (but same as Telegram itself)
   3. **Dependency**: Require internet + API keys (acceptable for production)

   ---

   ## Future Considerations

   ### Potential Improvements
   - **Cost optimization**: Cache common phrases, batch requests
   - **Hybrid mode**: Short = local, long = API (balanced approach)
   - **Multiple providers**: Add Azure, Google Speech for redundancy
   - **Local LLM**: Run DeepSeek-like model locally when hardware allows

   ### Monitoring Metrics
   - API cost per user/month
   - Average transcription time
   - Error rate (API failures)
   - Feature usage (structure/magic/summary)
   - User satisfaction feedback

   ---

   ## Related Documentation

   - [Current System Patterns](../systemPatterns.md)
   - [Technical Context](../techContext.md)
   - [Project Brief](../projectbrief.md)
   - [Product Context](../productContext.md)
   ```

3. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å:
   - –î–∞—Ç—ã —Å–æ–≤–ø–∞–¥–∞—é—Ç —Å git history
   - –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏ —Ç–æ—á–Ω—ã
   - –°—Å—ã–ª–∫–∏ –Ω–∞ –¥—Ä—É–≥–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Ä–∞–±–æ—Ç–∞—é—Ç

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ —É—Å–ø–µ—Ö–∞:**
- ‚úÖ –§–∞–π–ª —Å–æ–∑–¥–∞–Ω —Å –≤—ã—à–µ—É–∫–∞–∑–∞–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π
- ‚úÖ –ò—Å—Ç–æ—Ä–∏—è pivot –¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∞ —Å –¥–∞—Ç–∞–º–∏
- ‚úÖ –¢–µ–∫—É—â–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –æ–ø–∏—Å–∞–Ω–∞ —Ç–æ—á–Ω–æ
- ‚úÖ Diagram –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç data flow

---

### Phase 3: –û–±–Ω–æ–≤–∏—Ç—å projectbrief.md (1 hour)

**–¶–µ–ª—å:** –û—Ç—Ä–∞–∑–∏—Ç—å API-first –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É, —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é

**–®–∞–≥–∏:**

1. –ü—Ä–æ—á–∏—Ç–∞—Ç—å —Ç–µ–∫—É—â–∏–π —Ñ–∞–π–ª:
   ```bash
   cat memory-bank/projectbrief.md
   ```

2. –û–±–Ω–æ–≤–∏—Ç—å —Å–µ–∫—Ü–∏—é "Project Vision":

   ```markdown
   ## Project Vision

   Create a Telegram bot for transcribing voice messages with **exceptional user experience** using **API-first architecture** with intelligent text processing.

   **Current Approach (2025-01-09):**
   - OpenAI Whisper API for fast, accurate transcription (primary)
   - DeepSeek V3 for intelligent text structuring and processing
   - faster-whisper local model as fallback (reliability)
   - Interactive features: Structure, Magic, Summary modes

   **Initial Vision (2025-10-12):**
   - Local Whisper AI model only (faster-whisper)
   - Zero API costs
   - Privacy-focused (all processing local)
   - Cost-effective solution

   **Evolution:** See [architecture-evolution.md](./architecture-evolution.md) for complete pivot story
   ```

3. –û–±–Ω–æ–≤–∏—Ç—å "Core Objectives":

   ```markdown
   ## Core Objectives

   **Primary Goal**: Build a production-ready Telegram bot with the **best user experience** for voice-to-text transcription.

   **Current Objectives (2025-01-09):**
   1. Fast transcription (<15 seconds for 1-minute audio)
   2. High-quality Russian speech recognition
   3. Intelligent text processing (structuring, summary, magic)
   4. Reliable service (API with local fallback)
   5. Cost-effective operations (~$0.006 per message)

   **Initial Objectives (2025-10-12):**
   1. Zero API costs (local processing only)
   2. Privacy-focused (no data sent externally)
   3. Cost-effective (free to operate)

   **Status**: ‚úÖ Phase 10.15 Complete (2025-12-25)
   - Bot deployed and operational on VPS
   - API-first architecture with local fallback
   - All interactive features enabled
   - Full file type support (voice, audio, documents, videos)
   ```

4. –û–±–Ω–æ–≤–∏—Ç—å "Current Status":

   ```markdown
   **Current Status**: ‚úÖ Phase 10.15 Complete (2025-12-25)

   **Production Configuration:**
   - Transcription: OpenAI API (whisper-1, gpt-4o-transcribe)
   - Text Processing: DeepSeek V3 (structuring, magic, summary)
   - Fallback: faster-whisper medium/int8 (local)
   - VPS: 3GB RAM, 4 CPU cores (Russian VPS)
   - Cost: ~$0.006 per minute of transcription

   **Features:**
   - ‚úÖ Voice, audio, document, video file support (up to 2GB)
   - ‚úÖ Interactive keyboard with 3 processing modes
   - ‚úÖ Variant caching in database
   - ‚úÖ Long audio handling (>23 minutes with chunking)
   - ‚úÖ File export for text >3000 characters

   **Date**: Started 2025-10-12, Production-ready 2025-10-24, API-First 2025-12-25
   ```

5. –î–æ–±–∞–≤–∏—Ç—å —Å–µ–∫—Ü–∏—é "Architecture Evolution" –ø–æ—Å–ª–µ "Functionality Scope":

   ```markdown
   ## Architecture Evolution

   **Timeline:**
   - **2025-10-12**: Project started (local-only vision)
   - **2025-11-20**: LLM integration (DeepSeek V3)
   - **2025-12-15**: OpenAI API provider added
   - **2025-12-25**: API-first architecture (current)

   **Key Decisions:**
   - Why API-first? ‚Üí 4-7x faster, better quality, LLM features
   - Cost acceptability ‚Üí $0.006/min worth it for UX
   - Fallback strategy ‚Üí Local model for reliability

   **Complete evolution story:** See [architecture-evolution.md](./architecture-evolution.md)
   ```

6. –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Ü–µ–ª–∏ –≤ –∫–æ–Ω—Ü–µ —Ñ–∞–π–ª–∞:

   ```markdown
   ---

   ## Initial Vision (Archived 2025-12-25)

   **Original Goals (2025-10-12):**
   - Primary Goal: Build a bot using local Whisper AI model
   - Zero API costs (free to operate)
   - Privacy-focused (all processing local)
   - Cost-effective solution for personal use

   **Why Pivot?**
   - Speed: Local transcription too slow (20-36s vs 5-10s API)
   - Quality: OpenAI API better than local models
   - Features: LLM text processing impossible locally
   - UX: Users won't wait 30+ seconds for transcription

   **Migration documented in:** [architecture-evolution.md](./architecture-evolution.md)
   ```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ —É—Å–ø–µ—Ö–∞:**
- ‚úÖ Project Vision –æ—Ç—Ä–∞–∂–∞–µ—Ç API-first –ø–æ–¥—Ö–æ–¥
- ‚úÖ Initial Vision —Å–æ—Ö—Ä–∞–Ω—ë–Ω –∫–∞–∫ –∞—Ä—Ö–∏–≤
- ‚úÖ –°—Å—ã–ª–∫–∞ –Ω–∞ architecture-evolution.md –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç
- ‚úÖ Current Status –æ–±–Ω–æ–≤–ª—ë–Ω —Å –ø—Ä–æ–¥–∞–∫—à–Ω —Å—Ç–µ–∫–æ–º

---

### Phase 4: –û–±–Ω–æ–≤–∏—Ç—å productContext.md (1.5 hours)

**–¶–µ–ª—å:** –û–±–Ω–æ–≤–∏—Ç—å value propositions, —É–±—Ä–∞—Ç—å –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è

**–®–∞–≥–∏:**

1. –ü—Ä–æ—á–∏—Ç–∞—Ç—å —Ç–µ–∫—É—â–∏–π —Ñ–∞–π–ª:
   ```bash
   cat memory-bank/productContext.md
   ```

2. –ü–µ—Ä–µ–ø–∏—Å–∞—Ç—å "The Primary Need":

   ```markdown
   ### The Primary Need

   Voice messages in Telegram are popular but have key limitations:

   **Current Problem (2025-01-09):**
   - **Speed**: Local transcription takes 20-36 seconds (users won't wait)
   - **Quality**: Local models make mistakes on Russian speech
   - **Features**: Cannot intelligently process or structure text
   - **UX**: Slow processing = poor user experience

   **Original Problem (2025-10-12):**
   - Voice messages cannot be quickly scanned or searched
   - Time inefficiency (cannot skim content)
   - Accessibility issues (noise, hearing impairment)
   - Language barriers (difficult to translate)

   **Solution**: Fast API transcription + intelligent text processing
   ```

3. –ü–µ—Ä–µ–ø–∏—Å–∞—Ç—å "Value Proposition":

   ```markdown
   ## Value Proposition ‚úÖ UPDATED (2025-01-09)

   **Core Value**: Best-in-class voice-to-text with exceptional UX

   **Current Key Differentiators:**
   1. **Speed**: OpenAI API transcription in 5-10 seconds (vs 20-36s local)
   2. **Quality**: Best-in-class Russian speech recognition (OpenAI whisper-1)
   3. **Intelligence**: LLM-powered structuring, summary, magic modes
   4. **Reliability**: API with local fallback = always works
   5. **File Support**: Any audio/video format (up to 2GB)

   **Cost Structure:**
   - Transcription: $0.006 per minute
   - Text processing: ~$0.0002 per 60s
   - Total: ~$0.0062 per 60-second message
   - **Acceptable for quality provided**

   **Competitive Comparison (Updated):**
   - vs. Local-only bots: 4-7x faster, better quality, LLM features
   - vs. Other API bots: Interactive features, DeepSeek processing
   - vs. Telegram native: More accurate, searchable, structured
   - vs. Cloud APIs: Better UX, text processing, Russian-optimized

   **Original Value Proposition (Archived 2025-12-25):**
   - Zero API Costs: Local Whisper model, no per-request charges
   - Privacy: No data sent to external services
   - Speed: faster-whisper is 4x faster than alternatives
   - Free Tier: 60 seconds/day sustainable for casual users

   **Why Change?** See [architecture-evolution.md](./architecture-evolution.md)
   ```

4. –î–æ–±–∞–≤–∏—Ç—å —Å–µ–∫—Ü–∏—é "Current User Experience":

   ```markdown
   ## Current User Experience ‚úÖ NEW (2025-01-09)

   **Primary Flow:**
   1. User sends voice/audio/video file
   2. Bot acknowledges: "‚è≥ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é..."
   3. OpenAI API transcribes: 5-10 seconds (for 1-minute audio)
   4. DeepSeek V3 structures: 2-5 seconds
   5. User receives structured text with interactive keyboard:
      - –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç (Original)
      - üìù –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞—Ç—å (Structure)
      - ü™Ñ –°–¥–µ–ª–∞—Ç—å –∫—Ä–∞—Å–∏–≤–æ (Magic)
      - üìã –û —á–µ–º —ç—Ç–æ—Ç —Ç–µ–∫—Å—Ç (Summary)

   **Performance:**
   - Total time: 7-15 seconds (vs 20-36s local)
   - Quality: Excellent for Russian speech
   - Features: LLM-powered transformations
   - File types: Voice, audio, documents, videos (up to 2GB)

   **User Feedback:**
   - "–¢–∞–∫ –±—ã—Å—Ç—Ä–æ!" (So fast!)
   - "–£–¥–æ–±–Ω–æ, –º–æ–∂–Ω–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞—Ç—å" (Convenient, can structure)
   - "–ù–∞–º–Ω–æ–≥–æ –ª—É—á—à–µ —á–µ–º –¥—Ä—É–≥–∏–µ –±–æ—Ç—ã" (Much better than other bots)
   ```

5. –û–±–Ω–æ–≤–∏—Ç—å "Product Principles":

   ```markdown
   ## Product Principles ‚úÖ UPDATED (2025-01-09)

   1. **User Experience First**: Fast transcription (7-15s total) > free but slow
   2. **Quality Over Cost**: OpenAI API worth the price for accuracy
   3. **Intelligence**: LLM-powered features (structure, magic, summary)
   4. **Reliability**: API with fallback = always available
   5. **Privacy**: API processing (same as Telegram itself)
   6. **Simplicity**: Instant value, minimal learning curve
   7. **Extensibility**: Built to grow with user needs

   **Original Principles (Archived 2025-12-25):**
   1. Privacy First: Local processing by default, no data sharing
   2. Cost Conscious: Optimize for zero API costs
   3. User Respect: Clear status, helpful errors, no spam
   4. Accessibility: Free tier sufficient for casual use
   5. Simplicity: Minimal learning curve, instant value
   6. Extensibility: Built to grow with user needs
   ```

6. –û–±–Ω–æ–≤–∏—Ç—å "Non-Goals" (—É–±—Ä–∞—Ç—å –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è):

   ```markdown
   ## Non-Goals ‚úÖ UPDATED (2025-01-09)

   What this bot intentionally **won't do**:

   1. **Zero-Cost Operation**: API costs accepted for UX quality (~$0.006/min)
   2. **Local-Only Processing**: API-first architecture (local is fallback)
   3. **Real-time Translation**: Focus is transcription + text processing
   4. **Social Features**: No user profiles, friends, or sharing
   5. **Audio Generation**: No text-to-speech (TTS)
   6. **Advanced Analytics**: Basic usage tracking only
   7. **Multi-platform**: Telegram-only, no web app or mobile app

   **Removed (2025-12-25):**
   - ‚ùå "Audio Storage": Changed - files stored 7 days for retranscription
   - ‚ùå "Zero API Costs": Changed - API costs accepted for UX
   ```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ —É—Å–ø–µ—Ö–∞:**
- ‚úÖ Value Proposition –æ—Ç—Ä–∞–∂–∞–µ—Ç —Ç–µ–∫—É—â–∏–µ –≤—ã–≥–æ–¥—ã
- ‚úÖ Primary Problem –æ–ø–∏—Å—ã–≤–∞–µ—Ç –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –±–æ–ª–∏
- ‚úÖ Current User Experience –¥–æ–±–∞–≤–ª–µ–Ω
- ‚úÖ Product Principles –æ–±–Ω–æ–≤–ª–µ–Ω—ã
- ‚úÖ Original –∑–Ω–∞—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –∫–∞–∫ "Archived"
- ‚úÖ –ù–µ—Ç –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π —Å —Ç–µ–∫—É—â–µ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–µ–π

---

### Phase 5: –†–µ–æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞—Ç—å systemPatterns.md (2 hours)

**–¶–µ–ª—å:** –°–æ–∫—Ä–∞—Ç–∏—Ç—å –Ω–∞ 65%, —É–±—Ä–∞—Ç—å –¥—É–±–ª–∏, –æ—Å—Ç–∞–≤–∏—Ç—å –∫—Ä–∏—Ç–∏—á–Ω–æ–µ

**–®–∞–≥–∏:**

1. –°–æ–∑–¥–∞—Ç—å backup:
   ```bash
   cp memory-bank/systemPatterns.md memory-bank/systemPatterns-backup-2025-01-09.md
   ```

2. –°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É:

   ```markdown
   # System Patterns: Telegram Voice2Text Bot

   **Last Updated**: 2025-01-09
   **Architecture**: API-First (OpenAI + DeepSeek + Local Fallback)

   ---

   ## Current Architecture Overview

   ### High-Level Design

   Current architecture is **API-first** with intelligent text processing:

   ```
   User Input (Voice/Audio/Video)
     ‚Üì
   File Detection & Download
     ‚Üì
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Primary: OpenAI API (whisper-1)      ‚îÇ
   ‚îÇ - Fast: 5-10s for 1-minute audio      ‚îÇ
   ‚îÇ - Quality: Best-in-class Russian      ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚Üì (if API fails)
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Fallback: faster-whisper (local)     ‚îÇ
   ‚îÇ - Model: medium/int8/beam1           ‚îÇ
   ‚îÇ - Slower: 20-36s for 1-minute audio  ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚Üì
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ LLM: DeepSeek V3                     ‚îÇ
   ‚îÇ - Structure, Magic, Summary modes    ‚îÇ
   ‚îÇ - 2-5s processing time               ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚Üì
   Interactive Keyboard + Variant Caching
   ```

   **See:** [architecture-evolution.md](./architecture-evolution.md) for complete story

   ---

   ## Essential Patterns (15 Critical Patterns)

   ### 1. API-First Transcription Pattern

   **Problem:** Local transcription too slow (20-36s) for good UX

   **Solution:** Use OpenAI API as primary, local model as fallback

   **Implementation:**
   ```python
   # In routing strategy
   if primary_provider == "openai":
       try:
           result = await openai_provider.transcribe(audio)
       except APIError:
           result = await fallback_provider.transcribe(audio)
   ```

   **Benefits:**
   - 4-7x faster (5-10s vs 20-36s)
   - Better quality (OpenAI > local)
   - Reliable (fallback ensures availability)

   **Added:** 2025-12-25
   **Location:** `src/transcription/routing/strategies.py`

   ---

   ### 2. LLM Text Processing Pattern

   **Problem:** Raw transcription needs intelligent structuring

   **Solution:** Use DeepSeek V3 for text transformation

   **Implementation:**
   ```python
   # In TextProcessor
   async def create_structured(self, text: str) -> str:
       prompt = load_prompt("prompts/structured.md")
       refined = await llm_service.refine_text(text, prompt)
       return sanitize_html(refined)
   ```

   **Features:**
   - Structured mode (paragraphs, headings, lists)
   - Magic mode (publication-ready, author's voice)
   - Summary mode (key points)

   **Benefits:**
   - Professional-looking text
   - Scannable structure
   - Quick comprehension

   **Added:** 2025-12-04
   **Location:** `src/services/text_processor.py`

   ---

   ### 3. Fallback Strategy Pattern

   **Problem:** API can fail (network, rate limits, downtime)

   **Solution:** Graceful degradation to local model

   **Implementation:**
   ```python
   try:
       result = await openai_provider.transcribe(audio)
   except (APIError, Timeout, NetworkError) as e:
       logger.warning(f"OpenAI failed: {e}, using fallback")
       result = await faster_whisper_provider.transcribe(audio)
   ```

   **Key Features:**
   - Automatic fallback on errors
   - User sees result either way
   - Logging for monitoring

   **Benefits:**
   - Always returns transcription
   - No user-facing errors
   - Reliability = trust

   **Added:** 2025-12-15
   **Location:** `src/transcription/routing/strategies.py`

   ---

   ### 4. Variant Caching Pattern

   **Problem:** Re-generating same text variant wastes API money

   **Solution:** Cache in database with composite key

   **Implementation:**
   ```python
   # Check cache first
   existing = await variant_repo.get_variant(
       usage_id, mode="structured", length="default", emoji=1
   )
   if existing:
       return existing.text_content

   # Generate and cache
   structured = await text_processor.create_structured(original)
   await variant_repo.create(
       usage_id=usage_id,
       mode="structured",
       text_content=structured
   )
   ```

   **Key:**
   - Composite unique key: (usage_id, mode, length, emoji, timestamps)
   - Prevents duplicate LLM calls
   - Fast re-display on mode switch

   **Benefits:**
   - Cost savings (no duplicate API calls)
   - Fast re-display (database read vs LLM call)
   - Better UX (instant mode switching)

   **Added:** 2025-12-03
   **Location:** `src/storage/repositories.py`

   ---

   ### 5. Interactive Keyboard Pattern

   **Problem:** Users want text variations without re-sending

   **Solution:** Inline keyboard with mode switching

   **Implementation:**
   ```python
   keyboard = InlineKeyboardMarkup([
       [InlineKeyboardButton("–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç", callback_data="mode:original")],
       [InlineKeyboardButton("üìù –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞—Ç—å", callback_data="mode:structured")],
       [InlineKeyboardButton("ü™Ñ –°–¥–µ–ª–∞—Ç—å –∫—Ä–∞—Å–∏–≤–æ", callback_data="mode:magic")],
       [InlineKeyboardButton("üìã –û —á–µ–º —ç—Ç–æ—Ç —Ç–µ–∫—Å—Ç", callback_data="mode:summary")],
   ])
   ```

   **Key Features:**
   - Mode switching (original/structured/magic/summary)
   - Emoji/length/timestamps options
   - "Currently viewing" indicator
   - State tracking in database

   **Benefits:**
   - Rich interaction without re-sending
   - Explore different transformations
   - Single message for all variants

   **Added:** 2025-12-03
   **Location:** `src/bot/keyboards.py`, `src/bot/callbacks.py`

   ---

   ### 6. File Delivery Pattern

   **Problem:** Telegram 4096 char limit, long text needs file

   **Solution:** Threshold-based delivery (text vs file)

   **Implementation:**
   ```python
   FILE_THRESHOLD_CHARS = 3000  # Conservative limit

   if len(text) <= FILE_THRESHOLD_CHARS:
       # Send as message with keyboard
       await message.reply_text(text, reply_markup=keyboard)
   else:
       # Send as file + info message with keyboard
       info_msg = await message.reply_text("üìù –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –≥–æ—Ç–æ–≤–∞! –§–∞–π–ª –Ω–∏–∂–µ ‚Üì", reply_markup=keyboard)
       file_obj = io.BytesIO(text.encode("utf-8"))
       await message.reply_document(document=file_obj, filename="transcription.txt")
   ```

   **Key Features:**
   - Threshold: 3000 chars (conservative)
   - Two-message pattern (info + file)
   - Keyboard on info message
   - State tracking (is_file_message)

   **Benefits:**
   - No data loss for long text
   - Professional PDF for >3000 chars
   - Consistent UX across all message types

   **Added:** 2025-12-08
   **Location:** `src/bot/handlers.py`

   ---

   ### 7. Progress Tracking Pattern

   **Problem:** Long operations need user feedback

   **Solution:** Background task updates message every 5 seconds

   **Implementation:**
   ```python
   progress = ProgressTracker(
       message=status_msg,
       duration_seconds=audio_duration,
       rtf=0.3,  # Real-time factor
       update_interval=5
   )
   await progress.start()

   # Automatically updates with progress bar:
   # üîÑ [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë] 40% (12s / 30s)

   await progress.stop()
   ```

   **Key Features:**
   - Visual progress bar
   - RTF-based time estimation
   - Telegram rate limit handling
   - Graceful cleanup

   **Benefits:**
   - Reduces perceived wait time
   - Shows system is working
   - Transparent about progress

   **Added:** 2025-10-29
   **Location:** `src/services/progress_tracker.py`

   ---

   ### 8. Queue Management Pattern

   **Problem:** Concurrent transcriptions crash VPS (1GB RAM)

   **Solution:** Bounded queue with sequential processing

   **Implementation:**
   ```python
   queue_manager = QueueManager(
       max_queue_size=10,  # Max pending requests
       max_concurrent=1,    # Sequential processing
   )

   # Enqueue with position tracking
   position = await queue_manager.enqueue(request)

   # Worker processes sequentially
   async def _process_request(request):
       result = await transcribe(request.file_path)
       await send_result(request.chat_id, result)
   ```

   **Key Features:**
   - FIFO queue (max 10 pending)
   - Sequential processing (1 at a time)
   - Atomic position tracking
   - Graceful backpressure

   **Benefits:**
   - No crashes (resource limits respected)
   - Fair ordering (FIFO)
   - Predictable performance

   **Added:** 2025-10-29
   **Location:** `src/services/queue_manager.py`

   ---

   ### 9. Provider-Aware Preprocessing Pattern

   **Problem:** Different providers need different audio formats

   **Solution:** Preprocess based on target provider

   **Implementation:**
   ```python
   def preprocess_audio(audio_path, target_provider):
       if target_provider == "openai":
           model = settings.openai_model
           if model in ["gpt-4o-transcribe", "gpt-4o-mini-transcribe"]:
               # These models require MP3/WAV
               return convert_to_mp3(audio_path)
           else:
               # whisper-1 supports OGA
               return audio_path
       elif target_provider == "faster-whisper":
           # OGA optimal for local
           return audio_path
   ```

   **Key Features:**
   - Smart format conversion (only when needed)
   - Optimal format per provider
   - Backward compatible

   **Benefits:**
   - Enables new OpenAI models (gpt-4o-*)
   - No unnecessary conversions
   - Best performance per provider

   **Added:** 2025-12-15
   **Location:** `src/transcription/audio_handler.py`

   ---

   ### 10. Graceful Degradation Pattern

   **Problem:** LLM failures shouldn't block users

   **Solution:** Always fallback to original text

   **Implementation:**
   ```python
   try:
       structured = await llm_service.refine_text(original, prompt)
       final_text = sanitize_html(structured)
   except (LLMTimeoutError, LLMAPIError, Exception) as e:
       logger.error(f"LLM failed: {e}")
       final_text = original  # Fallback to original

   # Always return something
   await send_result(final_text)
   ```

   **Key Features:**
   - Try LLM first
   - Catch all exceptions
   - Fallback to original
   - Never block user

   **Benefits:**
   - Users always get result
   - No error messages for failures
   - Better UX (graceful vs broken)

   **Added:** 2025-11-20
   **Location:** `src/bot/handlers.py`, `src/services/llm_service.py`

   ---

   ### 11. Feature Flags Pattern

   **Problem:** Need safe rollout for new features

   **Solution:** Environment-based feature toggles

   **Implementation:**
   ```python
   # In config.py
   interactive_mode_enabled: bool = Field(default=False)
   enable_structured_mode: bool = Field(default=False)
   enable_magic_mode: bool = Field(default=False)

   # In handlers
   if settings.enable_structured_mode:
       keyboard.add(structured_button)
   ```

   **Key Features:**
   - Master switch + per-feature flags
   - Disabled by default
   - Gradual rollout possible

   **Benefits:**
   - Deploy code without activating
   - Test with specific users
   - Quick disable if issues

   **Added:** 2025-12-03
   **Location:** `src/config.py`, `src/bot/keyboards.py`

   ---

   ### 12. State Management Pattern

   **Problem:** Track UI state per transcription (mode, emoji, length)

   **Solution:** Database model with state fields

   **Implementation:**
   ```python
   class TranscriptionState(Base):
       id: Mapped[int] = mapped_column(primary_key=True)
       usage_id: Mapped[int] = mapped_column(ForeignKey("usage.id"))
       message_id: Mapped[int]  # For keyboard updates
       active_mode: Mapped[str]  # "original", "structured", "magic", "summary"
       emoji_level: Mapped[int]  # 0-3
       length_level: Mapped[str]  # "shorter", "short", "default", "long", "longer"
       timestamps_enabled: Mapped[bool]
       is_file_message: Mapped[bool]
   ```

   **Key Features:**
   - Tracks all UI state
   - Allows mode switching
   - Keyboard updates based on state

   **Benefits:**
   - Rich interactive UX
   - State persistence
   - Accurate keyboard display

   **Added:** 2025-12-03
   **Location:** `src/storage/models.py`

   ---

   ### 13. Error Handling Pattern

   **Problem:** Users see cryptic errors, don't know what to do

   **Solution:** Clear, actionable error messages in Russian

   **Implementation:**
   ```python
   try:
       result = await transcribe(audio)
   except AudioTooLongError:
       await message.reply_text(
           f"‚ö†Ô∏è –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {limit}—Å\n"
           f"–í–∞—à —Ñ–∞–π–ª: {user_duration}—Å ({user_duration // 60}–º {user_duration % 60}—Å)"
       )
   except QueueFullError:
       await message.reply_text(
           f"‚ö†Ô∏è –û—á–µ—Ä–µ–¥—å –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —á–µ—Ä–µ–∑ {wait_time}—Å.\n"
           f"–í –æ—á–µ—Ä–µ–¥–∏: {queue_depth} –∑–∞–ø—Ä–æ—Å–æ–≤"
       )
   except TranscriptionError:
       await message.reply_text("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.")
   ```

   **Key Features:**
   - Russian language
   - Emoji for clarity
   - Actionable advice
   - Context information (duration, queue depth)

   **Benefits:**
   - Users understand what happened
   - Clear next steps
   - Better support experience

   **Added:** 2025-10-29
   **Location:** `src/bot/handlers.py`

   ---

   ### 14. Long Audio Handling Pattern

   **Problem:** OpenAI gpt-4o models have ~23min duration limit

   **Solution:** Three strategies (model switch, parallel chunking, sequential)

   **Implementation:**
   ```python
   if duration > settings.openai_gpt4o_max_duration:
       if settings.openai_change_model:
           # Strategy 1: Switch to whisper-1 (no limit)
           return await openai_provider.transcribe(audio, model="whisper-1")
       elif settings.openai_chunking:
           if settings.openai_parallel_chunks:
               # Strategy 2: Parallel chunking (fast)
               chunks = split_audio(audio, overlap=2s)
               results = await asyncio.gather(*[transcribe(c) for c in chunks])
           else:
               # Strategy 3: Sequential chunking (context preservation)
               chunks = split_audio(audio, overlap=2s)
               results = []
               for chunk in chunks:
                   context = results[-1] if results else None
                   result = await transcribe(chunk, context=context)
                   results.append(result)
           return " ".join(results)
   ```

   **Key Features:**
   - Auto model switch (default)
   - Parallel chunking (2-3x faster)
   - Sequential chunking (context preservation)
   - 2-second overlap prevents word cutting

   **Benefits:**
   - Unlimited duration audio
   - Flexible strategies
   - No quality loss

   **Added:** 2025-12-17
   **Location:** `src/transcription/providers/openai_provider.py`

   ---

   ### 15. Multi-File Support Pattern

   **Problem:** Users send audio in various formats (documents, videos)

   **Solution:** Universal file type handling

   **Implementation:**
   ```python
   # Voice handler (standard)
   application.add_handler(MessageHandler(filters.VOICE, voice_handler))

   # Audio handler (music files)
   application.add_handler(MessageHandler(filters.AUDIO, audio_handler))

   # Document handler (audio files as documents)
   application.add_handler(MessageHandler(filters.DOCUMENT, document_handler))
   # MIME type validation: audio/* only

   # Video handler (extract audio)
   application.add_handler(MessageHandler(filters.VIDEO, video_handler))
   # Extract audio track with ffmpeg
   ```

   **Key Features:**
   - 4 handlers for different file types
   - MIME type validation for documents
   - Audio extraction from video
   - Unified transcription pipeline

   **Benefits:**
   - Support any audio-containing file
   - Better UX (no "wrong format" errors)
   - Professional (handles videos)

   **Added:** 2025-12-25
   **Location:** `src/bot/handlers.py`, `src/transcription/audio_handler.py`

   ---

   ## Historical Patterns Reference

   The following patterns have been implemented throughout the project's evolution.
   For detailed documentation, see the archived version: `systemPatterns-2025-01-09-pre-update.md`

   ### Architecture & Design Patterns (7 patterns)
   1. **Producer-Consumer Pattern** - Queue-based request processing
   2. **Repository Pattern** - Database access abstraction
   3. **Service Layer Pattern** - Business logic encapsulation
   4. **Thread Pool Pattern** - CPU-bound work isolation
   5. **Middleware Pattern** - Cross-cutting concerns (quota, rate limit)
   6. **Configuration Object Pattern** - Pydantic settings
   7. **Strategy Pattern** - Provider routing (single/fallback/benchmark/hybrid/structure)
   8. **Factory Pattern** - Provider instantiation

   ### Queue & Progress Patterns (4 patterns)
   9. **Queue-Based Request Management** - Bounded FIFO queue
   10. **Progress Tracking** - Live progress bar updates
   11. **Staged Database Writes** - Lifecycle tracking (download ‚Üí transcribe ‚Üí refine)
   12. **Graceful Degradation** - Clear error messages

   ### Large File Support (1 pattern)
   13. **Hybrid Download Strategy** - Bot API ‚â§20MB + Client API >20MB

   ### Logging & Observability (8 patterns)
   14. **Version-Enriched Logging** - Add version/container_id to all logs
   15. **Size-Based Log Rotation** - 10MB files, 5 backups
   16. **Deployment Event Tracking** - JSONL deployment log
   17. **Structured JSON Logging** - Machine-readable logs
   18. **Optional Remote Syslog** - Centralized logging
   19. **Automatic Semantic Versioning** - v0.X.Y auto-increment
   20. **Separated Build and Deploy Workflow** - Two-stage CI/CD
   21. **Workflow Dispatch for Manual Deployments** - gh workflow trigger
   22. **Version-Tagged Docker Images** - latest + SHA tags
   23. **GitHub Releases for Changelog** - Auto-generated
   24. **Iterative Workflow Testing** - Multiple CI/CD iterations

   ### Queue Management (5 patterns)
   25. **Atomic Counter for Queue Position** - Accurate position tracking
   26. **Unique File Naming with UUID** - Prevent concurrent access conflicts
   27. **Callback Pattern for Queue Changes** - Observer pattern
   28. **Dual List Tracking** - Pending + processing lists
   29. **Dynamic Message Update** - Update all pending messages

   ### Hybrid & LLM Patterns (7 patterns)
   30. **Hybrid Transcription Strategy** - Duration-based routing
   31. **LLM Post-Processing** - Draft + refinement workflow
   32. **Staged UI Updates** - Show draft, then refine
   33. **Audio Preprocessing Pipeline** - Mono conversion, speed adjustment
   34. **Abstract LLM Provider** - Pluggable LLM providers
   35. **Graceful Degradation for LLM** - Always return draft
   36. **Feature Flags for Production Safety** - Disable by default

   ### Text Processing Patterns (3 patterns)
   37. **Relative LLM Instructions** - Qualitative vs quantitative prompts
   38. **Threshold-Based File Delivery** - Text vs PDF based on length
   39. **Circular Import Resolution** - Function-level imports

   ### Advanced Features (3 patterns)
   40. **Parent-Child Usage Tracking** - Retranscription history
   41. **Progress Bar with Dynamic Duration** - Method-specific estimation
   42. **Refinement Control via Context** - Skip refinement for retranscription
   43. **Provider-Aware Preprocessing** - Format optimization per provider
   44. **Audio Chunking with Overlap** - Long audio splitting
   45. **Universal File Type Support** - Voice + audio + documents + video
   46. **Audio Extraction from Video** - ffmpeg-based extraction
   47. **ffprobe-Based Duration Detection** - Document/video duration

   **Total:** 47 patterns implemented across 4 development months

   ---

   ## Development Workflow

   (Keep existing section from current file)

   ---

   ## Key Technical Decisions

   **Update:** Mark local-first decisions as "Historical"

   | Date | Decision | Status | Rationale |
   |------|----------|--------|-----------|
   | 2025-10-12 | Python 3.11+ | ‚úÖ Active | Best Whisper integration |
   | 2025-10-12 | python-telegram-bot | ‚úÖ Active | Most mature library |
   | 2025-10-12 | faster-whisper | ‚ö†Ô∏è Fallback | Kept for reliability |
   | 2025-10-12 | Hybrid Queue Architecture | ‚úÖ Active | Balanced scalability |
   | 2025-10-12 | SQLAlchemy + SQLite/PostgreSQL | ‚úÖ Active | Good migration path |
   | 2025-12-25 | **OpenAI API Primary** | ‚úÖ **Active** | **Best UX (speed + quality)** |
   | 2025-12-25 | **DeepSeek V3 for LLM** | ‚úÖ **Active** | **Intelligent text processing** |
   | 2025-12-25 | **Local model fallback** | ‚úÖ **Active** | **Reliability** |

   **For complete evolution story:** See [architecture-evolution.md](./architecture-evolution.md)

   ---

   ## Related Documentation

   - [Architecture Evolution](./architecture-evolution.md) - Complete pivot story
   - [Technical Context](./techContext.md) - Current technology stack
   - [Project Brief](./projectbrief.md) - Project goals and status
   - [Product Context](./productContext.md) - User experience and value props
   ```

3. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å:
   - –í—Å–µ 15 –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –æ–ø–∏—Å–∞–Ω—ã
   - –ö–∞–∂–¥—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω 50-80 lines (–Ω–µ –±–æ–ª—å—à–µ!)
   - Historical Reference —Å–æ–¥–µ—Ä–∂–∏—Ç –≤—Å–µ 47 –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
   - –°—Å—ã–ª–∫–∏ –Ω–∞ architecture-evolution.md –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ —É—Å–ø–µ—Ö–∞:**
- ‚úÖ –§–∞–π–ª —Å–æ–∫—Ä–∞—â—ë–Ω –¥–æ ~800 lines
- ‚úÖ 15 –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —Å –∫—Ä–∞—Ç–∫–∏–º–∏ –æ–ø–∏—Å–∞–Ω–∏—è–º–∏
- ‚úÖ Historical Reference —Å –ø–æ–ª–Ω—ã–º —Å–ø–∏—Å–∫–æ–º
- ‚úÖ –°—Å—ã–ª–∫–∞ –Ω–∞ –ø–æ–ª–Ω—É—é –≤–µ—Ä—Å–∏—é (backup)
- ‚úÖ –í—Å–µ —Å—Å—ã–ª–∫–∏ —Ä–∞–±–æ—Ç–∞—é—Ç

---

### Phase 6: –û–±–Ω–æ–≤–∏—Ç—å techContext.md (1 hour)

**–¶–µ–ª—å:** –û–±–Ω–æ–≤–∏—Ç—å —Å—Ç–µ–∫, —É–±—Ä–∞—Ç—å –¥—É–±–ª–∏, –¥–æ–±–∞–≤–∏—Ç—å cost structure

**–®–∞–≥–∏:**

1. –ü—Ä–æ—á–∏—Ç–∞—Ç—å —Ç–µ–∫—É—â–∏–π —Ñ–∞–π–ª:
   ```bash
   cat memory-bank/techContext.md
   ```

2. –û–±–Ω–æ–≤–∏—Ç—å "Technology Stack" —Å–µ–∫—Ü–∏—é:

   ```markdown
   ## Technology Stack

   **Current Status**: ‚úÖ API-First Architecture (2025-01-09)

   ### Core Technologies

   **Programming Language:** Python 3.11+

   **Telegram Bot Framework:** python-telegram-bot v22.5

   **Transcription Stack:**
   - **Primary:** OpenAI Whisper API
     - Models: whisper-1, gpt-4o-transcribe, gpt-4o-mini-transcribe
     - Performance: 5-10s for 1-minute audio
     - Cost: $0.006/minute
   - **Fallback:** faster-whisper v1.2.0 (local)
     - Model: medium/int8/beam1
     - Performance: 20-36s for 1-minute audio
     - Cost: Free

   **Text Processing:**
   - **Provider:** DeepSeek V3
     - Model: deepseek-chat
     - Features: Structuring, Magic, Summary
     - Performance: 2-5s processing
     - Cost: ~$0.0002 per 60s audio

   **Decision Rationale:** See [architecture-evolution.md](./architecture-evolution.md)

   ---

   ## Current Production Configuration

   **Transcription Provider:** OpenAI API (Primary)
   - Model: whisper-1 (default), gpt-4o-transcribe (optional)
   - Routing Strategy: structure (auto-structured transcription)
   - Fallback: faster-whisper local (medium/int8/beam1)

   **LLM Text Processing:** DeepSeek V3
   - Refinement enabled: Yes
   - Interactive features: Structure, Magic, Summary
   - Cost effective: 30x cheaper than OpenAI

   **Supported File Types:**
   - Voice messages (Telegram standard)
   - Audio files (MP3, OGG, WAV, M4A, FLAC, etc.)
   - Documents (.aac, .flac, .wma, .amr, etc.)
   - Videos (.mp4, .mkv, .avi, .mov with audio extraction)
   - Large files: Up to 2GB via Telethon

   ---

   ## Cost Structure

   **Per 60-second message:**
   - OpenAI transcription: $0.006
   - DeepSeek processing: ~$0.0002
   - **Total: ~$0.0062**

   **Monthly estimates:**
   - 100 messages/day √ó 30 days = 3000 messages
   - 3000 √ó 60s = 180,000 seconds = 50 hours
   - **Cost: 3000 √ó $0.0062 = $18.60/month**

   **Comparison:**
   - Local-only: $0 but poor UX (20-36s wait time)
   - API-first: $18.60/month for excellent UX (7-15s total)

   **Cost Optimization:**
   - Variant caching prevents duplicate LLM calls
   - Fallback to local when API unavailable
   - Batching requests (future improvement)

   ---

   ## Technical Decisions Log

   **Recent Updates (2025-01-09):**

   | Date | Area | Decision | Status | Rationale |
   |------|------|----------|--------|-----------|
   | 2025-12-25 | Transcription | **OpenAI API primary** | ‚úÖ Active | 4-7x faster, better quality |
   | 2025-12-25 | Text Processing | **DeepSeek V3** | ‚úÖ Active | Cost-effective LLM |
   | 2025-12-25 | Fallback | **Local model** | ‚úÖ Active | Reliability |
   | 2025-10-12 | Language | Python 3.11+ | ‚úÖ Active | Best Whisper integration |
   | 2025-10-12 | Bot Framework | python-telegram-bot | ‚úÖ Active | Most mature, async |

   **Historical Decisions (2025-10-12 ‚Üí 2025-12-25):**

   | Date | Area | Decision | Status | Rationale |
   |------|------|----------|--------|-----------|
   | 2025-10-12 | Transcription | faster-whisper only | ‚ö†Ô∏è Fallback | Initial vision (too slow) |
   | 2025-10-12 | Cost | Zero API costs | ‚ùå Retired | Not worth poor UX |
   | 2025-10-12 | Privacy | Local processing | ‚ùå Retired | API processing accepted |

   **For complete evolution:** See [architecture-evolution.md](./architecture-evolution.md)
   ```

3. –£–±—Ä–∞—Ç—å –¥—É–±–ª–∏—Ä—É—é—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Å systemPatterns:
   - –£–¥–∞–ª–∏—Ç—å —Å–µ–∫—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –≤ systemPatterns
   - –û—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ —É–Ω–∏–∫–∞–ª—å–Ω—É—é –¥–ª—è techContext –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
   - –î–æ–±–∞–≤–∏—Ç—å cross-references

4. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Å—ã–ª–∫–∏ –∏ consistency

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ —É—Å–ø–µ—Ö–∞:**
- ‚úÖ Technology Stack –æ—Ç—Ä–∞–∂–∞–µ—Ç OpenAI primary
- ‚úÖ Cost Structure –¥–æ–±–∞–≤–ª–µ–Ω
- ‚úÖ Technical Decisions –æ–±–Ω–æ–≤–ª–µ–Ω—ã
- ‚úÖ –î—É–±–ª–∏ —Å systemPatterns —É–±—Ä–∞–Ω—ã
- ‚úÖ –°—Å—ã–ª–∫–∏ –Ω–∞ architecture-evolution.md

---

### Phase 7: –†–µ–≤–∏–∑–∏—è –∏ —Ñ–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è (30 min)

**–¶–µ–ª—å:** –£–±–µ–¥–∏—Ç—å—Å—è —á—Ç–æ –≤—Å—ë konsistentno

**–ß–µ–∫–ª–∏—Å—Ç:**

1. **–ü—Ä–æ–≤–µ—Ä–∏—Ç—å cross-reference —Å—Å—ã–ª–∫–∏:**
   ```bash
   grep -r "architecture-evolution.md" memory-bank/*.md
   # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å –≤: projectbrief.md, productContext.md, systemPatterns.md, techContext.md
   ```

2. **–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π:**
   - projectbrief.md: API-first mentioned?
   - productContext.md: Value props updated?
   - systemPatterns.md: Current architecture reflects API-first?
   - techContext.md: Technology stack updated?

3. **–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å:**
   - –î–∞—Ç—ã pivot —Å–æ–≤–ø–∞–¥–∞—é—Ç —Å git history?
   - Initial vision —Å–æ—Ö—Ä–∞–Ω—ë–Ω –∫–∞–∫ "Archived"?
   - Evolution story –ø–æ–ª–Ω–∞—è?

4. **–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ:**
   - Markdown syntax –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π?
   - –ó–∞–≥–æ–ª–æ–≤–∫–∏ consistent?
   - –°—Å—ã–ª–∫–∏ —Ä–∞–±–æ—Ç–∞—é—Ç?

5. **–°–æ–∑–¥–∞—Ç—å summary –∏–∑–º–µ–Ω–µ–Ω–∏–π:**
   ```bash
   cat > memory-bank/UPDATE_SUMMARY_2025-01-09.md << 'EOF'
   # Memory Bank Update Summary (2025-01-09)

   ## Purpose
   Reflect API-first architecture pivot in documentation

   ## Files Changed

   ### New Files
   - architecture-evolution.md (200 lines) - Complete pivot story

   ### Updated Files
   - projectbrief.md: 188 ‚Üí 120 lines (-36%)
     - Updated Project Vision (API-first)
     - Added Architecture Evolution section
     - Preserved Initial Vision as archived

   - productContext.md: 244 ‚Üí 180 lines (-26%)
     - Rewrote Value Proposition (current benefits)
     - Added Current User Experience section
     - Updated Product Principles

   - systemPatterns.md: 2262 ‚Üí 800 lines (-65%)
     - 15 essential patterns (short descriptions)
     - Historical patterns reference (47 patterns total)
     - Current architecture overview

   - techContext.md: 835 ‚Üí 500 lines (-40%)
     - Updated Technology Stack (OpenAI primary)
     - Added Cost Structure section
     - Updated Technical Decisions Log
     - Removed duplicates with systemPatterns

   ## Total Impact
   - **Before:** 4506 lines across 5 files
   - **After:** ~1800 lines across 6 files
   - **Reduction:** -60% (2706 lines removed)
   - **New file:** +200 lines (architecture-evolution.md)

   ## Key Changes
   1. API-first architecture reflected throughout
   2. Historical context preserved (not deleted)
   3. Significantly improved navigation (60% less text)
   4. Accurate cost information added
   5. Cross-references between all documents

   ## Backup
   - Branch: docs/update-memory-bank-2025-01-09
   - Backup dir: memory-bank.backup.2025-01-09
   - systemPatterns: systemPatterns-2025-01-09-pre-update.md

   ## Validation
   - ‚úÖ No contradictions between files
   - ‚úÖ All cross-references working
   - ‚úÖ Historical accuracy verified
   - ‚úÖ Markdown formatting correct

   ---

   Updated: 2025-01-09
   Approved: User (Balanced Option 2)
   Status: Ready for implementation
   EOF
   ```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ —É—Å–ø–µ—Ö–∞:**
- ‚úÖ –í—Å–µ —á–µ–∫–ª–∏—Å—Ç –ø—É–Ω–∫—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã
- ‚úÖ –ù–µ—Ç –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π
- ‚úÖ –í—Å–µ —Å—Å—ã–ª–∫–∏ —Ä–∞–±–æ—Ç–∞—é—Ç
- ‚úÖ Summary —Å–æ–∑–¥–∞–Ω
- ‚úÖ –ì–æ—Ç–æ–≤–æ –∫ commit

---

## Following Steps (–¥–ª—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏)

–ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö —Ñ–∞–∑:

1. **–°–æ–∑–¥–∞—Ç—å commit:**
   ```bash
   git add memory-bank/
   git commit -m "docs: update Memory Bank for API-first architecture

   - Add architecture-evolution.md documenting pivot from local to API-first
   - Update projectbrief.md: reflect current API-first stack
   - Update productContext.md: new value propositions, current UX
   - Condense systemPatterns.md: 2262 ‚Üí 800 lines (-65%)
   - Update techContext.md: OpenAI primary, cost structure added

   Total impact: -60% documentation size (4506 ‚Üí 1800 lines)
   Historical context: Preserved (not deleted)
   Cross-references: All documents link to architecture-evolution.md

   Breakdown of changes:
   - New file: architecture-evolution.md (200 lines)
   - projectbrief.md: 188 ‚Üí 120 lines (-36%)
   - productContext.md: 244 ‚Üí 180 lines (-26%)
   - systemPatterns.md: 2262 ‚Üí 800 lines (-65%)
   - techContext.md: 835 ‚Üí 500 lines (-40%)

   Closes: Memory Bank documentation update for API-first pivot
   "
   ```

2. **Push –∏ —Å–æ–∑–¥–∞—Ç—å PR:**
   ```bash
   git push origin docs/update-memory-bank-2025-01-09
   gh pr create --title "docs: update Memory Bank for API-first architecture" \
                --body "See memory-bank/UPDATE_SUMMARY_2025-01-09.md for details"
   ```

3. **Merge –ø–æ—Å–ª–µ —Ä–µ–≤—å—é:**
   ```bash
   gh pr merge --merge --delete-branch
   ```

---

## Success Criteria

–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—á–∏—Ç–∞–µ—Ç—Å—è —É—Å–ø–µ—à–Ω—ã–º –µ—Å–ª–∏:

1. ‚úÖ –í—Å–µ 6 —Ñ–∞–∑ –∑–∞–≤–µ—Ä—à–µ–Ω—ã
2. ‚úÖ Documentation reflects API-first architecture
3. ‚úÖ Historical context preserved (nothing deleted, only archived)
4. ‚úÖ System patterns condensed by 60%+
5. ‚úÖ No contradictions between documents
6. ‚úÖ Cross-references working
7. ‚úÖ Summary created and committed
8. ‚úÖ PR merged to main

---

## Risk Mitigation

**Risks:**
- Loss of information during condensation
- Inaccurate historical documentation
- Broken cross-references
- Contradictions between files

**Mitigation:**
- Full backup before changes
- Branch for changes (not direct to main)
- Preserve all historical info (mark as "Archived")
- Cross-reference validation
- Final review before commit

**Rollback Plan:**
If issues found after merge:
1. Restore from backup: `cp -r memory-bank.backup.2025-01-09/* memory-bank/`
2. Revert commit: `git revert <commit-hash>`
3. Fix issues and repeat process

---

**End of Plan**

---

## –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ

‚úÖ –ü–ª–∞–Ω —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤: `memory-bank/plans/2025-01-09-memory-bank-api-first-pivot.md`

**–°—Ç–∞—Ç—É—Å:** –ì–æ—Ç–æ–≤ –∫ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º —á–∞—Ç–µ
**–û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏:** 4-6 —á–∞—Å–æ–≤
**–°–ª–æ–∂–Ω–æ—Å—Ç—å:** Medium

**–°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:**
1. –û—Ç–∫—Ä—ã—Ç—å –Ω–æ–≤—ã–π —á–∞—Ç —Å Claude Code
2. –°–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å –ø–ª–∞–Ω –∏–∑ —Ñ–∞–π–ª–∞
3. –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø–æ —Ñ–∞–∑–∞–º
4. –°–æ–æ–±—â–∏—Ç—å –æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
