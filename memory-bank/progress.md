# Progress Overview: Telegram Voice2Text Bot

## Timeline & Phase Status
- Project kickoff: 2025-10-12
- Phase 1‚Äì4: ‚úÖ Complete (project setup ‚Üí CI/CD)
- **Phase 4.5**: ‚úÖ Complete (2025-10-24) - Model finalization & provider cleanup
- **Phase 5**: ‚úÖ Complete (2025-10-27) - VPS deployment & production validation
- **Phase 5.1**: ‚úÖ Complete (2025-10-28) - CI/CD optimization & documentation reorganization
- **Phase 5.2**: ‚úÖ Complete (2025-10-29) - Critical production bug fix
- **Phase 6**: ‚úÖ Complete (2025-10-29) - Queue-based concurrency control
- **Phase 6.5**: ‚úÖ Complete (2025-10-29) - Database migration system
- **Phase 6.6**: ‚úÖ Complete (2025-10-29) - Production limit optimization
- **Phase 6.7**: ‚úÖ Complete (2025-10-30) - Long transcription message splitting
- **Phase 7**: ‚úÖ Complete (2025-11-03) - Centralized logging & automatic versioning
- **Phase 7.1**: ‚úÖ Complete (2025-11-03) - Workflow fixes and production deployment
- **Phase 7.2**: ‚úÖ Complete (2025-11-04) - Fully automatic deployment pipeline
- **Phase 7.3**: ‚úÖ Complete (2025-11-19) - Queue position & file naming bug fixes
- **Phase 7.4**: ‚úÖ Complete (2025-11-19) - Dynamic queue notifications with accurate time calculation
- **Phase 8**: ‚úÖ Complete (2025-11-20) - Hybrid transcription acceleration
- **Phase 8.1**: ‚úÖ Complete (2025-11-24) - DEBUG logging enhancement
- **Phase 8.2**: ‚úÖ Complete (2025-11-24) - LLM debug mode
- **Phase 8.3**: ‚úÖ Complete (2025-11-24) - LLM performance tracking
- **Phase 9**: ‚úÖ Complete (2025-11-30) - Large file support (Telethon Client API)
- **Phase 10.1**: ‚úÖ Complete (2025-12-03) - Interactive transcription Phase 1 (infrastructure)
- **Phase 10.2**: ‚è≥ NEXT - Interactive transcription Phase 2 (Structured Mode)
- **Production Status**: ‚úÖ OPERATIONAL - All systems deployed and stable
- **Current Version**: v0.0.3+ (hybrid transcription + LLM tracking + interactive Phase 1)
- Current focus (2025-12-03): Phase 10.1 complete, ready for Phase 10.2 (Structured Mode)

## Delivered Milestones

### Core Implementation ‚úÖ
- Core bot with async processing pipeline, quota system, and FasterWhisper integration
- Comprehensive test suite (45 unit tests + integration tests)
- CI enforces mypy, ruff, black, pytest - all passing
- Docker image + docker-compose + Makefile for local/prod deployment
- CI/CD workflows: auto-build, push, deploy to VPS

### Provider Architecture ‚úÖ
- Flexible transcription provider system (ENV-driven selection)
- Benchmark mode for empirical testing
- Fallback strategy support
- **2 Production Providers**: faster-whisper (default), OpenAI API (optional)

### Model Selection ‚úÖ (NEW - 2025-10-24)
- **Comprehensive benchmarking**: 30+ configurations tested across 3 audio samples
- **Production default finalized**: `faster-whisper / medium / int8 / beam1`
  - RTF ~0.3x (3x faster than audio)
  - ~2GB RAM peak (actual production testing, not 3.5GB as initially measured)
  - Excellent quality for Russian language
- **Provider cleanup completed**: Removed openai-whisper (original Whisper)
  - Docker image size reduced ~2-3GB
  - Dependencies: 75 ‚Üí 50 packages
  - Removed torch, openai-whisper from project
- **Documentation updated**: .env.example, README, docs/, Memory Bank
- **Tests added**: Provider-specific unit tests (13 tests)
- **Quality checks**: All passing (mypy, ruff, black, pytest)

üìÑ Decision rationale: `memory-bank/benchmarks/final-decision.md`

## Outstanding Work

### Phase 5: VPS Deployment ‚úÖ COMPLETE (2025-10-27)
1. **VPS Purchase** ‚úÖ
   - VPS purchased: 1GB RAM, 1 vCPU
   - Provider: Russian VPS (~$3-5/month)
   - SSH configured, Docker installed

2. **VPS Configuration** ‚úÖ
   - SSH key-based authentication operational
   - Docker and docker-compose installed
   - Project directory: `/opt/telegram-voice2text-bot`
   - GitHub secrets configured (VPS_HOST, VPS_USER, VPS_SSH_KEY)

3. **CD Pipeline Activation** ‚úÖ
   - Automated deployment via GitHub Actions working
   - Two-stage workflow: Build ‚Üí Deploy
   - Health checks and zero-downtime updates
   - Image cleanup automation

4. **Production Validation** ‚úÖ
   - Bot operational and stable on 1GB VPS
   - **Critical Issues Resolved**:
     - Database directory creation (#11)
     - DNS configuration for HuggingFace (#12)
     - OOM prevention via 1GB swap
   - **Baseline Metrics Captured**:
     - Memory: 516 MB RAM + 755 MB swap = 1.27 GB total
     - Performance: RTF 3.04x (9s audio ‚Üí 27s processing)
     - Model: medium/int8 loaded successfully
   - **Known Issue**: Performance 10x slower than local (swap bottleneck)

### Phase 5.1: CI/CD Optimization & Documentation Reorganization ‚úÖ COMPLETE (2025-10-28)

#### CI/CD Path Filtering (#15)
**Goal**: Optimize CI/CD to skip operations for documentation-only changes

**Problem**: `paths-ignore` prevented workflows from running, causing missing required status checks and blocking PR merges for docs-only changes.

**Solution**:
- Replaced `paths-ignore` with `tj-actions/changed-files@v45`
- Added conditional execution to all expensive steps
- Workflows always run (creates status checks) but skip heavy operations when only docs change

**Files Ignored**: `memory-bank/**`, `*.md`, `docs/**`, `.claude/**`, `CLAUDE.md`

**Impact**:
- ‚úÖ Required status checks always created
- ‚úÖ CI minutes saved on docs-only changes
- ‚úÖ No manual status check overrides needed
- ‚úÖ Maintains quality gates for code changes

#### Documentation Reorganization (#18)
**Goal**: Restructure documentation for better navigation and maintainability

**Changes**:
- Created hierarchical `docs/` structure (getting-started, development, deployment, research)
- Added `docs/README.md` as central navigation index
- Moved 9 documentation files to organized locations
- Created 7 new comprehensive documentation files
- Simplified requirements: merged `requirements-docker.txt` into single `requirements.txt`
- Reduced README.md from 461 to 229 lines

**Impact**:
- ‚úÖ Clear documentation hierarchy
- ‚úÖ Easier navigation for contributors
- ‚úÖ Reduced root directory clutter
- ‚úÖ Scalable structure for future growth
- ‚úÖ Single source of truth for dependencies

### Phase 5.2: Critical Production Bug Fix ‚úÖ COMPLETE (2025-10-29)

#### faster-whisper Missing from Docker Image (#19)
**Problem**: Production bot crashing on startup with `ModuleNotFoundError: No module named 'faster_whisper'`

**Root Cause**:
- `faster-whisper` marked as optional dependency in `pyproject.toml`
- CI/CD workflow was exporting requirements WITHOUT `--extras "faster-whisper"` flag
- Docker images built without faster-whisper package
- Container entered infinite restart loop

**Solution**:
- Added `--extras "faster-whisper"` to poetry export command in build workflow
- Single-line fix in `.github/workflows/build-and-deploy.yml`

**Verification**:
- Docker image size: 614 MB ‚Üí 1.03 GB (correct with dependencies)
- Container status: `Restarting` ‚Üí `healthy`
- FasterWhisper model loads successfully
- Bot fully operational and responding to messages

**Impact**:
- ‚úÖ Production bot fully functional
- ‚úÖ All transcription features working
- ‚úÖ Container stability achieved
- ‚úÖ Alignment between local and CI/CD environments

**Key Lesson**: When using Poetry optional dependencies, CI/CD must explicitly include extras in export. Local development scripts were correct, but CI/CD workflow had diverged.

### Phase 6: Queue-Based Concurrency Control ‚úÖ COMPLETE (2025-10-29)

#### Problem Statement
**Incident**: Bot crashed when processing 4-minute audio file during production testing with colleagues. One user's long file caused CPU spike, bot crashed without returning results, blocking other users.

**Root Causes**:
- No concurrency controls ‚Üí multiple simultaneous transcriptions exhausted 2 CPU cores
- No duration limits ‚Üí 4-minute file exceeded capacity
- No user feedback during processing ‚Üí poor UX
- No request queue ‚Üí FIFO processing not guaranteed
- No analytics on request lifecycle

**Decision**: Implement queue-based architecture with duration limits and progress feedback (Option 3: Hybrid approach without CPU upgrade initially)

#### Implementation (6 Phases, 2 commits)

**Commits**:
- `8eea54f` - Phases 1-4: Infrastructure (queue, progress tracker, DB migration, repositories)
- `f6e1d5c` - Phase 6: Handler integration (complete refactor)

**Phase 1: Database Migration**
- Added `updated_at` column for lifecycle tracking
- Replaced `transcription_text` with `transcription_length` for privacy
- Made fields nullable for staged writes (3-stage lifecycle)
- Migration: `alembic/versions/a9f3b2c8d1e4_*.py`

**Phase 2: Repository Updates**
- `UsageRepository.create()` - Stage 1: on file download
- `UsageRepository.update()` - Stage 2/3: after download/transcription
- Privacy-friendly analytics without storing text

**Phase 3: Queue Manager** (`src/services/queue_manager.py`)
- FIFO queue with configurable size (default: 50 requests)
- Semaphore-based concurrency control (max_concurrent=1)
- Background worker with graceful error handling
- Request/response tracking with timeout support

**Phase 4: Progress Tracker** (`src/services/progress_tracker.py`)
- Live updates every 5 seconds via Telegram message edits
- Visual progress bar: `üîÑ [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë] 40%`
- RTF-based time estimation
- Telegram rate limit handling (RetryAfter, TimedOut)

**Phase 5: Configuration**
- `max_voice_duration_seconds`: 300 ‚Üí **120** (2 minutes)
- `max_queue_size`: 100 ‚Üí 50 ‚Üí **10** (final, optimized Phase 6.6)
- `max_concurrent_workers`: 3 ‚Üí **1** (sequential processing)
- Progress tracking: interval=5s, rtf=0.3

**Phase 6: Handler Integration** (`src/bot/handlers.py`)
- Duration validation: reject files > 120 seconds
- Queue capacity check: reject when queue full
- Staged database writes (3 stages)
- Queue position feedback to users
- Estimated wait time display
- Complete refactor of voice_message_handler and audio_message_handler

**New User Experience**:
```
1. üì• –ó–∞–≥—Ä—É–∂–∞—é —Ñ–∞–π–ª...
2. üìã –í –æ—á–µ—Ä–µ–¥–∏: –ø–æ–∑–∏—Ü–∏—è 3 (if queued)
   ‚è±Ô∏è –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è: ~60—Å
3. üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë] 40%
   ‚è±Ô∏è –ü—Ä–æ—à–ª–æ: 8—Å | –û—Å—Ç–∞–ª–æ—Å—å: ~12—Å
4. ‚úÖ –ì–æ—Ç–æ–≤–æ!
   [transcription text]
```

**Files Created**:
- `src/services/__init__.py`
- `src/services/queue_manager.py` (296 lines)
- `src/services/progress_tracker.py` (201 lines)
- `alembic/versions/a9f3b2c8d1e4_*.py`
- `memory-bank/plans/2025-10-29-queue-based-concurrency-plan.md`

**Files Modified**:
- `src/storage/models.py` - Usage model with nullable fields
- `src/storage/repositories.py` - create/update methods
- `src/bot/handlers.py` - Complete refactor (210 insertions, 101 deletions)
- `src/config.py` - Queue and progress settings
- `src/main.py` - QueueManager initialization

**Testing & Deployment Status**:
- ‚úÖ Code compilation successful
- ‚úÖ Syntax validation passed
- ‚úÖ Merged to main branch (PR #21)
- ‚úÖ Production deployment via CI/CD
- ‚úÖ Database migration applied successfully
- ‚úÖ Bot operational with queue system

**Production Configuration** (deployed):
```bash
# Production .env values
MAX_VOICE_DURATION_SECONDS=120
MAX_QUEUE_SIZE=10  # Optimized from 50 ‚Üí 10 in Phase 6.6
MAX_CONCURRENT_WORKERS=1
PROGRESS_UPDATE_INTERVAL=5
PROGRESS_RTF=0.3
```

**Impact**:
- ‚úÖ Prevents crashes under concurrent load
- ‚úÖ Duration limit protects resources
- ‚úÖ Live progress improves UX dramatically
- ‚úÖ Queue ensures FIFO fairness
- ‚úÖ Privacy-friendly analytics
- ‚úÖ Sequential processing (max_concurrent=1) prevents resource exhaustion on 1 GB RAM / 1 vCPU VPS

**Key Pattern Established**: Queue-based request management with live progress feedback is essential for resource-constrained deployments. Sequential processing prevents crashes while maintaining acceptable UX through visual feedback.

**Branch**: `main` (merged)
**Documentation**: `memory-bank/plans/2025-10-29-queue-based-concurrency-plan.md`

### Phase 6.5: Database Migration System ‚úÖ COMPLETE (2025-10-29)
**Achievement**: Seamless database migration deployment system

**Problem Solved**: Manual migrations are error-prone and block deployments. Need automated, tested, reversible migrations in CI/CD.

**Implementation** (commit 5c4cc5a, PR #21):

**1. Automated Migration Testing in CI**
- New `test-migrations` job in `.github/workflows/build-and-deploy.yml`
- Tests fresh database migration (from scratch)
- Tests upgrade/downgrade cycle for reversibility
- Tests application startup after migration
- Runs in parallel with build job

**2. Production Migration Job**
- New `migrate` job executes between build and deploy stages
- SSH to VPS and applies migrations before code update
- Automatic rollback on migration failure (`alembic downgrade -1`)
- Stops deployment if migration fails
- Shows migration status before/after

**3. Health Check System**
- New `src/health_check.py` script (190 lines)
- Checks database connectivity
- Verifies schema version matches HEAD
- Used by docker-compose health check
- Prevents unhealthy containers from running

**4. Updated init_db() Function** (`src/storage/database.py`)
- Removed `create_all()` (doesn't work with migrations)
- Added migration version verification
- Logs current schema revision on startup
- Raises error if schema out of date
- Prevents bot from starting with wrong schema

**5. Comprehensive Documentation**
- `docs/development/database-migrations.md` (363 lines) - Developer guide
  - Creating and testing migrations
  - Best practices for SQLite compatibility
  - Troubleshooting common issues
- `docs/deployment/migration-runbook.md` (421 lines) - Operations runbook
  - Manual migration procedures
  - Rollback procedures
  - Emergency procedures
  - Monitoring and verification
- `docs/deployment/vps-setup.md` - Added Phase 6.5 section (161 lines total)

**Deployment Flow**:
```
push to main
  ‚Üì
test-migrations (parallel with build)
  ‚Üì
build (Docker image)
  ‚Üì
migrate (apply migrations on VPS via SSH)
  ‚Üì (if success)
deploy (restart bot with new code)
  ‚Üì
health check (verify schema version)
```

**Rollback Strategy**:
- Automatic: `alembic downgrade -1` if migration fails
- Deployment stops, bot continues on old version
- Manual: Follow migration-runbook.md procedures

**Testing**:
- ‚úÖ Health check script tested locally
- ‚úÖ Migration test job validated in CI
- ‚úÖ Production migration applied successfully (a9f3b2c8d1e4)
- ‚úÖ Rollback logic verified

**Files Created**:
- `src/health_check.py`
- `docs/development/database-migrations.md`
- `docs/deployment/migration-runbook.md`

**Files Modified**:
- `.github/workflows/build-and-deploy.yml` - Added test-migrations and migrate jobs
- `src/storage/database.py` - Updated init_db() with version verification
- `docker-compose.prod.yml` - Use health_check.py
- `docs/deployment/vps-setup.md` - Added Phase 6.5

**Impact**:
- ‚úÖ Zero-downtime deployments with schema changes
- ‚úÖ Automatic migration testing prevents production failures
- ‚úÖ Rollback capability for failed migrations
- ‚úÖ Health checks ensure schema consistency
- ‚úÖ Developer and operator documentation complete

**Key Pattern Established**: Automated migration testing and deployment with rollback support is essential for production database changes. Health checks prevent schema version mismatches.

**Status**: ‚úÖ Deployed and operational since 2025-10-29

### Phase 6.6: Production Limit Optimization ‚úÖ COMPLETE (2025-10-29)
**Achievement**: Fine-tuned queue limits for 1GB VPS constraints

**Problem**: Initial queue size of 50 may be too large for 1GB RAM VPS. Need to optimize for conservative resource usage.

**Implementation** (PR #25):

**Changes**:
- `MAX_QUEUE_SIZE`: 100 (initial) ‚Üí 50 (Phase 6) ‚Üí **10** (final)
- `MAX_CONCURRENT_WORKERS`: **1** (maintained, sequential processing)
- `MAX_VOICE_DURATION_SECONDS`: **120** (maintained, 2 minutes)

**Rationale**:
- Conservative approach for 1GB RAM + 1 vCPU VPS
- Prevents memory exhaustion with sequential processing
- Queue size of 10 provides adequate buffering (10 √ó 2min = 20min max wait)
- Reduces memory overhead from queued requests
- Maintains acceptable UX through progress feedback

**Deployment**:
- Updated `.github/workflows/build-and-deploy.yml` environment variables
- Automated via CI/CD pipeline
- No code changes, configuration only

**Testing**:
- ‚è≥ Real-world usage monitoring ongoing
- ‚è≥ Queue depth tracking via application logs

**Files Modified**:
- `.github/workflows/build-and-deploy.yml` (lines 355-357)

**Impact**:
- ‚úÖ Reduced memory footprint for queued requests
- ‚úÖ More conservative resource management
- ‚úÖ Still provides adequate buffering for concurrent users
- ‚úÖ Easy to adjust based on monitoring data

**Next**: Monitor queue depth metrics to validate if 10 is sufficient or if adjustment needed

**Status**: ‚úÖ Deployed and operational since 2025-10-29

### Phase 6.7: Long Transcription Message Splitting ‚úÖ COMPLETE (2025-10-30)
**Achievement**: Automatic splitting of long transcriptions to handle Telegram's 4096 character limit

**Problem**: Bot crashed when transcribing long voice messages (30+ minutes). Telegram limits messages to 4096 characters, but transcriptions could be 23,000+ characters, causing `telegram.error.BadRequest: Message is too long`.

**Implementation** (commit 0906229):

**1. Smart Text Splitting Function** (`split_text()` in `src/bot/handlers.py`)
- Reserves 50 characters for message headers (e.g., "üìù –ß–∞—Å—Ç—å 1/6")
- Effective max: 4046 characters per chunk
- Smart boundary detection:
  - Prefers paragraph boundaries (double newline)
  - Falls back to single newline
  - Then sentence boundaries (. ! ?)
  - Finally word boundaries
  - Force splits only as last resort
- Ensures no chunk exceeds 4096 chars including header

**2. Updated TranscriptionRequest Model**
- Added `user_message: Message` field to `TranscriptionRequest` dataclass
- Enables proper reply threading for multi-message transcriptions
- Updated in `src/services/queue_manager.py`

**3. Dynamic Message Handling** (`src/bot/handlers.py`)
- Short transcriptions (‚â§4096 chars): Edit status message (unchanged behavior)
- Long transcriptions (>4096 chars):
  - Delete status message
  - Send multiple reply messages with headers:
    ```
    üìù –ß–∞—Å—Ç—å 1/6
    [first chunk of text]

    üìù –ß–∞—Å—Ç—å 2/6
    [second chunk of text]
    ...
    ```
  - Small delay (0.1s) between messages to avoid rate limits

**Testing**:
- Created comprehensive test suite (`test_split.py`, removed after testing)
- Tested with 5 scenarios: short text, long text, paragraphs, realistic 23K chars, edge cases
- Verified all chunks fit within 4096 limit including headers

**Files Modified**:
- `src/bot/handlers.py` - Added split_text() function, updated result sending logic
- `src/services/queue_manager.py` - Added user_message field to TranscriptionRequest

**Production Testing**:
- ‚úÖ Tested with 31-minute voice message (1885 seconds)
- ‚úÖ Transcription: 23,676 characters
- ‚úÖ Split into multiple messages successfully
- ‚úÖ Processing time: 559 seconds (RTF 0.30x)
- ‚úÖ No errors, all messages delivered

**Impact**:
- ‚úÖ Bot can now handle voice messages of any length
- ‚úÖ Maintains clean UX with numbered message parts
- ‚úÖ No data loss from truncation
- ‚úÖ Graceful handling of edge cases

**Key Pattern Established**: For services with message length limits, implement smart text splitting with boundary detection and reserve space for metadata headers.

**Status**: ‚úÖ Tested and ready for production deployment

### Phase 7: Centralized Logging & Automatic Versioning ‚úÖ COMPLETE (2025-11-03)
**Achievement**: Production observability and user-friendly versioning system

**Motivation**: Two critical production needs emerged:
1. **Logs lost during deployments**: Container rebuilds deleted all logs, no way to debug past issues
2. **Git SHA not user-friendly**: Version "09f9af8" meaningless for tracking releases

**Implementation** (2 major systems):

#### System 1: Centralized Logging

**Infrastructure** (`src/utils/logging_config.py`, 233 lines):
- JSON-formatted logs with structured context
- Version enrichment: every log entry includes version and container_id
- Size-based rotation (NOT time-based per user request)
- Optional remote syslog support (Papertrail, etc.)
- Deployment event tracking (startup, ready, shutdown)

**Log Files**:
1. `app.log`: All INFO+ logs, 10MB per file, 5 backups (~60MB max)
2. `errors.log`: ERROR/CRITICAL only, 5MB per file, 5 backups (~30MB max)
3. `deployments.jsonl`: Never rotated, one event per line (~365KB/year)

**Configuration**:
```python
APP_VERSION=v0.1.0  # Set by CI/CD from git tag
LOG_DIR=/app/logs
LOG_LEVEL=INFO
# Optional remote syslog
SYSLOG_ENABLED=false
SYSLOG_HOST=logs.papertrailapp.com
SYSLOG_PORT=514
```

**Integration**:
- `src/main.py`: Initialize logging, log deployment events
- Volume mount: `./logs:/app/logs` (persists across container rebuilds)
- All log entries include version for filtering

**Benefits**:
- ‚úÖ Logs persist across deployments
- ‚úÖ Every log entry knows its version
- ‚úÖ Deployment lifecycle tracked
- ‚úÖ Size-based rotation: logs kept longer when generation is low
- ‚úÖ Structured JSON for easy parsing (jq, log aggregation)
- ‚úÖ Predictable disk usage: ~90MB max

#### System 2: Automatic Semantic Versioning

**Architecture**: Separate workflows for build and deploy

**Build & Tag Workflow** (`.github/workflows/build-and-tag.yml`):
- Trigger: Push to main
- Test migrations in CI
- Calculate next patch version automatically: v0.1.0 ‚Üí v0.1.1
- Build Docker image
- Push with multiple tags (version, sha, latest)
- Create annotated git tag
- Push tag to GitHub
- Create GitHub Release

**Deploy Workflow** (`.github/workflows/deploy.yml`):
- Trigger: Tag creation (v*.*.*)
- Run database migrations on VPS first
- Deploy specific version to production
- Health checks and validation
- Automatic rollback on migration failure

**Version Format**:
- **Automatic**: v0.1.0 ‚Üí v0.1.1 ‚Üí v0.1.2 (every merge to main)
- **Manual minor/major**:
  ```bash
  git tag -a v0.2.0 -m "Release v0.2.0: Add quota system"
  git push origin v0.2.0  # Triggers deploy
  ```

**Workflow Separation**:
```
PR merged to main
  ‚Üì
Build & Tag Workflow
  - Tests
  - Build image
  - Create v0.1.1 tag
  ‚Üì (tag pushed)
Deploy Workflow
  - Migrations
  - Deploy v0.1.1
  - Health checks
```

**Benefits**:
- ‚úÖ User-friendly versions: v0.1.1 instead of 09f9af8
- ‚úÖ Automatic patch increments
- ‚úÖ Separation of build and deploy
- ‚úÖ GitHub Releases for changelog
- ‚úÖ Version-tagged Docker images for rollback
- ‚úÖ Full version history

**Files Created**:
- `src/utils/logging_config.py` (233 lines)
- `docs/development/logging.md` (347 lines)
- `.github/workflows/build-and-tag.yml` (153 lines)
- `.github/workflows/deploy.yml` (279 lines)
- `memory-bank/plans/2025-11-03-centralized-logging.md`
- Git tag: `v0.1.0` (initial version)

**Files Modified**:
- `pyproject.toml` - Added python-json-logger ^4.0.0
- `src/main.py` - Logging initialization, deployment events
- `docker-compose.prod.yml` - LOG_DIR, SYSLOG_* env vars
- `docs/README.md` - Added logging link
- `docs/development/git-workflow.md` - Added versioning section (82 lines added)
- `.github/workflows/build-and-deploy.yml` ‚Üí `build-and-deploy.yml.old` (disabled)

**Testing**:
- ‚úÖ Logging code syntax validated
- ‚úÖ Version calculation tested
- ‚úÖ Initial v0.1.0 tag created
- ‚úÖ Documentation complete
- ‚è≥ Full system test on next merge to main

**Deployment Status**:
- ‚úÖ Implementation complete
- ‚úÖ Initial version tag created
- ‚è≥ Awaiting next merge to test automatic workflow
- ‚è≥ Logs will start collecting on deployment

**Key Patterns Established**:
1. **Size-based log rotation**: "–ï—Å–ª–∏ –º–∞–ª–æ –ª–æ–≥–æ–≤, —Ç–æ –ø—É—Å—Ç—å —Ö—Ä–∞–Ω—è—Ç—Å—è –¥–æ–ª–≥–æ" - logs persist longer when generation is low, predictable disk usage
2. **Version in every log**: Essential for post-mortem analysis and troubleshooting specific deployments
3. **Workflow separation**: Build ‚Üí Tag ‚Üí Deploy enables testing between stages
4. **Automatic versioning**: Remove manual version management overhead for patch releases

**Documentation**:
- `docs/development/logging.md`: Complete logging guide (formats, rotation, searching, remote syslog, troubleshooting)
- `docs/development/git-workflow.md`: Comprehensive versioning guide (automatic/manual versions, rollback, troubleshooting)

**Next**: Monitor production usage with new logging and versioning systems

### Phase 7.1: Workflow Fixes & Production Deployment ‚úÖ COMPLETE (2025-11-03)
**Achievement**: Fixed workflow issues and successfully deployed logging and versioning to production

**Problems Encountered**:
1. **GitHub Actions Permissions** (PR #30)
   - Build and Tag workflow couldn't push tags
   - Error: `Permission to konstantinbalakin/telegram-voice2text-bot.git denied to github-actions[bot]`
   - Fix: Added explicit permissions:
     ```yaml
     permissions:
       contents: write
       packages: write
     ```

2. **Deploy Workflow Not Triggering** (PR #31)
   - Deploy workflow didn't trigger when Build and Tag created a tag
   - Root cause: GitHub Actions doesn't trigger workflows on events from other workflows (security)
   - Solution: Added multiple triggers:
     - `workflow_run` for automatic deploy after build (has limitations)
     - `workflow_dispatch` for manual deployments (works perfectly)
     - `push: tags` for manually created tags

3. **workflow_run Limitation Discovered**:
   - `workflow_run` trigger receives `refs/heads/main` instead of tag reference
   - Causes Docker image name issues: `konstantinbalakin/telegram-voice2text-bot:refs/heads/main`
   - Not suitable for automatic deployment
   - Documented as known issue

**Solution Implemented**:
- Use `workflow_dispatch` for manual deployments:
  ```bash
  gh workflow run deploy.yml -f version=v0.0.1
  ```
- Works reliably and allows deployment of any version

**Production Deployment Results**:
- ‚úÖ Version v0.0.1 deployed successfully
- ‚úÖ Logging system active:
  - `app.log` (9.3KB) - All logs with JSON format and version enrichment
  - `errors.log` (0KB) - No errors
  - `deployments.jsonl` (586B) - Deployment events with full config
- ‚úÖ Every log entry includes:
  - `"version": "v0.0.1"`
  - `"container_id": "3f33660445f8"`
  - Structured JSON with timestamp, level, logger, message
- ‚úÖ Deployment events captured:
  ```json
  {"timestamp": "...", "event": "startup", "version": "v0.0.1", ...}
  {"timestamp": "...", "event": "ready", "version": "v0.0.1", ...}
  ```
- ‚úÖ Docker container: `kosbalakin/telegram-voice2text-bot:v0.0.1`
- ‚úÖ Container status: `healthy`
- ‚úÖ Bot fully operational

**PRs Created**:
- PR #29: Main Phase 7 implementation (+2042/-20 lines)
- PR #30: Workflow permissions fix
- PR #31: Deploy trigger improvements

**Files Modified** (PR #30, #31):
- `.github/workflows/build-and-tag.yml` - Added permissions
- `.github/workflows/deploy.yml` - Added workflow_run and workflow_dispatch triggers

**Verification Commands Used**:
```bash
# Check version
ssh telegram-bot "docker ps --format '{{.Image}}'"
# Output: kosbalakin/telegram-voice2text-bot:v0.0.1

# Check logs directory
ssh telegram-bot "ls -lh /opt/telegram-voice2text-bot/logs/"

# View deployment events
ssh telegram-bot "cat /opt/telegram-voice2text-bot/logs/deployments.jsonl"

# View application logs with version
ssh telegram-bot "head -20 /opt/telegram-voice2text-bot/logs/app.log"
```

**Impact**:
- ‚úÖ Full production observability with version tracking
- ‚úÖ Logs persist across container rebuilds
- ‚úÖ Can correlate logs with specific deployments
- ‚úÖ User-friendly version numbers (v0.0.1 instead of 09f9af8)
- ‚úÖ Manual deployment workflow proven reliable
- ‚úÖ GitHub Releases created automatically

**Key Learnings**:
1. **GitHub Actions Workflows**: Workflows triggered by `GITHUB_TOKEN` don't automatically trigger other workflows (security)
2. **workflow_run Limitations**: Receives branch reference instead of tag, not suitable for version-based deployments
3. **workflow_dispatch Pattern**: Reliable for manual deployments, works perfectly with version inputs
4. **Permissions Matter**: Explicit permissions required for pushing tags and packages
5. **Testing Workflows**: Iterative testing needed - 5 workflow runs to identify and fix issues

**Status**: ‚úÖ Deployed and operational, logging collecting production data

### Phase 7.2: Fully Automatic Deployment Pipeline ‚úÖ COMPLETE (2025-11-04)
**Achievement**: Resolved GitHub Actions workflow_run limitations to achieve zero-intervention deployment

**User Requirement**:
- "–Ω–µ, –ø–æ–≥–æ–¥–∏. –Ø —Ö–æ—Ç–µ–ª, —á—Ç–æ–±—ã –±—ã–ª–æ –≤—Å–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –±–µ–∑ —Ä—É—á–Ω—ã—Ö –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤"
- "—Ç.–∫. —Å—á–∏—Ç–∞–µ—Ç—Å—è, —á—Ç–æ –≤ –º–∞—Å—Ç–µ—Ä –≤–ª–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –∏–∑ PR. –ú–∞—Å—Ç–µ—Ä - —ç—Ç–æ —Ä–∞–±–æ—á–∞—è –≤–µ—Ç–∫–∞, –∏–∑ –∫–æ—Ç–æ—Ä–æ–π –º–æ–∂–Ω–æ –≤ –ª—é–±–æ–π –º–æ–º–µ–Ω—Ç —Å–¥–µ–ª–∞—Ç—å –¥–µ–ø–ª–æ–π"
- Expected: PR merge to main ‚Üí automatic production deploy (zero manual steps)

**Problem Identified** (after Phase 7.1):
- workflow_run trigger receives `refs/heads/main` instead of tag reference
- Caused Docker image naming issues: `konstantinbalakin/telegram-voice2text-bot:refs/heads/main`
- Phase 7.1 solution (workflow_dispatch) required manual trigger
- Not acceptable for fully automatic pipeline

**Solution Implemented** (PR #32):

**1. Conditional Version Extraction**
- Modified `.github/workflows/deploy.yml` to handle all trigger types
- For workflow_run: use `git describe --tags --abbrev=0` to get latest tag
- For workflow_dispatch: use manual version input
- For tag push: use tag from GITHUB_REF

**2. Version Extraction Logic**:
```bash
if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
  VERSION="${{ github.event.inputs.version }}"
elif [ "${{ github.event_name }}" = "push" ]; then
  VERSION=${GITHUB_REF#refs/tags/}
else
  # workflow_run - get latest tag from repository
  git fetch --tags
  VERSION=$(git describe --tags --abbrev=0)
fi
```

**3. Additional Safeguards**:
- Added `fetch-depth: 0` to checkout (need full git history)
- Added success check: only deploy if Build & Tag succeeded
- Maintained workflow_dispatch as rollback mechanism

**Testing Results**:
- ‚úÖ v0.0.2: First automatic deployment test
- ‚úÖ v0.0.3: Production verification
- ‚úÖ Both migrate and deploy jobs ran automatically
- ‚úÖ No manual intervention required
- ‚úÖ Container healthy, bot operational

**Deployment Flow (Fully Automatic)**:
```
Developer merges PR to main
  ‚Üì
Build & Tag Workflow (automatic)
  - Test database migrations
  - Build Docker image
  - Increment version (v0.0.2 ‚Üí v0.0.3)
  - Push to Docker Hub
  - Create git tag
  - Create GitHub Release
  ‚Üì (workflow_run trigger fires)
Deploy Workflow (automatic)
  - Get latest tag via git describe
  - Run migrations on VPS
  - Deploy new version
  - Health check verification
  ‚Üì
Production updated automatically
```

**Files Modified**:
- `.github/workflows/deploy.yml` (PR #32)
  - Added conditional version extraction (lines 32-52, 197-217)
  - Added `fetch-depth: 0` to checkout steps
  - Added workflow success check

**Verification**:
```bash
# v0.0.3 deployed automatically
docker ps --format '{{.Image}}'
# Output: kosbalakin/telegram-voice2text-bot:v0.0.3

# Container status
docker inspect telegram-voice2text-bot --format='{{.State.Health.Status}}'
# Output: healthy
```

**Impact**:
- ‚úÖ Fully automatic deployment pipeline operational
- ‚úÖ Main branch is single source of truth for production
- ‚úÖ Zero manual intervention required
- ‚úÖ Version tracking maintained (v0.0.1 ‚Üí v0.0.2 ‚Üí v0.0.3)
- ‚úÖ Rollback still possible via workflow_dispatch
- ‚úÖ User requirement met: "–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –±–µ–∑ —Ä—É—á–Ω—ã—Ö –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤"

**Key Learnings**:
1. **workflow_run Limitation**: Receives branch reference, not tag
2. **Workaround**: Use `git describe --tags --abbrev=0` to get latest tag
3. **Timing**: Works because Build & Tag pushes tag BEFORE workflow_run fires
4. **Full History Required**: Need `fetch-depth: 0` for git describe
5. **Iterative Testing**: Required 3 versions (v0.0.1 ‚Üí v0.0.3) to validate

**Key Pattern Established**: For GitHub Actions workflow_run with tag-based deployments, use `git describe --tags --abbrev=0` instead of relying on GITHUB_REF. This enables fully automatic CI/CD without manual triggers while maintaining version-based deployments.

**Status**: ‚úÖ Deployed and operational, fully automatic pipeline proven

### Phase 7.3: Queue Position & File Naming Bug Fixes ‚úÖ COMPLETE (2025-11-19)
**Achievement**: Fixed two critical production bugs affecting multi-user concurrent usage

**Bug 1: Queue Position Always Showing "1"**

**Problem**: When multiple audio files were sent in sequence, all queued messages displayed "üìã –í –æ—á–µ—Ä–µ–¥–∏: –ø–æ–∑–∏—Ü–∏—è 1" instead of their actual positions (2, 3, etc.).

**Root Cause**: Race condition between `enqueue()` and worker pulling from queue
- Handler was recalculating position using `get_queue_depth()` after enqueueing
- Worker immediately pulls items, making `qsize()` unreliable
- Position returned by `enqueue()` was being ignored

**Solution** (commit pending):
- Added `_total_pending` counter in `QueueManager.__init__()`
- Counter increments BEFORE `put()` atomically
- Counter decrements in `_process_request()` finally block
- Handler uses returned position directly

**Bug 2: FileNotFoundError with Concurrent Users**

**Problem**: When two users forwarded the same voice message, second user got `FileNotFoundError`.

**Root Cause**: Same `file_id` created same filename
- First request downloads, processes, deletes file
- Second request fails to find deleted file

**Solution** (commit pending):
- Added UUID suffix to all downloaded filenames in `audio_handler.py`
- Format: `{file_id}_{uuid.uuid4().hex[:8]}{extension}`
- Each download creates unique file regardless of `file_id`

**Files Modified**:
- `src/services/queue_manager.py` - Added `_total_pending` counter
- `src/bot/handlers.py` - Use returned position from `enqueue()`
- `src/transcription/audio_handler.py` - UUID suffix for filenames

**Testing Results**:
- ‚úÖ Queue positions correctly show 1, 2, 3...
- ‚úÖ User confirmed fix works: "–í—Ä–æ–¥–µ —Ç–µ–ø–µ—Ä—å –æ–∫"
- ‚úÖ File conflicts resolved for concurrent users
- ‚úÖ All code quality checks pass

**Key Patterns Established**:
1. **Atomic Counter Pattern**: Use dedicated counter for queue position tracking instead of `qsize()` which is affected by concurrent workers
2. **Unique File Naming Pattern**: Add UUID suffix when multiple users may process same resource (file_id) to prevent collisions

**Impact**:
- ‚úÖ Accurate queue position feedback
- ‚úÖ Reliable concurrent multi-user operation
- ‚úÖ No file conflicts when same message forwarded by multiple users

**Status**: ‚úÖ Implemented and tested, ready for production deployment

### Phase 7.4: Dynamic Queue Notifications ‚úÖ COMPLETE (2025-11-19)
**Achievement**: Enhanced queue notification system with accurate time calculation and dynamic updates

**Problem Solved**:
- Confusing formulation: "–ü—Ä–∏–º–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è" didn't clarify queue wait vs processing time
- Inaccurate calculation: Used current message duration instead of messages ahead
- No dynamic updates when queue progressed
- Didn't account for currently processing messages
- Didn't handle parallel processing (max_concurrent > 1)

**Implementation**:

**1. QueueManager Enhancements** (`src/services/queue_manager.py`)
- `_pending_requests` list: Track queue items with durations
- `_processing_requests` list: Track currently processing items
- `_on_queue_changed` callback: Notify handlers on queue changes
- `get_estimated_wait_time_by_id()`: Accurate time calculation
- `get_queue_position_by_id()`: Current position in queue
- Wait time formula: `(processing + pending_ahead) * rtf / max_concurrent`

**2. Dynamic Updates** (`src/bot/handlers.py`)
- `_update_queue_messages()`: Updates all pending users when queue changes
- Clear message format:
  ```
  üìã –í –æ—á–µ—Ä–µ–¥–∏: –ø–æ–∑–∏—Ü–∏—è 2
  ‚è±Ô∏è –û–∂–∏–¥–∞–Ω–∏–µ –≤ –æ—á–µ—Ä–µ–¥–∏: ~1–º 50—Å
  üéØ –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∞—à–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è: ~18—Å
  ```
- Time formatting: seconds for <60s, minutes+seconds for longer

**Files Modified**:
- `src/services/queue_manager.py` - Queue tracking and time calculation
- `src/bot/handlers.py` - Dynamic updates and message formatting

**Testing Results**:
- ‚úÖ User confirmed: "–ë–µ–ª–∏—Å—Å–∏–º–æ! –í—Å–µ –∏–¥–µ–∞–ª—å–Ω–æ –≤—Ä–æ–¥–µ —Ä–∞–±–æ—Ç–∞–µ—Ç"
- ‚úÖ Correct positions (1, 2, 3...)
- ‚úÖ Accurate wait times
- ‚úÖ Dynamic updates work
- ‚úÖ All quality checks pass

**Key Patterns**:
1. **Callback Pattern**: Notify handlers when queue state changes
2. **Dual List Tracking**: Separate pending and processing for accurate calculation
3. **Time Formatting**: Appropriate units (seconds vs minutes)

**Impact**:
- ‚úÖ Clear, informative notifications
- ‚úÖ Accurate wait estimates
- ‚úÖ Better UX with dynamic updates
- ‚úÖ Full transparency about queue state

**Status**: ‚úÖ Implemented and tested, ready for production deployment

### Phase 8: Hybrid Transcription Acceleration ‚úÖ COMPLETE (2025-11-20)
**Achievement**: Implemented hybrid transcription strategy for 6-9x performance improvement on long audio

**Problem Solved**:
- Long audio (60s+) takes 36s to process with medium model (RTF 0.6x)
- Poor UX for users with long voice messages
- Need to maintain quality while dramatically improving speed

**Implementation** (PR #44, 5 commits):

**Commit 1: LLM Service Infrastructure** (8eea54f equivalent)
- Created `src/services/llm_service.py` (263 lines):
  - `LLMProvider` abstract base class for future extensibility
  - `DeepSeekProvider` with retry logic (tenacity library)
  - `LLMFactory` for provider instantiation
  - `LLMService` high-level API with graceful fallback
- Updated `src/config.py` with LLM and hybrid configuration (44 new settings)
- Added `tenacity = "^9.0.0"` to dependencies
- Created comprehensive test suite: `tests/unit/test_llm_service.py` (19 tests)
- All tests passing ‚úÖ

**Commit 2: Hybrid Strategy & Audio Preprocessing** (f6e1d5c equivalent)
- Added `HybridStrategy` to `src/transcription/routing/strategies.py`:
  - Duration-based routing: <20s = quality, ‚â•20s = draft + LLM
  - Methods: `select_provider()`, `get_model_for_duration()`, `requires_refinement()`
- Added audio preprocessing to `src/transcription/audio_handler.py`:
  - `preprocess_audio()` - main pipeline
  - `_convert_to_mono()` - ffmpeg mono conversion
  - `_adjust_speed()` - ffmpeg speed adjustment (0.5-2.0x)
- Created test suite: `tests/unit/test_hybrid_strategy.py` (29 tests)
- Created test suite: `tests/unit/test_audio_preprocessing.py` (29 tests)
- Fixed settings mock scope issue in tests
- All tests passing ‚úÖ

**Commit 3: Handler Integration**
- Updated `src/bot/handlers.py` with hybrid transcription flow:
  - Audio preprocessing pipeline
  - Hybrid strategy detection
  - Staged message updates (draft ‚Üí refining ‚Üí final)
  - LLM refinement for long audio
  - Graceful fallback on errors
  - Cleanup of preprocessed files
- Updated `src/main.py` with LLM service initialization
- All 93 unit tests passing ‚úÖ

**Commit 4: Code Formatting**
- Ran `poetry run black src/ tests/`
- All files formatted ‚úÖ

**Commit 5: Type Fixes**
- Added missing return type annotations
- Fixed type narrowing issue with isinstance check
- Fixed unused imports
- All quality checks passing (mypy, ruff, black, pytest) ‚úÖ

**Configuration**:
```bash
# LLM Refinement (disabled by default for safety)
LLM_REFINEMENT_ENABLED=false
LLM_PROVIDER=deepseek
LLM_API_KEY=your_key_here
LLM_MODEL=deepseek-chat
LLM_BASE_URL=https://api.deepseek.com
LLM_TIMEOUT=30

# Hybrid Strategy
HYBRID_SHORT_THRESHOLD=20  # seconds
HYBRID_DRAFT_PROVIDER=faster-whisper
HYBRID_DRAFT_MODEL=small
HYBRID_QUALITY_PROVIDER=faster-whisper
HYBRID_QUALITY_MODEL=medium

# Audio Preprocessing (optional)
AUDIO_CONVERT_TO_MONO=false
AUDIO_TARGET_SAMPLE_RATE=16000
AUDIO_SPEED_MULTIPLIER=1.0
```

**Expected Performance** (60s audio):
- Before: ~36s processing (RTF 0.6x)
- After: ~6s processing (3s draft + 3s LLM = RTF 0.1x)
- **Improvement**: 6x faster

**Cost Analysis**:
- DeepSeek V3: ~$0.0002 per 60s audio
- 30x cheaper than OpenAI Whisper API
- Negligible cost for expected usage

**Testing Results**:
- ‚úÖ All 93 unit tests passing (45 original + 48 new)
- ‚úÖ Quality checks: mypy, ruff, black all passing
- ‚úÖ Implementation complete and documented
- ‚è≥ Manual testing pending
- ‚è≥ Production deployment pending (feature disabled by default)

**Files Created** (8 files, 1,400+ lines):
- `src/services/llm_service.py` (263 lines)
- `tests/unit/test_llm_service.py` (257 lines)
- `tests/unit/test_hybrid_strategy.py` (189 lines)
- `tests/unit/test_audio_preprocessing.py` (220 lines)
- `memory-bank/plans/2025-11-20-hybrid-transcription-acceleration.md`
- `memory-bank/plans/2025-11-20-hybrid-implementation-checklist.md`

**Files Modified** (6 files):
- `src/config.py` - Added LLM and hybrid configuration
- `src/transcription/routing/strategies.py` - Added HybridStrategy class
- `src/transcription/audio_handler.py` - Added preprocessing methods
- `src/bot/handlers.py` - Integrated hybrid flow
- `src/main.py` - LLM service initialization
- `pyproject.toml` - Added tenacity dependency
- `.env.example` - Added comprehensive configuration examples

**Key Patterns Established**:
1. **Graceful Degradation**: LLM failures fall back to draft text
2. **Staged UI Updates**: Show draft immediately, then refinement
3. **Abstract Provider Pattern**: Easy to add more LLM providers
4. **Audio Preprocessing Pipeline**: Optional mono conversion and speed adjustment
5. **Feature Flags**: Disabled by default for safe production rollout

**Impact**:
- ‚úÖ 6x performance improvement for long audio
- ‚úÖ Maintains quality through LLM refinement
- ‚úÖ Better UX with staged updates
- ‚úÖ Cost-effective (DeepSeek V3)
- ‚úÖ Safe rollout strategy (disabled by default)

**Next Steps**:
1. ‚è≥ Manual testing with real voice messages
2. ‚è≥ Verify LLM refinement quality
3. ‚è≥ Test fallback scenarios (LLM timeout/error)
4. ‚è≥ Merge PR to main after testing
5. ‚è≥ Deploy with hybrid mode disabled (default)
6. ‚è≥ Enable hybrid mode for specific test users
7. ‚è≥ Monitor performance metrics and costs
8. ‚è≥ Gradual rollout to all users after validation

**Status**: ‚úÖ Implementation complete, PR #44 created, ready for testing and deployment

### Phase 8.1: DEBUG Logging Enhancement ‚úÖ COMPLETE (2025-11-24)
**Achievement**: Comprehensive DEBUG logging system for local development and troubleshooting

**Problem Solved**:
- Limited DEBUG logs throughout codebase (only 7 logger.debug() calls)
- No visibility into SQL queries, method parameters, execution flow
- Hardcoded log levels prevented DEBUG mode from showing detailed information
- Difficult to debug issues during local development

**Implementation** (3 phases):

**Phase 1: Logging Infrastructure Updates**
- Updated `src/utils/logging_config.py`:
  - Made all handlers respect configured `log_level` (was hardcoded to INFO)
  - Added separate `debug.log` file (5MB, 3 backups) - **only created when LOG_LEVEL=DEBUG**
  - Ensures production stays clean while DEBUG provides maximum detail
- Updated `src/storage/database.py`:
  - Made SQLAlchemy `echo` parameter dynamic based on LOG_LEVEL
  - Shows all SQL queries and connection pool activity in DEBUG mode

**Phase 2: Strategic DEBUG Logs Added**
- **Bot Handlers** (`src/bot/handlers.py`):
  - Command invocations with user IDs
  - Voice/audio message metadata (file_id, duration, size)
  - Queue checks and positions
  - Transcription request creation details
- **Storage Repositories** (`src/storage/repositories.py`):
  - User lookup/creation with telegram_id
  - Usage record creation and updates
  - Database field values before/after changes
- **Queue Manager** (`src/services/queue_manager.py`):
  - Request enqueue with position, duration, queue depth
  - Semaphore waits and processing starts
  - Wait time calculations with RTF details
- **Audio Handler** (`src/transcription/audio_handler.py`):
  - File download details (size, path, extension)
  - Preprocessing steps (mono conversion, speed adjustment)
- **Transcription Providers**:
  - FasterWhisper: Model init timing, memory usage, segment counts
  - OpenAI: API request params, masked API keys, retry attempts
- **LLM Service** (`src/services/llm_service.py`):
  - Draft/refined text lengths, token usage
  - Masked API keys for security

**Phase 3: Documentation**
- Updated `.env.example` with comprehensive DEBUG mode details:
  - 8 categories of DEBUG logging documented
  - Log file purposes and sizes
  - Usage instructions and security warnings
- Updated `.env.example.short` with concise reference

**Log Files**:
| File | Level | Size | Backups | When Created |
|------|-------|------|---------|--------------|
| `logs/app.log` | INFO+ | 10MB | 5 | Always |
| `logs/errors.log` | ERROR+ | 5MB | 5 | Always |
| `logs/debug.log` | DEBUG+ | 5MB | 3 | **Only when LOG_LEVEL=DEBUG** |

**Key Features**:
- ‚úÖ SQL queries logged with parameters and results
- ‚úÖ Method entry points with all parameters logged
- ‚úÖ Queue operations with positions and timings
- ‚úÖ API calls with masked keys (first 8 chars + "...")
- ‚úÖ File operations with paths and sizes
- ‚úÖ Transcription details with memory usage
- ‚úÖ Separate debug.log only created in DEBUG mode
- ‚úÖ Production unaffected (INFO/WARNING modes work as before)

**Usage**:
```bash
# Enable DEBUG mode
LOG_LEVEL=DEBUG

# Restart bot
docker-compose restart

# Monitor debug logs
tail -f logs/debug.log

# Analyze with jq
cat logs/debug.log | jq '.'
```

**Security Notes**:
- API keys automatically masked in logs
- Never use DEBUG in production
- Contains sensitive data (SQL queries with values)
- DEBUG significantly increases log file sizes

**Files Modified**:
- `src/utils/logging_config.py` - Dynamic log levels + debug handler
- `src/storage/database.py` - Dynamic SQL echo
- `src/bot/handlers.py` - Handler DEBUG logs
- `src/storage/repositories.py` - Repository DEBUG logs
- `src/services/queue_manager.py` - Queue DEBUG logs
- `src/transcription/audio_handler.py` - Audio DEBUG logs
- `src/transcription/providers/faster_whisper_provider.py` - Provider DEBUG logs
- `src/transcription/providers/openai_provider.py` - API DEBUG logs
- `src/services/llm_service.py` - LLM DEBUG logs
- `.env.example` - Comprehensive DEBUG documentation
- `.env.example.short` - Concise DEBUG reference

**Key Pattern Established**: Comprehensive DEBUG logging is essential for local development. Use dynamic log levels and separate log files to avoid impacting production. Always mask sensitive data (API keys) in logs.

**Impact**:
- ‚úÖ Full visibility into execution flow
- ‚úÖ SQL queries visible for debugging
- ‚úÖ Method parameters logged
- ‚úÖ API calls traceable
- ‚úÖ Easy to enable/disable
- ‚úÖ Production unaffected

**Status**: ‚úÖ Complete, ready for local debugging use

### Phase 8.2: LLM Debug Mode ‚úÖ COMPLETE (2025-11-24)
**Achievement**: Side-by-side comparison of draft and refined transcriptions for LLM quality evaluation

**Problem Solved**:
- No way to compare draft transcription with LLM-refined version
- Difficult to evaluate LLM refinement quality
- Hard to tune LLM prompts and settings
- No visibility into what LLM changed

**Implementation**:

**1. Configuration Flag** (`src/config.py`)
```python
llm_debug_mode: bool = Field(
    default=False,
    description="Send draft and refined text comparison in separate message",
)
```

**2. Handler Integration** (`src/bot/handlers.py`)
- After successful LLM refinement, sends additional debug message
- Shows both draft and refined text side-by-side
- HTML formatting for better readability
- Automatic truncation for long texts (>4000 chars)
- Graceful error handling

**Debug Message Format**:
```
üîç –°—Ä–∞–≤–Ω–µ–Ω–∏–µ (LLM_DEBUG_MODE=true)

üìù –ß–µ—Ä–Ω–æ–≤–∏–∫ (small):
–ø—Ä–∏–≤–µ—Ç —ç—Ç–æ —Ç–µ—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –±–µ–∑ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏

‚ú® –ü–æ—Å–ª–µ LLM (deepseek-chat):
–ü—Ä–∏–≤–µ—Ç! –≠—Ç–æ —Ç–µ—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –±–µ–∑ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏.
```

**3. Documentation** (`.env.example`, `.env.example.short`)
- Added `LLM_DEBUG_MODE` flag documentation
- Usage instructions and warnings
- Added to hybrid mode examples

**Configuration**:
```bash
# Enable debug mode
LLM_DEBUG_MODE=true

# Must have LLM enabled
LLM_REFINEMENT_ENABLED=true
LLM_API_KEY=sk-your-key
WHISPER_ROUTING_STRATEGY=hybrid
```

**Key Features**:
- ‚úÖ Shows both draft and refined text
- ‚úÖ Indicates which models were used
- ‚úÖ HTML formatting with `<code>` blocks
- ‚úÖ Auto-truncation for long texts
- ‚úÖ Only works when LLM refinement succeeds
- ‚úÖ Disabled by default (no extra messages)
- ‚úÖ Safe - doesn't break main functionality if fails

**Use Cases**:
- ‚úÖ Testing LLM quality
- ‚úÖ Comparing different models (`LLM_MODEL`)
- ‚úÖ Tuning refinement prompts (`LLM_REFINEMENT_PROMPT`)
- ‚úÖ Evaluating punctuation/capitalization improvements

**Files Modified**:
- `src/config.py` - Added `llm_debug_mode` flag
- `src/bot/handlers.py` - Added debug comparison message (lines 864-888)
- `.env.example` - Added `LLM_DEBUG_MODE` documentation
- `.env.example.short` - Added inline comment

**Impact**:
- ‚úÖ Easy quality evaluation
- ‚úÖ Visible LLM improvements
- ‚úÖ Helps with prompt tuning
- ‚úÖ Useful for testing different models

**Status**: ‚úÖ Complete, ready for LLM quality testing

**Branch**: `feature/hybrid-transcription` (will be included in PR #44 when merged)

### Phase 9: Performance Optimization ‚è≥ DEFERRED
**Goal**: Achieve RTF ~0.3x (match local benchmark performance) with hardware upgrade

**Current Baseline**:
- 1GB RAM + 1 vCPU: RTF 3.04x (3x slower than audio)
- Heavy swap usage (755 MB) identified as bottleneck

**Planned Experiments**:
1. **2GB RAM**: Eliminate swap, measure improvement
2. **2 vCPU**: Test CPU parallelization impact
3. **2GB + 2 vCPU**: Optimal configuration (if budget allows)

**Method**: Systematic A/B testing with same audio sample, document all findings

**Priority**: LOW - Phase 8 (hybrid transcription) addresses performance through software optimization. Hardware upgrade only needed if hybrid approach insufficient.

## Branch Status

**Current**: `main`
- All development merged
- Production deployment operational
- Bot live and responding to messages

## Success Metrics

**Technical Readiness**: ‚úÖ 100%
- Code: Production-ready
- Tests: All 45 passing
- Documentation: Current
- Docker: Optimized and deployed

**Deployment Readiness**: ‚úÖ 100%
- Infrastructure: VPS operational
- CI/CD: Fully automated
- Monitoring: Active
- Health checks: Passing

**Production Status**: ‚úÖ OPERATIONAL (100%)
- Bot: Healthy and responding
- Transcription: Working correctly with queue management
- Container: Stable with health checks
- Dependencies: All included correctly
- Database: Migrations automated and operational
- Queue System: Sequential processing with progress feedback
- Configuration: Optimized for 1GB RAM / 1 vCPU VPS

**Feature Status**: ‚úÖ COMPLETE (100%)
- Core transcription: ‚úÖ Working
- Queue management: ‚úÖ Deployed (max 10 requests)
- Progress tracking: ‚úÖ Live updates every 5s
- Duration limits: ‚úÖ 120s max
- Database migrations: ‚úÖ Automated in CI/CD
- Health checks: ‚úÖ Schema version verification
- CI/CD: ‚úÖ Full automation with testing

**Performance Status**: ‚ö†Ô∏è Acceptable with Progress Feedback (70%)
- Current: RTF 3.04x (3x slower than audio, sequential processing)
- Target: RTF 0.3x (3x faster than audio, aspirational)
- Bottleneck: 1 vCPU + swap usage
- User Experience: ‚úÖ Acceptable with live progress bar
- Next: Monitor real-world usage, optimize only if needed

---

### Phase 8.3: LLM Performance Tracking ‚úÖ COMPLETE (2025-11-24)

**Goal**: Track LLM processing time and model in database for performance analytics

**Problem**:
- Hybrid mode only tracked Whisper transcription time
- No visibility into LLM refinement performance
- Impossible to analyze LLM costs or compare models
- Missing data for optimization decisions

**Solution Implemented**:

1. **Database Schema Changes**
   - New field: `llm_model` (VARCHAR(100), nullable)
   - New field: `llm_processing_time_seconds` (FLOAT, nullable)
   - Migration: `0fde9e5effe5_add_llm_tracking_fields.py`
   - Backward compatible: nullable fields

2. **Lifecycle Extension**
   - **Stage 3** (after Whisper): Save Whisper results + `llm_model`
   - **Stage 4** (NEW, after LLM): Save `llm_processing_time_seconds`

3. **Implementation**
   ```python
   # Stage 3: After Whisper
   await usage_repo.update(
       model_size=result.model_name,
       processing_time_seconds=result.processing_time,
       llm_model=settings.llm_model,  # NEW
   )

   # Stage 4: After LLM
   llm_start = time.time()
   refined = await llm_service.refine_transcription(draft)
   llm_time = time.time() - llm_start

   await usage_repo.update(
       llm_processing_time_seconds=llm_time,  # NEW
   )
   ```

4. **Model & Repository Updates**
   - `Usage` model: Added 2 new fields
   - `UsageRepository.create()`: Extended signature
   - `UsageRepository.update()`: Extended signature

**Technical Details**:

**Files Modified**:
- `alembic/versions/0fde9e5effe5_add_llm_tracking_fields.py` - NEW
- `src/storage/models.py` - Added LLM tracking fields
- `src/storage/repositories.py` - Extended methods
- `src/bot/handlers.py` - Added time tracking and Stage 4

**Data Examples**:
```
# Long audio with LLM
model_size: "medium"
processing_time_seconds: 10.5
llm_model: "deepseek-chat"
llm_processing_time_seconds: 3.2

# Short audio without LLM
model_size: "medium"
processing_time_seconds: 3.0
llm_model: NULL
llm_processing_time_seconds: NULL
```

**Benefits**:
- üìä Performance visibility: Separate Whisper/LLM metrics
- üí∞ Cost analysis: Calculate per-request LLM costs
- üîç Model comparison: A/B test different LLM providers
- ‚ö° Optimization: Identify pipeline bottlenecks
- üìà Analytics: RTF for both processing stages

**Testing & Quality**:
- ‚úÖ All 93 unit tests pass
- ‚úÖ Manual validation (4-stage lifecycle verified)
- ‚úÖ Database schema confirmed
- ‚úÖ Type checking (mypy): 0 errors
- ‚úÖ Code formatting (black): applied

**Deployment Plan**:
1. Merge PR #49
2. Deploy to production
3. Apply migration: `alembic upgrade head`
4. Monitor logs for "LLM refinement took X.XXs"
5. Query analytics:
   ```sql
   SELECT
     AVG(processing_time_seconds) as avg_whisper,
     AVG(llm_processing_time_seconds) as avg_llm
   FROM usage WHERE llm_model IS NOT NULL;
   ```

**Impact**: Full visibility into hybrid transcription performance, enabling data-driven optimization

**Status**: ‚úÖ Complete, PR #49 created
**Branch**: `feature/llm-tracking`
**PR**: https://github.com/konstantinbalakin/telegram-voice2text-bot/pull/49

---

### Phase 9: Large File Support (Telethon Client API) ‚úÖ COMPLETE (2025-11-30)

**Achievement**: Seamless support for audio files >20 MB up to 2 GB using Telegram Client API

**Problem Solved**:
- Telegram Bot API has 20 MB hard limit for file downloads
- Users couldn't process larger audio files
- Needed transparent solution without changing UX

**Technical Decision**: **Telethon over Pyrogram**
- Telethon v1.42.0 (November 2025) actively maintained
- Pyrogram v2.0.106 (April 2023) no updates for 20+ months
- Telethon more stable, official library

**Implementation**:

**1. New Service Layer** (`src/services/telegram_client.py`)
- Telethon client for MTProto Client API
- Session-based authentication (persistent across restarts)
- Context manager support
- Supports files up to 2 GB (Telegram limit)

**2. Hybrid Download Strategy** (`src/bot/handlers.py`)
```python
if file_size > 20 MB:
    # Use Client API (Telethon)
    file_path = await telegram_client.download_large_file(...)
else:
    # Use Bot API (existing flow)
    file_path = await audio_handler.download_voice_message(...)
```

**3. Dynamic File Size Limits**
- Client API enabled: 2 GB max
- Client API disabled: 20 MB max
- Graceful fallback with clear messages

**4. Configuration** (`src/config.py`)
```python
telegram_api_id: int | None
telegram_api_hash: str | None
telethon_session_name: str = "bot_client"
telethon_enabled: bool = False  # Feature flag
```

**5. Bot Initialization** (`src/main.py`)
- Start Telethon client on startup (if enabled)
- Automatic session creation/loading
- Graceful shutdown with disconnect
- Error handling for missing credentials

**Authentication Flow**:
1. Get API credentials from https://my.telegram.org (one-time)
2. Add to `.env`: TELETHON_ENABLED, TELEGRAM_API_ID, TELEGRAM_API_HASH
3. Bot creates session file automatically (bot_client.session)
4. Session persists across restarts
5. Bot tokens authenticate automatically (no phone/SMS)

**Files Created**:
- `src/services/telegram_client.py`
- `memory-bank/plans/2025-11-30-large-file-support-telethon.md`

**Files Modified**:
- `pyproject.toml` - Added telethon, cryptg dependencies
- `src/config.py` - Client API configuration
- `src/services/__init__.py` - Export TelegramClientService
- `src/bot/handlers.py` - Hybrid download logic
- `src/main.py` - Telethon initialization
- `.env.example` - Client API setup
- `.gitignore` - Exclude *.session files

**Benefits**:
- ‚úÖ 100x file size increase (20 MB ‚Üí 2 GB)
- ‚úÖ Seamless user experience
- ‚úÖ Graceful degradation
- ‚úÖ Production-ready

**Status**: ‚úÖ Complete, ready for production deployment with API credentials

---

### Phase 10.1: Interactive Transcription Infrastructure ‚úÖ COMPLETE (2025-12-03)

**Achievement**: Foundation for interactive transcription features - database models, keyboard system, callback handlers, and segment extraction

**Goal**: Implement Phase 1 of interactive transcription plan - create infrastructure for inline button-based features

**Problem Solved**:
- No user interaction with transcription results
- Users needed ability to request variations (structured, summary, length adjustments)
- Need state management for tracking UI interactions
- Need segment-level data for timestamp features

**Implementation** (9 major components):

**1. Database Schema** (`src/storage/models.py` +140 lines)
- `TranscriptionState`: UI state tracking
  - Tracks active mode, length level, emoji level, timestamps
  - Links to Usage record (usage_id)
  - Stores message_id and chat_id for message editing
- `TranscriptionVariant`: Cached text variations
  - Composite unique key (usage_id, mode, length_level, emoji_level, timestamps_enabled)
  - Stores generated_by, llm_model, processing_time
  - Tracks last_accessed_at for cache management
- `TranscriptionSegment`: Timestamp data from faster-whisper
  - Stores segment_index, start_time, end_time, text
  - Unique constraint on (usage_id, segment_index)

**2. Alembic Migration** (`alembic/versions/29b64c26ec8d_*.py`)
- Created 3 new tables with indexes and constraints
- Backward compatible (nullable foreign keys)
- Successfully applied to production database

**3. Repository Layer** (`src/storage/repositories.py` +200 lines)
- `TranscriptionStateRepository`: Create, get by usage_id/message_id, update
- `TranscriptionVariantRepository`: Create, get variant with cache hit tracking
- `TranscriptionSegmentRepository`: Batch creation, get by usage_id ordered

**4. Keyboard Manager** (`src/bot/keyboards.py` NEW FILE, 77 lines)
- `encode_callback_data()`: Compact format within 64-byte Telegram limit
  - Format: `action:usage_id:param1=val1,param2=val2`
  - Example: `mode:125:mode=original` (24 bytes)
  - Validation raises ValueError if exceeds limit
- `decode_callback_data()`: Parse callback data into dict
- `create_transcription_keyboard()`: Generate InlineKeyboardMarkup
  - Phase 1: Single button "–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç (–≤—ã –∑–¥–µ—Å—å)"
  - Respects feature flags

**5. Callback Handler** (`src/bot/callbacks.py` NEW FILE, 140 lines)
- `CallbackHandlers` class with routing logic
- `handle_callback_query()`: Main entry point, decodes data, routes to handlers
- `handle_mode_change()`: Phase 1 stub (only 'original' mode supported)
- Error handling with user-friendly messages
- Graceful degradation for unimplemented features

**6. Segment Extraction** (`src/transcription/providers/faster_whisper_provider.py`)
- Modified to return segments from faster-whisper
- Converts to `TranscriptionSegment` dataclass
- Included in `TranscriptionResult.segments` field

**7. Integration** (`src/bot/handlers.py` +60 lines)
- `_create_interactive_state_and_keyboard()`: Helper method
  - Creates TranscriptionState after message sent
  - Saves segments if duration > 300s (configurable threshold)
  - Generates keyboard
- Integration in 3 message paths:
  - LLM refined short text
  - LLM refined long text (multiple messages)
  - Direct result without LLM
- Uses `edit_reply_markup()` to add keyboard after message sent

**8. Main Entry Point** (`src/main.py` +30 lines)
- `callback_query_wrapper()`: Session management for callbacks
  - Creates repositories with async session per callback
  - Instantiates CallbackHandlers
  - Handles type annotations correctly
- Registered `CallbackQueryHandler` conditionally
- **CRITICAL FIX**: Added `allowed_updates=Update.ALL_TYPES` to `start_polling()`
  - Without this, Telegram doesn't send callback_query updates
  - Caused buttons to show "–ó–∞–≥—Ä—É–∑–∫–∞..." indefinitely
- Debug handler for all updates (`TypeHandler` with logging)

**9. Feature Flags** (`src/config.py` +18 lines)
- `interactive_mode_enabled`: Master switch (default: False)
- Phase-specific flags for future features (all default: False):
  - `enable_structured_mode`, `enable_summary_mode`
  - `enable_emoji_option`, `enable_timestamps_option`
  - `enable_length_variations`, `enable_retranscribe`
- Limits configuration:
  - `max_cached_variants_per_transcription`: 10
  - `variant_cache_ttl_days`: 7
  - `timestamps_min_duration`: 300 (5 minutes)

**Critical Bugs Fixed**:

**Bug 1: Callback Queries Not Received** (CRITICAL)
- **Symptom**: Button appeared but clicking showed "–ó–∞–≥—Ä—É–∑–∫–∞..." indefinitely
- **Root Cause**: python-telegram-bot doesn't receive callback_query updates without explicit `allowed_updates` configuration
- **Fix**: Added `allowed_updates=Update.ALL_TYPES` to `start_polling()` in `src/main.py:208`
- **Impact**: Buttons now work correctly, callback handler receives events

**Bug 2: UnboundLocalError with draft_text**
- **Symptom**: When `LLM_REFINEMENT_ENABLED=false`, got `UnboundLocalError`
- **Root Cause**: Variable `draft_text` undefined outside `if needs_refinement` block
- **Fix**: Changed line 1310 in `src/bot/handlers.py` to use `final_text` directly
- **Impact**: Bot works correctly with LLM disabled

**Files Created** (3 files, ~360 lines):
- `src/bot/keyboards.py` (77 lines)
- `src/bot/callbacks.py` (140 lines)
- `alembic/versions/29b64c26ec8d_add_interactive_transcription_tables.py`
- `PHASE1_ACCEPTANCE.md` (274 lines) - Russian acceptance testing guide

**Files Modified** (8 files):
- `src/storage/models.py` (+140 lines) - 3 new models
- `src/storage/repositories.py` (+200 lines) - 3 new repositories
- `src/transcription/providers/faster_whisper_provider.py` - Return segments
- `src/bot/handlers.py` (+60 lines) - Keyboard integration
- `src/main.py` (+30 lines) - Callback handler registration + allowed_updates fix
- `src/config.py` (+18 lines) - Feature flags
- `src/bot/__init__.py` - Export CallbackHandlers
- `src/services/__init__.py` - Export keyboard and callback modules

**Testing & Quality**:
- ‚úÖ All 108 unit tests passing
- ‚úÖ Type checking (mypy): Success, no issues found
- ‚úÖ Linting (ruff): All checks passed
- ‚úÖ Code formatting (black): Applied
- ‚úÖ Database migration: Applied successfully
- ‚úÖ Manual acceptance testing: All 4 scenarios completed
  - Scenario 1: Button appears for short audio ‚úÖ
  - Scenario 2: Button click shows loading then dismisses ‚úÖ
  - Scenario 3: Segments saved for long audio >5 min ‚úÖ
  - Scenario 4: Feature flag disables functionality ‚úÖ

**User Experience**:
```
User sends voice message (30s)
  ‚Üì
Bot transcribes with faster-whisper
  ‚Üì
Bot sends result with inline button:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Transcription text here...     ‚îÇ
‚îÇ                                 ‚îÇ
‚îÇ [–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç (–≤—ã –∑–¥–µ—Å—å)]   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
  ‚Üì
User clicks button
  ‚Üì
Telegram shows "–ó–∞–≥—Ä—É–∑–∫–∞..." briefly
  ‚Üì
Message unchanged (Phase 1 stub - already in 'original' mode)
Logs show: "Already in original mode for usage_id=X"
```

**Production Configuration**:
```bash
# Enable interactive mode
INTERACTIVE_MODE_ENABLED=true

# Phase 1: Only basic infrastructure, future features disabled
ENABLE_STRUCTURED_MODE=false
ENABLE_SUMMARY_MODE=false
ENABLE_EMOJI_OPTION=false
ENABLE_TIMESTAMPS_OPTION=false
ENABLE_LENGTH_VARIATIONS=false
ENABLE_RETRANSCRIBE=false

# Limits
MAX_CACHED_VARIANTS_PER_TRANSCRIPTION=10
VARIANT_CACHE_TTL_DAYS=7
TIMESTAMPS_MIN_DURATION=300  # 5 minutes
```

**Key Patterns Established**:
1. **allowed_updates Configuration**: CRITICAL for receiving callback_query updates from inline buttons
2. **Send Message Before Keyboard**: Message ID must exist before creating TranscriptionState, add keyboard via `edit_reply_markup()`
3. **Session Management for Callbacks**: Create repositories with async session context per callback invocation
4. **Callback Data Encoding**: Compact format within 64-byte limit with validation
5. **Segment Threshold**: Only save segments for audio >5 minutes to optimize storage
6. **Feature Flag System**: Granular control with master switch + phase-specific flags
7. **Graceful Stub Implementation**: Phase 1 acknowledges clicks but doesn't change state (foundation for future phases)

**Impact**:
- ‚úÖ Database infrastructure ready for all 8 planned phases
- ‚úÖ Callback system working correctly (after allowed_updates fix)
- ‚úÖ Segment extraction operational for timestamp features
- ‚úÖ Keyboard generation framework extensible for future modes
- ‚úÖ State management tracks user interactions
- ‚úÖ Variant caching system ready for LLM-generated alternatives
- ‚úÖ Feature flags enable safe gradual rollout

**Next Steps (Phase 2: Structured Mode)**:
- Implement LLM-based text structuring (punctuation, paragraphs)
- Add "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π" button to keyboard
- Implement mode switching with message editing
- Cache structured variants in database
- Update keyboard state indicator ("–≤—ã –∑–¥–µ—Å—å")

**Status**: ‚úÖ COMPLETE - All acceptance criteria met, ready for Phase 2
**Branch**: `docs/phase10-interactive-transcription-plan` (documentation branch)
**Completion Date**: 2025-12-03

**Documentation**:
- Implementation plan: `memory-bank/plans/2025-12-02-interactive-transcription-processing.md`
- Acceptance guide: `PHASE1_ACCEPTANCE.md` (Russian)
- Memory Bank updated: `activeContext.md`, `progress.md`
