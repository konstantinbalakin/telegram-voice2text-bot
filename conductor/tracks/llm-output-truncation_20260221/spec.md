# Specification: Обрезка ответа DeepSeek при обработке длинных текстов

**Track ID:** llm-output-truncation_20260221
**Type:** Bug
**Created:** 2026-02-21
**Status:** Draft

## Summary

При обработке длинных транскрипций (>15000 символов) через DeepSeek API ответ обрезается до `max_tokens=4000`, что приводит к неполному результату для пользователя. Проблема затрагивает режимы "Структурировать" и "Сделать красиво" (режим "О чём текст" не затронут — его output всегда короткий по дизайну промпта).

## Context

Бот транскрибирует голосовые сообщения через OpenAI Whisper API, затем обрабатывает текст через DeepSeek V3 API. При длинных аудиосообщениях (>20-30 минут) транскрипция может достигать 20000+ символов. Текущий лимит `max_tokens=4000` в конфигурации приводит к обрезке output при обработке таких текстов.

## Problem Description

### Шаги воспроизведения

1. Отправить боту длинное голосовое сообщение (~30+ минут)
2. Получить транскрипцию (~22000 символов)
3. Нажать кнопку "Структурировать" или "Сделать красиво"
4. Получить обрезанный результат

### Подтверждённые данные из логов

- Аудио: 33 минуты → 22396 символов транскрипции
- Input в DeepSeek: 16047 токенов (промпт + текст)
- Output из DeepSeek: **ровно 4000 токенов** (упёрся в лимит) → 11899 символов
- Текст обрезан примерно на **47%**

### Ожидаемое поведение

Пользователь должен получать полный обработанный текст без молчаливой обрезки.

### Фактическое поведение

- Текст молча обрезается до 4000 output-токенов
- Нет проверки `finish_reason == "length"` в ответе API
- Пользователь не получает никакого уведомления об обрезке
- Результат выглядит как полный текст, но содержит только ~53% контента

## Анализ ограничений DeepSeek API

| Параметр | DeepSeek-chat (V3) | DeepSeek-reasoner |
|----------|-------------------|-------------------|
| Контекстное окно | 128K токенов | 128K токенов |
| Max output tokens | **8K (8192)** | **64K** |
| Цена (input cache miss) | $0.28/M | $0.28/M |
| Цена (output) | $0.42/M | $0.42/M |

**Ключевой вывод**: Текущий `max_tokens=4000` — это только **половина** реального лимита модели (8192). Простое увеличение до 8192 решит проблему для части случаев, но не для всех.

## Решение (утверждённый план)

### Фаза 1: Базовые улучшения (обязательно)

1. **Увеличить `max_tokens` до 8192** — использовать полный лимит модели `deepseek-chat`. Решает ~80% случаев обрезки.
2. **Добавить обработку `finish_reason == "length"`** — гигиена: логирование warning при обрезке + возврат информации о truncation вызывающему коду.

### Фаза 2: Выбор модели через ENV (обязательно)

3. **Добавить настройку выбора модели DeepSeek в `.env`** — возможность переключения между `deepseek-chat` и `deepseek-reasoner`. Reasoner имеет лимит 64K output tokens, что снимает проблему обрезки для подавляющего большинства случаев. Качество ответов reasoner'а нужно будет протестировать вручную после внедрения.

### Фаза 3: Стратегия для сверхдлинных текстов (только для `deepseek-chat`)

> **Важно**: Фаза 3 актуальна **только если в Фазе 2 выбрана модель `deepseek-chat`**. Если основная модель — `deepseek-reasoner` (64K output), стратегии из Фазы 3 не нужны, т.к. лимит output достаточен для любых реальных текстов.

4. **Эвристическая оценка токенов** — перед отправкой в DeepSeek оценивать примерное количество output-токенов по количеству символов текста + промпта. Если оценка превышает 8K токенов:
   - **Стратегия A: Авто-переключение на reasoner** — если доступен reasoner, автоматически использовать его для длинных текстов (64K output limit)
   - **Стратегия B: Chunking** — разбить текст на чанки "в тупую" по количеству символов, обработать каждый отдельно, склеить результат (без семантического анализа)
   - Выбор стратегии — через конфигурацию в `.env`

5. **Исключение для режима "О чём текст" (summary)** — для кнопки "О чём текст" переключение на reasoner и chunking **не нужны**, т.к. суть промпта — вычленить суть из большого текста и изложить кратко, output всегда будет < 8K токенов.

## Acceptance Criteria

- [ ] `max_tokens` увеличен до 8192 (максимум модели `deepseek-chat`)
- [ ] Добавлена проверка `finish_reason == "length"` с warning-логированием
- [ ] Добавлена настройка `.env` для выбора модели (`deepseek-chat` / `deepseek-reasoner`)
- [ ] Реализована эвристическая оценка количества output-токенов по символам
- [ ] При превышении лимита 8K: авто-переключение на reasoner ИЛИ chunking (настраивается)
- [ ] Chunking реализован простым разбиением по символам (без семантики)
- [ ] Режим "О чём текст" (summary) исключён из логики chunking/переключения
- [ ] Добавлены тесты для всех сценариев
- [ ] Все режимы обработки (structured, magic) корректно работают с длинными текстами

## Dependencies

- DeepSeek API — лимит 8192 output tokens для `deepseek-chat`, 64K для `deepseek-reasoner`
- Существующий код в `src/services/llm_service.py` и `src/services/text_processor.py`
- Конфигурация в `src/config.py`

## Out of Scope

- Переход на другого LLM-провайдера
- Изменение лимитов Telegram на размер сообщений (4096 символов — отдельная проблема)
- Оптимизация промптов для уменьшения размера output
- Billing/quotas для пользователей
- Семантический chunking (разбиение по темам/абзацам)
- Ручное тестирование качества reasoner (делается владельцем после внедрения)

## Technical Notes

- DeepSeek V3 (`deepseek-chat`) context window: 128K tokens, max output: 8192 tokens
- DeepSeek Reasoner max output: 64K tokens (качество для форматирования текста — TBD)
- Текущий `max_tokens=4000` — вдвое ниже реального лимита модели
- `finish_reason == "length"` — стандартный способ детекции обрезки в OpenAI-compatible API
- Эвристика оценки токенов: ~1 токен ≈ 3-4 символа для русского текста (нужна калибровка)
- Для chunking достаточно простого разбиения по символам с перекрытием на границах предложений

---

_Generated by Conductor. Review and edit as needed._
