# =============================================================================
# Telegram Voice2Text Bot Configuration
# =============================================================================
# Copy this file to .env and fill in your values:
# cp .env.example .env

# =============================================================================
# Telegram Bot Settings (REQUIRED)
# =============================================================================

# Bot token from @BotFather
# Get your token: https://t.me/BotFather -> /newbot
# Format: 1234567890:ABCdefGHIjklMNOpqrsTUVwxyz
TELEGRAM_BOT_TOKEN=your_bot_token_here

# Bot mode: "polling" for local development, "webhook" for production
BOT_MODE=polling

# =============================================================================
# Transcription Provider Configuration
# =============================================================================

# Enabled providers (comma-separated list in JSON format)
# Options: faster-whisper, whisper, openai
# Examples:
#   Single provider:   ["faster-whisper"]
#   Multiple providers: ["faster-whisper", "openai"]
#   All providers:     ["faster-whisper", "whisper", "openai"]
WHISPER_PROVIDERS=["faster-whisper"]

# Routing strategy
# Options:
#   single    - Use one provider (configured in PRIMARY_PROVIDER)
#   fallback  - Use PRIMARY_PROVIDER with FALLBACK_PROVIDER as backup
#   benchmark - Auto-test all configurations (requires BENCHMARK_MODE=true)
WHISPER_ROUTING_STRATEGY=single

# Primary provider name (used with 'single' and 'fallback' strategies)
PRIMARY_PROVIDER=faster-whisper

# Fallback provider name (used with 'fallback' strategy)
FALLBACK_PROVIDER=openai

# Duration threshold for routing (future use with duration-based strategy)
DURATION_THRESHOLD_SECONDS=30

# =============================================================================
# FasterWhisper Configuration (Recommended for CPU)
# =============================================================================

# Model size: tiny, base, small, medium, large-v2, large-v3
# Performance on CPU (approximate):
#   tiny     - ~40 MB,  0.3x realtime,  ~75% quality vs OpenAI
#   base     - ~140 MB, 0.6x realtime,  ~90% quality (GOOD FOR TESTING)
#   small    - ~480 MB, 1.5x realtime,  ~95% quality (RECOMMENDED)
#   medium   - ~1.5 GB, 5.5x realtime,  ~98% quality
#   large-v2 - ~3 GB,   15x realtime,   ~99% quality (SLOW on CPU)
#   large-v3 - ~3 GB,   15x realtime,   ~99% quality (SLOW on CPU)
FASTER_WHISPER_MODEL_SIZE=base

# Device: "cpu" or "cuda" (for NVIDIA GPU)
FASTER_WHISPER_DEVICE=cpu

# Compute type:
#   int8    - Fastest, ~2x faster than float32, minimal quality loss (RECOMMENDED for CPU)
#   float32 - Slower, best quality for CPU
#   float16 - Only for CUDA
FASTER_WHISPER_COMPUTE_TYPE=int8

# Beam size (quality vs speed trade-off):
#   1  - Greedy decoding, fastest, lower quality
#   5  - Default, good balance (RECOMMENDED)
#   10 - Better quality, slower
FASTER_WHISPER_BEAM_SIZE=5

# Voice Activity Detection filter:
#   true  - Filters silence, faster, better for noisy audio (RECOMMENDED)
#   false - Processes everything
FASTER_WHISPER_VAD_FILTER=true

# =============================================================================
# Original Whisper Configuration (Usually slower than FasterWhisper)
# =============================================================================

# Model size: tiny, base, small, medium, large
# Note: Original Whisper is typically SLOWER than faster-whisper on CPU
WHISPER_MODEL_SIZE=base

# Device: "cpu" or "cuda"
WHISPER_DEVICE=cpu

# =============================================================================
# OpenAI API Configuration (Reference Quality)
# =============================================================================

# OpenAI API key (required if using openai provider)
# Get your key from: https://platform.openai.com/api-keys
# Cost: $0.006 per minute of audio
# OPENAI_API_KEY=sk-your_api_key_here

# Model name (currently only whisper-1 available)
OPENAI_MODEL=whisper-1

# API request timeout in seconds
OPENAI_TIMEOUT=60

# =============================================================================
# Benchmark Mode (Model Testing & Comparison)
# =============================================================================

# Enable benchmark mode
# When true, ONE voice message tests ALL configured models automatically
# Generates comprehensive comparison report with quality and speed metrics
# WARNING: Can be expensive with OpenAI API! Use for testing only.
BENCHMARK_MODE=false

# Benchmark configurations are defined in src/config.py
# Default tests include:
#   - FasterWhisper: tiny, base, small, medium (int8, beam5)
#   - FasterWhisper: small (float32, beam10) for quality comparison
#   - Original Whisper: base, small
#   - OpenAI API: whisper-1 (reference quality)

# =============================================================================
# Usage Examples
# =============================================================================

# Example 1: Test FasterWhisper models one by one
# WHISPER_PROVIDERS=["faster-whisper"]
# WHISPER_ROUTING_STRATEGY=single
# PRIMARY_PROVIDER=faster-whisper
# FASTER_WHISPER_MODEL_SIZE=base  # Change to: tiny, small, medium, large-v3

# Example 2: Benchmark mode (auto-test all models)
# BENCHMARK_MODE=true
# WHISPER_PROVIDERS=["faster-whisper", "whisper", "openai"]
# WHISPER_ROUTING_STRATEGY=benchmark
# OPENAI_API_KEY=sk-...

# Example 3: Production with fallback to OpenAI
# WHISPER_PROVIDERS=["faster-whisper", "openai"]
# WHISPER_ROUTING_STRATEGY=fallback
# PRIMARY_PROVIDER=faster-whisper
# FALLBACK_PROVIDER=openai
# FASTER_WHISPER_MODEL_SIZE=small
# OPENAI_API_KEY=sk-...

# Example 4: OpenAI API only (reference quality)
# WHISPER_PROVIDERS=["openai"]
# WHISPER_ROUTING_STRATEGY=single
# PRIMARY_PROVIDER=openai
# OPENAI_API_KEY=sk-...

# =============================================================================
# Database Settings
# =============================================================================

# Database URL
# SQLite (default, recommended for local/small scale):
#   sqlite+aiosqlite:///./data/bot.db
# PostgreSQL (recommended for production):
#   postgresql+asyncpg://user:password@localhost:5432/dbname
DATABASE_URL=sqlite+aiosqlite:///./data/bot.db

# =============================================================================
# Processing Settings
# =============================================================================

# Maximum queue size for pending transcription tasks
# If queue is full, new requests will be rejected
# Default: 100
MAX_QUEUE_SIZE=100

# Maximum number of concurrent transcription workers
# Higher = more parallel processing but more CPU/memory usage
# Recommended: 3 for CPU, 5-10 for GPU
MAX_CONCURRENT_WORKERS=3

# Transcription timeout in seconds
# If transcription takes longer than this, it will be cancelled
# Recommended: 120 for base model, 300 for larger models
TRANSCRIPTION_TIMEOUT=120

# Maximum voice message duration in seconds
# Messages longer than this will be rejected
# Default: 300 (5 minutes)
MAX_VOICE_DURATION_SECONDS=300

# =============================================================================
# Quota Settings (Future Feature)
# =============================================================================

# Default daily quota in seconds (60 seconds = 1 minute)
# Users will be able to transcribe this many seconds per day for free
DEFAULT_DAILY_QUOTA_SECONDS=60

# =============================================================================
# Logging Settings
# =============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
# DEBUG   - detailed information, for debugging (shows all HTTP requests including bot token in URLs)
# INFO    - general information (RECOMMENDED for development)
# WARNING - only warnings and errors (RECOMMENDED for production, hides HTTP logs with bot token)
# ERROR   - only errors
# CRITICAL - only critical errors
#
# Recommended configurations:
# - Local development: LOG_LEVEL=INFO (balanced visibility)
# - Production:        LOG_LEVEL=WARNING (hide sensitive data in HTTP logs)
# - Debugging:         LOG_LEVEL=DEBUG (maximum detail, CAREFUL: exposes bot token in logs)
#
# SECURITY NOTE: DEBUG level shows full HTTP request URLs including bot token
# Use WARNING or higher in production to prevent token leaks in logs
LOG_LEVEL=INFO

# =============================================================================
# Webhook Settings (Only for production with BOT_MODE=webhook)
# =============================================================================

# Public URL where Telegram will send updates
# Example: https://yourdomain.com/webhook
# WEBHOOK_URL=

# Port for webhook server (usually 8443, 443, 80, or 88)
# WEBHOOK_PORT=8443

# Listen address (0.0.0.0 for all interfaces, 127.0.0.1 for localhost only)
# WEBHOOK_LISTEN=0.0.0.0

# Path to SSL certificate (required for custom cert)
# WEBHOOK_CERT_PATH=

# Path to SSL private key (required for custom cert)
# WEBHOOK_KEY_PATH=
