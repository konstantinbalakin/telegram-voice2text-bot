# =============================================================================
# Telegram Voice2Text Bot Configuration
# =============================================================================
# Copy this file to .env and fill in your values:
# cp .env.example .env

# =============================================================================
# Telegram Bot Settings (REQUIRED)
# =============================================================================

# Bot token from @BotFather
# Get your token: https://t.me/BotFather -> /newbot
# Format: 1234567890:ABCdefGHIjklMNOpqrsTUVwxyz
TELEGRAM_BOT_TOKEN=your_bot_token_here

# Bot mode: "polling" for local development, "webhook" for production
BOT_MODE=polling

# =============================================================================
# Transcription Provider Configuration
# =============================================================================

# Enabled providers (comma-separated list in JSON format)
# Options: faster-whisper, openai
# Examples:
#   Single provider:   ["faster-whisper"]
#   Multiple providers: ["faster-whisper", "openai"]
WHISPER_PROVIDERS=["faster-whisper"]

# Routing strategy
# Options:
#   single    - Use one provider (configured in PRIMARY_PROVIDER)
#   fallback  - Use PRIMARY_PROVIDER with FALLBACK_PROVIDER as backup
#   benchmark - Auto-test all configurations (requires BENCHMARK_MODE=true)
WHISPER_ROUTING_STRATEGY=single

# Primary provider name (used with 'single' and 'fallback' strategies)
PRIMARY_PROVIDER=faster-whisper

# Fallback provider name (used with 'fallback' strategy)
FALLBACK_PROVIDER=openai

# Duration threshold for routing (future use with duration-based strategy)
DURATION_THRESHOLD_SECONDS=30

# =============================================================================
# FasterWhisper Configuration (Recommended for CPU)
# =============================================================================

# Model size: tiny, base, small, medium, large-v2, large-v3
#
# Production configuration (based on benchmark results):
#   medium / int8 / beam1 - RTF ~0.3x (3x faster than audio duration)
#   - 7s audio:  ~2s processing
#   - 30s audio: ~10s processing
#   - 60s audio: ~20s processing
#   - Memory: ~2 GB RAM peak (tested in production)
#   - Quality: Excellent for Russian, good for long audio
#
# Alternative configurations:
#   tiny   - ~40 MB,  RTF 0.05x, fast but lower quality
#   base   - ~140 MB, RTF 0.1x,  good for testing
#   small  - ~480 MB, RTF 0.2x,  balanced option
#   medium - ~1.5 GB, RTF 0.3x,  production default ⭐
#   large  - ~3 GB,   RTF 0.5x+, best quality but slow on CPU
FASTER_WHISPER_MODEL_SIZE=medium

# Device: "cpu" or "cuda" (for NVIDIA GPU)
FASTER_WHISPER_DEVICE=cpu

# Compute type:
#   int8    - Fastest, ~2x faster than float32, minimal quality loss (RECOMMENDED for CPU)
#   float32 - Slower, best quality for CPU
#   float16 - Only for CUDA
FASTER_WHISPER_COMPUTE_TYPE=int8

# Beam size (quality vs speed trade-off):
#   1  - Greedy decoding, fastest (PRODUCTION DEFAULT ⭐)
#   5  - Default, better quality
#   10 - Best quality, slower
FASTER_WHISPER_BEAM_SIZE=1

# Voice Activity Detection filter:
#   true  - Filters silence, faster, better for noisy audio (RECOMMENDED)
#   false - Processes everything
FASTER_WHISPER_VAD_FILTER=true

# =============================================================================
# OpenAI API Configuration (Reference Quality)
# =============================================================================

# OpenAI API key (required if using openai provider)
# Get your key from: https://platform.openai.com/api-keys
# Cost: $0.006 per minute of audio
# OPENAI_API_KEY=sk-your_api_key_here

# Model name (currently only whisper-1 available)
OPENAI_MODEL=whisper-1

# API request timeout in seconds
OPENAI_TIMEOUT=60

# =============================================================================
# Benchmark Mode (Model Testing & Comparison)
# =============================================================================

# Enable benchmark mode
# When true, ONE voice message tests ALL configured models automatically
# Generates comprehensive comparison report with quality and speed metrics
# WARNING: Can be expensive with OpenAI API! Use for testing only.
BENCHMARK_MODE=false

# Benchmark configurations are defined in src/config.py
# Default tests:
#   - FasterWhisper: medium/int8/beam1 (production default)
#   - OpenAI API: whisper-1 (reference quality, requires API key)

# =============================================================================
# Usage Examples
# =============================================================================

# Example 1: Production default (recommended)
# WHISPER_PROVIDERS=["faster-whisper"]
# WHISPER_ROUTING_STRATEGY=single
# PRIMARY_PROVIDER=faster-whisper
# FASTER_WHISPER_MODEL_SIZE=medium
# FASTER_WHISPER_COMPUTE_TYPE=int8
# FASTER_WHISPER_BEAM_SIZE=1

# Example 2: Benchmark mode (compare with OpenAI)
# BENCHMARK_MODE=true
# WHISPER_PROVIDERS=["faster-whisper", "openai"]
# WHISPER_ROUTING_STRATEGY=benchmark
# OPENAI_API_KEY=sk-...

# Example 3: Production with fallback to OpenAI
# WHISPER_PROVIDERS=["faster-whisper", "openai"]
# WHISPER_ROUTING_STRATEGY=fallback
# PRIMARY_PROVIDER=faster-whisper
# FALLBACK_PROVIDER=openai
# FASTER_WHISPER_MODEL_SIZE=medium
# OPENAI_API_KEY=sk-...

# Example 4: OpenAI API only (reference quality)
# WHISPER_PROVIDERS=["openai"]
# WHISPER_ROUTING_STRATEGY=single
# PRIMARY_PROVIDER=openai
# OPENAI_API_KEY=sk-...

# =============================================================================
# Database Settings
# =============================================================================

# Database URL
# SQLite (default, recommended for local/small scale):
#   sqlite+aiosqlite:///./data/bot.db
# PostgreSQL (recommended for production):
#   postgresql+asyncpg://user:password@localhost:5432/dbname
DATABASE_URL=sqlite+aiosqlite:///./data/bot.db

# =============================================================================
# Processing Settings
# =============================================================================

# Maximum queue size for pending transcription tasks
# If queue is full, new requests will be rejected
# Production value: 10 (optimized for 1GB RAM VPS, 2025-10-29)
# Development value: 50-100 (more resources available)
MAX_QUEUE_SIZE=100

# Maximum number of concurrent transcription workers
# Higher = more parallel processing but more CPU/memory usage
# Production value: 1 (sequential processing for stability on 1GB RAM / 1 vCPU VPS)
# Development value: 3 (if you have more resources)
# Recommended: 1 for 1 vCPU, 3 for multi-core CPU, 5-10 for GPU
MAX_CONCURRENT_WORKERS=3

# Transcription timeout in seconds
# If transcription takes longer than this, it will be cancelled
# Recommended: 120 for base/medium model, 300 for larger models
TRANSCRIPTION_TIMEOUT=120

# Maximum voice message duration in seconds
# Messages longer than this will be rejected
# Production value: 120 (2 minutes, enforced since 2025-10-29)
# Development value: 300 (5 minutes, if testing longer files)
MAX_VOICE_DURATION_SECONDS=300

# Progress update interval in seconds (Phase 6: Queue System)
# How often to update the progress bar during transcription
# Recommended: 5 seconds (balances UX with Telegram API rate limits)
PROGRESS_UPDATE_INTERVAL=5

# Progress RTF (Real-Time Factor) for estimation (Phase 6: Queue System)
# Used to estimate processing time: processing_time = duration × RTF
# Default: 0.3 (medium model with int8 processes 3x faster than audio duration)
# Adjust based on your actual model performance
PROGRESS_RTF=0.3

# =============================================================================
# Quota Settings (Future Feature)
# =============================================================================

# Default daily quota in seconds (60 seconds = 1 minute)
# Users will be able to transcribe this many seconds per day for free
DEFAULT_DAILY_QUOTA_SECONDS=60

# =============================================================================
# Logging Settings
# =============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
# DEBUG   - detailed information, for debugging (shows all HTTP requests including bot token in URLs)
# INFO    - general information (RECOMMENDED for development)
# WARNING - only warnings and errors (RECOMMENDED for production, hides HTTP logs with bot token)
# ERROR   - only errors
# CRITICAL - only critical errors
#
# Recommended configurations:
# - Local development: LOG_LEVEL=INFO (balanced visibility)
# - Production:        LOG_LEVEL=WARNING (hide sensitive data in HTTP logs)
# - Debugging:         LOG_LEVEL=DEBUG (maximum detail, CAREFUL: exposes bot token in logs)
#
# SECURITY NOTE: DEBUG level shows full HTTP request URLs including bot token
# Use WARNING or higher in production to prevent token leaks in logs
LOG_LEVEL=INFO

# =============================================================================
# Webhook Settings (Only for production with BOT_MODE=webhook)
# =============================================================================

# Public URL where Telegram will send updates
# Example: https://yourdomain.com/webhook
# WEBHOOK_URL=

# Port for webhook server (usually 8443, 443, 80, or 88)
# WEBHOOK_PORT=8443

# Listen address (0.0.0.0 for all interfaces, 127.0.0.1 for localhost only)
# WEBHOOK_LISTEN=0.0.0.0

# Path to SSL certificate (required for custom cert)
# WEBHOOK_CERT_PATH=

# Path to SSL private key (required for custom cert)
# WEBHOOK_KEY_PATH=
