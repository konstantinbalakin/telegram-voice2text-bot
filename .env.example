# =============================================================================
# Telegram Voice2Text Bot Configuration
# =============================================================================
# Copy this file to .env and fill in your values:
# cp .env.example .env

# =============================================================================
# Docker Build Settings (Optional)
# =============================================================================

# Path to corporate CA certificate for building behind SSL proxy
# If your corporate network intercepts SSL traffic (MITM proxy), pip install
# inside Docker will fail with SSL: CERTIFICATE_VERIFY_FAILED.
# Set the full path to your corporate root CA certificate (.pem or .crt).
# Leave empty if not behind a corporate proxy.
# Example: /Users/username/certs/CORPORATE-ROOTCA.pem
CORP_CA_CERT_PATH=

# =============================================================================
# Telegram Bot Settings (REQUIRED)
# =============================================================================

# Bot token from @BotFather
# Get your token: https://t.me/BotFather -> /newbot
# Format: 1234567890:ABCdefGHIjklMNOpqrsTUVwxyz
TELEGRAM_BOT_TOKEN=your_bot_token_here

# Bot mode: "polling" for local development, "webhook" for production
BOT_MODE=polling

# =============================================================================
# Telegram Client API (for Large Files >20 MB) - OPTIONAL
# =============================================================================

# Enable Client API support for files >20 MB (default: false)
# When enabled, bot can download files up to 2 GB using MTProto Client API
# Requires TELEGRAM_API_ID and TELEGRAM_API_HASH from https://my.telegram.org
TELETHON_ENABLED=false

# Telegram API credentials from https://my.telegram.org
# Steps to obtain:
#   1. Login to https://my.telegram.org with your phone number
#   2. Go to "API development tools"
#   3. Create a new application
#   4. Copy api_id and api_hash
# Note: These are different from your bot token!
TELEGRAM_API_ID=12345678
TELEGRAM_API_HASH=your_api_hash_here

# Session file name (will be created as {name}.session)
# Default: bot_client
TELETHON_SESSION_NAME=bot_client

# =============================================================================
# Transcription Provider Configuration
# =============================================================================

# Enabled providers (comma-separated list in JSON format)
# Options: faster-whisper, openai
# Production uses OpenAI for best quality and reliability
# Examples:
#   API-based (production):  ["openai"]
#   Local development:       ["faster-whisper"]
#   Multiple providers:      ["faster-whisper", "openai"]
WHISPER_PROVIDERS=["openai"]

# Routing strategy
# Options:
#   structure - Automatic text structuring with LLM (PRODUCTION DEFAULT â­)
#   single    - Use one provider (configured in PRIMARY_PROVIDER)
#   fallback  - Use PRIMARY_PROVIDER with FALLBACK_PROVIDER as backup
#   hybrid    - Duration-based: short=quality, long=draft+LLM
#   benchmark - Auto-test all configurations (requires BENCHMARK_MODE=true)
WHISPER_ROUTING_STRATEGY=structure

# Primary provider name (used with 'single' and 'fallback' strategies)
PRIMARY_PROVIDER=openai

# Fallback provider name (used with 'fallback' strategy)
FALLBACK_PROVIDER=openai

# Duration threshold for routing (future use with duration-based strategy)
DURATION_THRESHOLD_SECONDS=30

# =============================================================================
# FasterWhisper Configuration (Recommended for CPU)
# =============================================================================

# Model size: tiny, base, small, medium, large-v2, large-v3
#
# Production configuration (based on benchmark results):
#   medium / int8 / beam1 - RTF ~0.3x (3x faster than audio duration)
#   - 7s audio:  ~2s processing
#   - 30s audio: ~10s processing
#   - 60s audio: ~20s processing
#   - Memory: ~2 GB RAM peak (tested in production)
#   - Quality: Excellent for Russian, good for long audio
#
# Alternative configurations:
#   tiny   - ~40 MB,  RTF 0.05x, fast but lower quality
#   base   - ~140 MB, RTF 0.1x,  good for testing
#   small  - ~480 MB, RTF 0.2x,  balanced option
#   medium - ~1.5 GB, RTF 0.3x,  production default â­
#   large  - ~3 GB,   RTF 0.5x+, best quality but slow on CPU
FASTER_WHISPER_MODEL_SIZE=medium

# Device: "cpu" or "cuda" (for NVIDIA GPU)
FASTER_WHISPER_DEVICE=cpu

# Compute type:
#   int8    - Fastest, ~2x faster than float32, minimal quality loss (RECOMMENDED for CPU)
#   float32 - Slower, best quality for CPU
#   float16 - Only for CUDA
FASTER_WHISPER_COMPUTE_TYPE=int8

# Beam size (quality vs speed trade-off):
#   1  - Greedy decoding, fastest (PRODUCTION DEFAULT â­)
#   5  - Default, better quality
#   10 - Best quality, slower
FASTER_WHISPER_BEAM_SIZE=1

# Voice Activity Detection filter:
#   true  - Filters silence, faster, better for noisy audio (RECOMMENDED)
#   false - Processes everything
FASTER_WHISPER_VAD_FILTER=true

# =============================================================================
# OpenAI API Configuration (REQUIRED for Production)
# =============================================================================

# OpenAI API key - REQUIRED for transcription
# Get your key from: https://platform.openai.com/api-keys
# Production cost: $0.006 per minute (whisper-1) or $0.012 (gpt-4o models)
OPENAI_API_KEY=sk-your_api_key_here

# Model name
# Options:
#   whisper-1             - Legacy model, supports OGA format, $0.006/min
#   gpt-4o-transcribe     - New model (2024), requires MP3/WAV, better quality
#   gpt-4o-mini-transcribe - New mini model (2024), requires MP3/WAV, faster
# Note: New models (gpt-4o-*) require audio format conversion for OGA files
OPENAI_MODEL=whisper-1

# API request timeout in seconds
OPENAI_TIMEOUT=60

# Preferred audio format for gpt-4o-transcribe and gpt-4o-mini-transcribe models
# Options:
#   mp3 - Recommended, good compression (64kbps for speech), smaller files
#   wav - Lossless quality, larger files, avoids double compression
# Note: Only applies to new gpt-4o-* models. whisper-1 supports OGA natively.
# The bot will automatically convert OGA files to the preferred format when needed.
OPENAI_4O_TRANSCRIBE_PREFERRED_FORMAT=mp3

# =============================================================================
# OpenAI Long Audio Handling
# =============================================================================

# Maximum audio duration for gpt-4o models before special handling
# gpt-4o-transcribe/mini have a limit of ~1400-1500 seconds (23-25 minutes)
# If audio exceeds this limit, use one of the strategies below
OPENAI_GPT4O_MAX_DURATION=1400

# Automatically switch to whisper-1 for audio exceeding max duration
# whisper-1 has no duration limit (only 25MB file size limit)
# true  - Switch to whisper-1 automatically (recommended, default)
# false - Use only the configured model (may fail for long audio)
OPENAI_CHANGE_MODEL=true

# Enable audio chunking for long files
# Splits audio into segments and transcribes separately
# true  - Split and transcribe in chunks (preserves gpt-4o quality)
# false - Don't split (default, simpler approach)
# Note: Requires pydub library
OPENAI_CHUNKING=false

# Size of each audio chunk in seconds (5-23 minutes recommended)
# Must be less than OPENAI_GPT4O_MAX_DURATION
# Recommended: 1200 (20 minutes) - safe buffer below limit
OPENAI_CHUNK_SIZE_SECONDS=1200

# Overlap between chunks in seconds for better context
# Helps preserve context at chunk boundaries
# Recommended: 2 seconds (prevents word cutoff)
# Range: 0-10 seconds
OPENAI_CHUNK_OVERLAP_SECONDS=2

# Process chunks in parallel for faster transcription
# true  - Faster (2-3x) but loses context between chunks
# false - Slower but preserves context via prompt parameter
# Recommended: true (speed matters more than context for most use cases)
OPENAI_PARALLEL_CHUNKS=true

# Maximum number of chunks to process simultaneously
# Helps prevent rate limiting (429 errors)
# Recommended: 3 (balance between speed and rate limits)
# Range: 1-10
OPENAI_MAX_PARALLEL_CHUNKS=3

# =============================================================================
# OpenAI Long Audio Handling - Usage Examples
# =============================================================================

# Scenario 1: Fast handling (default)
# Audio >23min automatically uses whisper-1
# OPENAI_GPT4O_MAX_DURATION=1400
# OPENAI_CHANGE_MODEL=true
# OPENAI_CHUNKING=false
#
# Result: 30min file â†’ whisper-1 (lower quality, but fast)

# Scenario 2: Maximum quality with parallelization
# Split audio and use gpt-4o-transcribe for all chunks
# OPENAI_CHANGE_MODEL=false
# OPENAI_CHUNKING=true
# OPENAI_PARALLEL_CHUNKS=true
# OPENAI_MAX_PARALLEL_CHUNKS=3
#
# Result: 30min file â†’ Split into 2 chunks â†’ Process in parallel
#         Time: ~10min for 30min audio
#         Quality: High (gpt-4o)

# Scenario 3: Maximum quality with context preservation
# Sequential processing preserves context between chunks
# OPENAI_CHUNKING=true
# OPENAI_PARALLEL_CHUNKS=false
# OPENAI_CHANGE_MODEL=false
#
# Result: 30min file â†’ Split into 2 chunks â†’ Process sequentially
#         Time: ~30min for 30min audio
#         Quality: Maximum (gpt-4o + context)

# Scenario 4: Balanced (recommended for production)
# Fast parallel processing with whisper-1 fallback
# OPENAI_CHUNKING=true
# OPENAI_PARALLEL_CHUNKS=true
# OPENAI_CHANGE_MODEL=true
#
# Result: Fast processing with automatic fallback if needed

# =============================================================================
# Benchmark Mode (Model Testing & Comparison)
# =============================================================================

# Enable benchmark mode
# When true, ONE voice message tests ALL configured models automatically
# Generates comprehensive comparison report with quality and speed metrics
# WARNING: Can be expensive with OpenAI API! Use for testing only.
BENCHMARK_MODE=false

# Benchmark configurations are defined in src/config.py
# Default tests:
#   - FasterWhisper: medium/int8/beam1 (production default)
#   - OpenAI API: whisper-1 (reference quality, requires API key)

# =============================================================================
# Usage Examples
# =============================================================================

# Example 1: Production default (recommended)
# WHISPER_PROVIDERS=["faster-whisper"]
# WHISPER_ROUTING_STRATEGY=single
# PRIMARY_PROVIDER=faster-whisper
# FASTER_WHISPER_MODEL_SIZE=medium
# FASTER_WHISPER_COMPUTE_TYPE=int8
# FASTER_WHISPER_BEAM_SIZE=1

# Example 2: Benchmark mode (compare with OpenAI)
# BENCHMARK_MODE=true
# WHISPER_PROVIDERS=["faster-whisper", "openai"]
# WHISPER_ROUTING_STRATEGY=benchmark
# OPENAI_API_KEY=sk-...

# Example 3: Production with fallback to OpenAI
# WHISPER_PROVIDERS=["faster-whisper", "openai"]
# WHISPER_ROUTING_STRATEGY=fallback
# PRIMARY_PROVIDER=faster-whisper
# FALLBACK_PROVIDER=openai
# FASTER_WHISPER_MODEL_SIZE=medium
# OPENAI_API_KEY=sk-...

# Example 4: OpenAI API only (reference quality)
# WHISPER_PROVIDERS=["openai"]
# WHISPER_ROUTING_STRATEGY=single
# PRIMARY_PROVIDER=openai
# OPENAI_API_KEY=sk-...

# =============================================================================
# Database Settings
# =============================================================================

# Database URL
# SQLite (default, recommended for local/small scale):
#   sqlite+aiosqlite:///./data/bot.db
# PostgreSQL (recommended for production):
#   postgresql+asyncpg://user:password@localhost:5432/dbname
DATABASE_URL=sqlite+aiosqlite:///./data/bot.db

# =============================================================================
# Processing Settings
# =============================================================================

# Maximum queue size for pending transcription tasks
# If queue is full, new requests will be rejected
# Production value: 10 (optimized for 1GB RAM VPS, 2025-10-29)
# Development value: 50-100 (more resources available)
MAX_QUEUE_SIZE=100

# Maximum number of concurrent transcription workers
# Higher = more parallel processing but more CPU/memory usage
# Production value: 1 (sequential processing for stability on 1GB RAM / 1 vCPU VPS)
# Development value: 3 (if you have more resources)
# Recommended: 1 for 1 vCPU, 3 for multi-core CPU, 5-10 for GPU
MAX_CONCURRENT_WORKERS=3

# Transcription timeout in seconds
# If transcription takes longer than this, it will be cancelled
# Recommended: 120 for base/medium model, 300 for larger models
TRANSCRIPTION_TIMEOUT=120

# Maximum voice message duration in seconds
# Messages longer than this will be rejected
# Production value: 120 (2 minutes, enforced since 2025-10-29)
# Development value: 300 (5 minutes, if testing longer files)
MAX_VOICE_DURATION_SECONDS=300

# Progress update interval in seconds (Phase 6: Queue System)
# How often to update the progress bar during transcription
# Recommended: 5 seconds (balances UX with Telegram API rate limits)
PROGRESS_UPDATE_INTERVAL=5

# Progress RTF (Real-Time Factor) for estimation (Phase 6: Queue System)
# Used to estimate processing time: processing_time = duration Ã— RTF
# Default: 0.3 (medium model with int8 processes 3x faster than audio duration)
# Adjust based on your actual model performance
PROGRESS_RTF=0.3

# LLM processing duration for progress bar (Phase 10: Interactive Transcription)
# Estimated duration in seconds for LLM text processing (structuring, summarization)
# Used to show progress bar with realistic time estimates
# Default: 30 seconds (typical for structured mode with moderate text length)
LLM_PROCESSING_DURATION=30

# =============================================================================
# Quota Settings (Future Feature)
# =============================================================================

# Default daily quota in seconds (60 seconds = 1 minute)
# Users will be able to transcribe this many seconds per day for free
DEFAULT_DAILY_QUOTA_SECONDS=60

# =============================================================================
# Logging Settings
# =============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
# DEBUG   - detailed information, for debugging (shows all HTTP requests including bot token in URLs)
# INFO    - general information (RECOMMENDED for development)
# WARNING - only warnings and errors (RECOMMENDED for production, hides HTTP logs with bot token)
# ERROR   - only errors
# CRITICAL - only critical errors
#
# Recommended configurations:
# - Local development: LOG_LEVEL=INFO (balanced visibility)
# - Production:        LOG_LEVEL=WARNING (hide sensitive data in HTTP logs)
# - Debugging:         LOG_LEVEL=DEBUG (maximum detail, CAREFUL: exposes bot token in logs)
#
# SECURITY NOTE: DEBUG level shows full HTTP request URLs including bot token
# Use WARNING or higher in production to prevent token leaks in logs
LOG_LEVEL=INFO

# =============================================================================
# DEBUG Mode Details
# =============================================================================
#
# When LOG_LEVEL=DEBUG is set, the bot logs extensive information for debugging:
#
# 1. SQL Queries:
#    - All database queries (SELECT, INSERT, UPDATE)
#    - Query parameters and results
#    - Connection pool activity
#
# 2. Bot Handlers:
#    - Command invocations with user IDs
#    - Voice message metadata (file_id, duration, size)
#    - Queue position and depth
#
# 3. Storage Operations:
#    - User creation/lookup with telegram_id
#    - Usage record creation and updates
#    - Database field values before/after updates
#
# 4. Queue Management:
#    - Request enqueue with position and duration
#    - Semaphore waits and processing starts
#    - Wait time calculations with RTF
#    - Queue depth and pending counts
#
# 5. Audio Processing:
#    - File download details (size, path, extension)
#    - Preprocessing steps (mono conversion, speed adjustment)
#    - Output file paths
#
# 6. Transcription:
#    - Model initialization timing
#    - Transcription requests (audio path, language, timeout)
#    - Memory usage before/after
#    - Segment counts and detailed results
#
# 7. API Calls (OpenAI, DeepSeek):
#    - Request parameters (model, file size)
#    - API keys (masked: first 8 chars + "...")
#    - Response metadata (text length, tokens)
#    - Retry attempts
#
# 8. LLM Refinement:
#    - Draft/refined text lengths
#    - Token usage (prompt/completion/total)
#    - API endpoints
#
# Log Files (DEBUG mode creates additional file):
# - logs/app.log     - All INFO+ logs (10MB, 5 backups)
# - logs/errors.log  - ERROR+ only (5MB, 5 backups)
# - logs/debug.log   - DEBUG+ logs (5MB, 3 backups) [ONLY in DEBUG mode]
# - logs/deployments.jsonl - Deployment events
#
# Usage:
#   1. Set LOG_LEVEL=DEBUG in .env
#   2. Restart bot: docker-compose restart (or ctrl+C and python -m src.bot)
#   3. Check logs: tail -f logs/debug.log
#   4. Analyze: cat logs/debug.log | jq '.' (requires jq for JSON parsing)
#
# IMPORTANT:
# - DEBUG mode significantly increases log file sizes
# - Contains sensitive data (API keys are masked but queries/responses are logged)
# - Do NOT use in production
# - Useful for local debugging and understanding execution flow

# =============================================================================
# Webhook Settings (Only for production with BOT_MODE=webhook)
# =============================================================================

# Public URL where Telegram will send updates
# Example: https://yourdomain.com/webhook
# WEBHOOK_URL=

# Port for webhook server (usually 8443, 443, 80, or 88)
# WEBHOOK_PORT=8443

# Listen address (0.0.0.0 for all interfaces, 127.0.0.1 for localhost only)
# WEBHOOK_LISTEN=0.0.0.0

# Path to SSL certificate (required for custom cert)
# WEBHOOK_CERT_PATH=

# Path to SSL private key (required for custom cert)
# WEBHOOK_KEY_PATH=

# =============================================================================
# Hybrid Transcription Strategy (NEW - Phase 8)
# =============================================================================

# Enable hybrid strategy for fast feedback + quality improvement
# Strategy options: single, fallback, benchmark, hybrid, structure
#
# Hybrid mode:
#   - Short audio (<threshold): Use quality model directly (medium)
#   - Long audio (>=threshold): Fast draft (small) â†’ LLM refinement
#
# Benefits:
#   - 6x-9x faster for long audio (60s: 36s â†’ 6s)
#   - Immediate feedback with draft
#   - Improved quality with LLM refinement
WHISPER_ROUTING_STRATEGY=single

# Duration threshold for hybrid strategy (seconds)
# Audio < threshold: quality model (medium)
# Audio >= threshold: draft model (small) + LLM refinement
# Recommended: 20 seconds
HYBRID_SHORT_THRESHOLD=20

# Draft provider for long audio
# Options: faster-whisper, openai
# Recommended: faster-whisper (free, fast)
HYBRID_DRAFT_PROVIDER=faster-whisper

# Draft model (for faster-whisper)
# Options: tiny, small, base
# Recommended: small (good balance of speed/quality, RTF ~0.2x)
# Alternative: tiny (faster RTF ~0.05x, lower quality)
HYBRID_DRAFT_MODEL=small

# Quality provider (for short audio)
# Usually same as main provider
HYBRID_QUALITY_PROVIDER=faster-whisper

# Quality model
# Recommended: medium (current production default)
HYBRID_QUALITY_MODEL=medium

# -------------------------------------------
# Structure Strategy (WHISPER_ROUTING_STRATEGY=structure)
# -------------------------------------------
# Automatically structures transcription with LLM formatting
# - Short audio (<threshold): Transcribe â†’ Structure â†’ Show result
# - Long audio (â‰¥threshold): Transcribe â†’ Show draft â†’ Structure â†’ Show result
# - Saves both original and structured variants to database
# - Requires LLM_REFINEMENT_ENABLED=true

# Provider for structure strategy
# Options: faster-whisper, openai
# Recommended: faster-whisper (free, quality)
STRUCTURE_PROVIDER=faster-whisper

# Model for structure strategy transcription
# Recommended: medium (production default)
STRUCTURE_MODEL=medium

# Duration threshold (seconds) for showing draft before structuring
# Audio < threshold: No draft, direct structuring
# Audio >= threshold: Show draft first, then structure
# Recommended: 20 seconds
STRUCTURE_DRAFT_THRESHOLD=20

# Emoji level for structured text
# 0 = No emojis
# 1 = Few emojis (default, balanced)
# 2 = Moderate emojis
# 3 = Many emojis (very expressive)
STRUCTURE_EMOJI_LEVEL=1

# =============================================================================
# LLM Text Processing (REQUIRED for Production)
# =============================================================================

# Enable LLM for intelligent text processing - REQUIRED for structure strategy
# Powers: Automatic structuring, "Make it beautiful", Summarization
# Used in production for all interactive features
LLM_REFINEMENT_ENABLED=true

# LLM provider
# Options: deepseek (PRODUCTION â­), openai, gigachat
# DeepSeek V3: Excellent quality at ~$0.0002 per 60s audio (30x cheaper than OpenAI)
LLM_PROVIDER=deepseek

# DeepSeek API key - REQUIRED for production features
# Get from: https://platform.deepseek.com/api_keys
# Free tier: $5 credit, then pay-as-you-go (~$0.0002 per structured text)
LLM_API_KEY=sk-your-deepseek-api-key-here

# LLM model name
# For DeepSeek:
#   deepseek-chat     - V3 model, max 8192 output tokens, general purpose
#   deepseek-reasoner - Reasoning model, max 64K output tokens, better for long texts
LLM_MODEL=deepseek-chat

# LLM API base URL
LLM_BASE_URL=https://api.deepseek.com

# Max output tokens for deepseek-reasoner model
# Only used when LLM_MODEL=deepseek-reasoner
# Default: 64000 (model maximum is 65536)
LLM_MAX_TOKENS_REASONER=64000

# System prompt for text refinement
# Note: Refinement prompt is stored in prompts/refinement.md
# Edit that file to customize refinement behavior

# LLM request timeout (seconds)
# Typical response time: 2-5 seconds
LLM_TIMEOUT=30

# LLM Debug Mode (for quality testing)
# When true, sends an additional message comparing draft and refined text
# Useful for evaluating LLM refinement quality
# Format: Shows both draft (from Whisper) and refined (from LLM) side-by-side
# WARNING: Only use for testing! Sends extra messages to users.
# Default: false (disabled)
LLM_DEBUG_MODE=false

# =============================================================================
# Interactive Transcription Features (Phase 10) - PRODUCTION ENABLED
# =============================================================================

# Enable interactive mode with inline buttons - ENABLED in production
# Shows buttons after transcription for text processing options
INTERACTIVE_MODE_ENABLED=true

# Enable "ðŸ“ Ð¡Ñ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ" button (PRODUCTION â­)
# Automatically structures text with paragraphs, headings, and lists
# Requires: LLM_REFINEMENT_ENABLED=true
ENABLE_STRUCTURED_MODE=true

# Enable "ðŸª„ Ð¡Ð´ÐµÐ»Ð°Ñ‚ÑŒ ÐºÑ€Ð°ÑÐ¸Ð²Ð¾" button (PRODUCTION â­)
# Transforms transcription into publication-ready text while preserving author's voice
# Requires: LLM_REFINEMENT_ENABLED=true
ENABLE_MAGIC_MODE=true

# Enable "ðŸ“‹ Ðž Ñ‡ÐµÐ¼ ÑÑ‚Ð¾Ñ‚ Ñ‚ÐµÐºÑÑ‚" button (PRODUCTION â­)
# Creates concise summary with key points
# Requires: LLM_REFINEMENT_ENABLED=true
ENABLE_SUMMARY_MODE=true

# Enable length variations (shorter/longer)
# Allows users to adjust text length with dynamic buttons
# 5 levels: shorter, short, default, long, longer
# Optional feature, disabled by default
ENABLE_LENGTH_VARIATIONS=false

# Enable emoji option
# Adds emojis to text (4 levels: 0=none, 1=few, 2=moderate, 3=many)
# Optional feature, disabled by default
ENABLE_EMOJI_OPTION=false

# Enable timestamps option
# Shows timestamps for audio segments (only for >5min audio)
# Optional feature, disabled by default
ENABLE_TIMESTAMPS_OPTION=false

# Enable retranscribe option
# Allows users to retranscribe with better quality models
# Optional feature, disabled by default
ENABLE_RETRANSCRIBE=false

# Enable download button for exporting transcriptions
# Allows users to download transcription as MD, TXT, PDF, DOCX
# Optional feature, disabled by default
ENABLE_DOWNLOAD_BUTTON=false

# Maximum cached variants per transcription
# Limits memory usage for generated text variations
# Recommended: 10 (enough for typical user interactions)
MAX_CACHED_VARIANTS_PER_TRANSCRIPTION=10

# Variant cache TTL (days)
# How long to keep generated variants in database
# Recommended: 7 days (balance between UX and storage)
VARIANT_CACHE_TTL_DAYS=7

# Minimum audio duration for timestamps (seconds)
# Only show timestamp option for audio longer than this
# Recommended: 300 (5 minutes)
TIMESTAMPS_MIN_DURATION=300

# =============================================================================
# Interactive Mode Usage Examples
# =============================================================================

# Example 1: Basic Interactive Mode (Phase 1-2)
# Enable interactive buttons with structured mode only
# INTERACTIVE_MODE_ENABLED=true
# ENABLE_STRUCTURED_MODE=true
# LLM_REFINEMENT_ENABLED=true
# LLM_API_KEY=sk-your-key

# Example 2: Full Interactive (Phase 1-3)
# Add length variations to structured mode
# INTERACTIVE_MODE_ENABLED=true
# ENABLE_STRUCTURED_MODE=true
# ENABLE_LENGTH_VARIATIONS=true
# LLM_REFINEMENT_ENABLED=true

# Example 3: Complete Feature Set (Phase 1-6)
# All interactive features enabled
# INTERACTIVE_MODE_ENABLED=true
# ENABLE_STRUCTURED_MODE=true
# ENABLE_LENGTH_VARIATIONS=true
# ENABLE_SUMMARY_MODE=true
# ENABLE_EMOJI_OPTION=true
# ENABLE_TIMESTAMPS_OPTION=true
# LLM_REFINEMENT_ENABLED=true

# =============================================================================
# Audio Preprocessing (NEW - Phase 8)
# =============================================================================

# Convert audio to mono before transcription
# Pros: Faster processing, smaller files, ~5-10% speed improvement
# Cons: May reduce quality for stereo recordings
# Recommended: false (test first with your audio)
AUDIO_CONVERT_TO_MONO=false

# Target sample rate for mono conversion (Hz)
# 16000 Hz is sufficient for speech recognition
# Lower rate = faster processing, smaller files
AUDIO_TARGET_SAMPLE_RATE=16000

# Audio speed multiplier
# 1.0 = original speed (no change)
# 1.5 = 50% faster (recommended after testing)
# 2.0 = 2x faster (may reduce quality)
# Range: 0.5-2.0
#
# Note: This speeds up the audio for transcription, not the actual voice
# Whisper model processes sped-up audio faster
# Test quality impact before using in production
AUDIO_SPEED_MULTIPLIER=1.0

# =============================================================================
# Document and Video Support
# =============================================================================

# Enable processing of audio files sent as documents
# Supports: .aac, .flac, .wma, .amr, .webm, .3gp and other audio formats
# When users send audio files as documents (not via audio message), bot will process them
# Default: true
ENABLE_DOCUMENT_HANDLER=true

# Enable processing of video files with audio extraction
# Bot will extract audio track from video and transcribe it
# Supports: .mp4, .mkv, .avi, .mov and other video formats
# Requires ffmpeg installed
# Default: true
ENABLE_VIDEO_HANDLER=true

# =============================================================================
# Hybrid Mode Usage Examples
# =============================================================================

# Example 1: Conservative (Start Here)
# Test hybrid strategy without LLM refinement first
# WHISPER_ROUTING_STRATEGY=hybrid
# HYBRID_SHORT_THRESHOLD=20
# HYBRID_DRAFT_MODEL=small
# LLM_REFINEMENT_ENABLED=false
# AUDIO_SPEED_MULTIPLIER=1.0

# Example 2: Full Hybrid (After Testing)
# Fast draft + LLM refinement + speed boost
# WHISPER_ROUTING_STRATEGY=hybrid
# HYBRID_SHORT_THRESHOLD=20
# HYBRID_DRAFT_MODEL=small
# LLM_REFINEMENT_ENABLED=true
# LLM_API_KEY=sk-your-key
# LLM_DEBUG_MODE=true  # Enable to see draft vs refined comparison
# AUDIO_SPEED_MULTIPLIER=1.5

# Example 3: Aggressive Optimization
# Maximum speed with all optimizations
# WHISPER_ROUTING_STRATEGY=hybrid
# HYBRID_DRAFT_MODEL=small
# LLM_REFINEMENT_ENABLED=true
# AUDIO_SPEED_MULTIPLIER=1.5
# AUDIO_CONVERT_TO_MONO=true

# Example 4: Back to Original (Disable Hybrid)
# Return to single strategy if needed
# WHISPER_ROUTING_STRATEGY=single
# PRIMARY_PROVIDER=faster-whisper
# FASTER_WHISPER_MODEL_SIZE=medium
