# =============================================================================
# Telegram Voice2Text Bot Configuration
# =============================================================================
# Copy this file to .env and fill in your values:
# cp .env.example .env

# =============================================================================
# Telegram Bot Settings (REQUIRED)
# =============================================================================

# Bot token from @BotFather
# Get your token: https://t.me/BotFather -> /newbot
# Format: 1234567890:ABCdefGHIjklMNOpqrsTUVwxyz
TELEGRAM_BOT_TOKEN=your_bot_token_here

# Bot mode: "polling" for local development, "webhook" for production
BOT_MODE=polling

# =============================================================================
# Telegram Client API (for Large Files >20 MB) - OPTIONAL
# =============================================================================

# Enable Client API support for files >20 MB (default: false)
# When enabled, bot can download files up to 2 GB using MTProto Client API
# Requires TELEGRAM_API_ID and TELEGRAM_API_HASH from https://my.telegram.org
TELETHON_ENABLED=false

# Telegram API credentials from https://my.telegram.org
# Steps to obtain:
#   1. Login to https://my.telegram.org with your phone number
#   2. Go to "API development tools"
#   3. Create a new application
#   4. Copy api_id and api_hash
# Note: These are different from your bot token!
TELEGRAM_API_ID=12345678
TELEGRAM_API_HASH=your_api_hash_here

# Session file name (will be created as {name}.session)
# Default: bot_client
TELETHON_SESSION_NAME=bot_client

# =============================================================================
# Transcription Provider Configuration
# =============================================================================

# Enabled providers (comma-separated list in JSON format)
# Options: faster-whisper, openai
# Examples:
#   Single provider:   ["faster-whisper"]
#   Multiple providers: ["faster-whisper", "openai"]
WHISPER_PROVIDERS=["faster-whisper"]

# Routing strategy
# Options:
#   single    - Use one provider (configured in PRIMARY_PROVIDER)
#   fallback  - Use PRIMARY_PROVIDER with FALLBACK_PROVIDER as backup
#   benchmark - Auto-test all configurations (requires BENCHMARK_MODE=true)
WHISPER_ROUTING_STRATEGY=single

# Primary provider name (used with 'single' and 'fallback' strategies)
PRIMARY_PROVIDER=faster-whisper

# Fallback provider name (used with 'fallback' strategy)
FALLBACK_PROVIDER=openai

# Duration threshold for routing (future use with duration-based strategy)
DURATION_THRESHOLD_SECONDS=30

# =============================================================================
# FasterWhisper Configuration (Recommended for CPU)
# =============================================================================

# Model size: tiny, base, small, medium, large-v2, large-v3
#
# Production configuration (based on benchmark results):
#   medium / int8 / beam1 - RTF ~0.3x (3x faster than audio duration)
#   - 7s audio:  ~2s processing
#   - 30s audio: ~10s processing
#   - 60s audio: ~20s processing
#   - Memory: ~2 GB RAM peak (tested in production)
#   - Quality: Excellent for Russian, good for long audio
#
# Alternative configurations:
#   tiny   - ~40 MB,  RTF 0.05x, fast but lower quality
#   base   - ~140 MB, RTF 0.1x,  good for testing
#   small  - ~480 MB, RTF 0.2x,  balanced option
#   medium - ~1.5 GB, RTF 0.3x,  production default ⭐
#   large  - ~3 GB,   RTF 0.5x+, best quality but slow on CPU
FASTER_WHISPER_MODEL_SIZE=medium

# Device: "cpu" or "cuda" (for NVIDIA GPU)
FASTER_WHISPER_DEVICE=cpu

# Compute type:
#   int8    - Fastest, ~2x faster than float32, minimal quality loss (RECOMMENDED for CPU)
#   float32 - Slower, best quality for CPU
#   float16 - Only for CUDA
FASTER_WHISPER_COMPUTE_TYPE=int8

# Beam size (quality vs speed trade-off):
#   1  - Greedy decoding, fastest (PRODUCTION DEFAULT ⭐)
#   5  - Default, better quality
#   10 - Best quality, slower
FASTER_WHISPER_BEAM_SIZE=1

# Voice Activity Detection filter:
#   true  - Filters silence, faster, better for noisy audio (RECOMMENDED)
#   false - Processes everything
FASTER_WHISPER_VAD_FILTER=true

# =============================================================================
# OpenAI API Configuration (Reference Quality)
# =============================================================================

# OpenAI API key (required if using openai provider)
# Get your key from: https://platform.openai.com/api-keys
# Cost: $0.006 per minute of audio
# OPENAI_API_KEY=sk-your_api_key_here

# Model name (currently only whisper-1 available)
OPENAI_MODEL=whisper-1

# API request timeout in seconds
OPENAI_TIMEOUT=60

# =============================================================================
# Benchmark Mode (Model Testing & Comparison)
# =============================================================================

# Enable benchmark mode
# When true, ONE voice message tests ALL configured models automatically
# Generates comprehensive comparison report with quality and speed metrics
# WARNING: Can be expensive with OpenAI API! Use for testing only.
BENCHMARK_MODE=false

# Benchmark configurations are defined in src/config.py
# Default tests:
#   - FasterWhisper: medium/int8/beam1 (production default)
#   - OpenAI API: whisper-1 (reference quality, requires API key)

# =============================================================================
# Usage Examples
# =============================================================================

# Example 1: Production default (recommended)
# WHISPER_PROVIDERS=["faster-whisper"]
# WHISPER_ROUTING_STRATEGY=single
# PRIMARY_PROVIDER=faster-whisper
# FASTER_WHISPER_MODEL_SIZE=medium
# FASTER_WHISPER_COMPUTE_TYPE=int8
# FASTER_WHISPER_BEAM_SIZE=1

# Example 2: Benchmark mode (compare with OpenAI)
# BENCHMARK_MODE=true
# WHISPER_PROVIDERS=["faster-whisper", "openai"]
# WHISPER_ROUTING_STRATEGY=benchmark
# OPENAI_API_KEY=sk-...

# Example 3: Production with fallback to OpenAI
# WHISPER_PROVIDERS=["faster-whisper", "openai"]
# WHISPER_ROUTING_STRATEGY=fallback
# PRIMARY_PROVIDER=faster-whisper
# FALLBACK_PROVIDER=openai
# FASTER_WHISPER_MODEL_SIZE=medium
# OPENAI_API_KEY=sk-...

# Example 4: OpenAI API only (reference quality)
# WHISPER_PROVIDERS=["openai"]
# WHISPER_ROUTING_STRATEGY=single
# PRIMARY_PROVIDER=openai
# OPENAI_API_KEY=sk-...

# =============================================================================
# Database Settings
# =============================================================================

# Database URL
# SQLite (default, recommended for local/small scale):
#   sqlite+aiosqlite:///./data/bot.db
# PostgreSQL (recommended for production):
#   postgresql+asyncpg://user:password@localhost:5432/dbname
DATABASE_URL=sqlite+aiosqlite:///./data/bot.db

# =============================================================================
# Processing Settings
# =============================================================================

# Maximum queue size for pending transcription tasks
# If queue is full, new requests will be rejected
# Production value: 10 (optimized for 1GB RAM VPS, 2025-10-29)
# Development value: 50-100 (more resources available)
MAX_QUEUE_SIZE=100

# Maximum number of concurrent transcription workers
# Higher = more parallel processing but more CPU/memory usage
# Production value: 1 (sequential processing for stability on 1GB RAM / 1 vCPU VPS)
# Development value: 3 (if you have more resources)
# Recommended: 1 for 1 vCPU, 3 for multi-core CPU, 5-10 for GPU
MAX_CONCURRENT_WORKERS=3

# Transcription timeout in seconds
# If transcription takes longer than this, it will be cancelled
# Recommended: 120 for base/medium model, 300 for larger models
TRANSCRIPTION_TIMEOUT=120

# Maximum voice message duration in seconds
# Messages longer than this will be rejected
# Production value: 120 (2 minutes, enforced since 2025-10-29)
# Development value: 300 (5 minutes, if testing longer files)
MAX_VOICE_DURATION_SECONDS=300

# Progress update interval in seconds (Phase 6: Queue System)
# How often to update the progress bar during transcription
# Recommended: 5 seconds (balances UX with Telegram API rate limits)
PROGRESS_UPDATE_INTERVAL=5

# Progress RTF (Real-Time Factor) for estimation (Phase 6: Queue System)
# Used to estimate processing time: processing_time = duration × RTF
# Default: 0.3 (medium model with int8 processes 3x faster than audio duration)
# Adjust based on your actual model performance
PROGRESS_RTF=0.3

# LLM processing duration for progress bar (Phase 10: Interactive Transcription)
# Estimated duration in seconds for LLM text processing (structuring, summarization)
# Used to show progress bar with realistic time estimates
# Default: 30 seconds (typical for structured mode with moderate text length)
LLM_PROCESSING_DURATION=30

# =============================================================================
# Quota Settings (Future Feature)
# =============================================================================

# Default daily quota in seconds (60 seconds = 1 minute)
# Users will be able to transcribe this many seconds per day for free
DEFAULT_DAILY_QUOTA_SECONDS=60

# =============================================================================
# Logging Settings
# =============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
# DEBUG   - detailed information, for debugging (shows all HTTP requests including bot token in URLs)
# INFO    - general information (RECOMMENDED for development)
# WARNING - only warnings and errors (RECOMMENDED for production, hides HTTP logs with bot token)
# ERROR   - only errors
# CRITICAL - only critical errors
#
# Recommended configurations:
# - Local development: LOG_LEVEL=INFO (balanced visibility)
# - Production:        LOG_LEVEL=WARNING (hide sensitive data in HTTP logs)
# - Debugging:         LOG_LEVEL=DEBUG (maximum detail, CAREFUL: exposes bot token in logs)
#
# SECURITY NOTE: DEBUG level shows full HTTP request URLs including bot token
# Use WARNING or higher in production to prevent token leaks in logs
LOG_LEVEL=INFO

# =============================================================================
# DEBUG Mode Details
# =============================================================================
#
# When LOG_LEVEL=DEBUG is set, the bot logs extensive information for debugging:
#
# 1. SQL Queries:
#    - All database queries (SELECT, INSERT, UPDATE)
#    - Query parameters and results
#    - Connection pool activity
#
# 2. Bot Handlers:
#    - Command invocations with user IDs
#    - Voice message metadata (file_id, duration, size)
#    - Queue position and depth
#
# 3. Storage Operations:
#    - User creation/lookup with telegram_id
#    - Usage record creation and updates
#    - Database field values before/after updates
#
# 4. Queue Management:
#    - Request enqueue with position and duration
#    - Semaphore waits and processing starts
#    - Wait time calculations with RTF
#    - Queue depth and pending counts
#
# 5. Audio Processing:
#    - File download details (size, path, extension)
#    - Preprocessing steps (mono conversion, speed adjustment)
#    - Output file paths
#
# 6. Transcription:
#    - Model initialization timing
#    - Transcription requests (audio path, language, timeout)
#    - Memory usage before/after
#    - Segment counts and detailed results
#
# 7. API Calls (OpenAI, DeepSeek):
#    - Request parameters (model, file size)
#    - API keys (masked: first 8 chars + "...")
#    - Response metadata (text length, tokens)
#    - Retry attempts
#
# 8. LLM Refinement:
#    - Draft/refined text lengths
#    - Token usage (prompt/completion/total)
#    - API endpoints
#
# Log Files (DEBUG mode creates additional file):
# - logs/app.log     - All INFO+ logs (10MB, 5 backups)
# - logs/errors.log  - ERROR+ only (5MB, 5 backups)
# - logs/debug.log   - DEBUG+ logs (5MB, 3 backups) [ONLY in DEBUG mode]
# - logs/deployments.jsonl - Deployment events
#
# Usage:
#   1. Set LOG_LEVEL=DEBUG in .env
#   2. Restart bot: docker-compose restart (or ctrl+C and python -m src.bot)
#   3. Check logs: tail -f logs/debug.log
#   4. Analyze: cat logs/debug.log | jq '.' (requires jq for JSON parsing)
#
# IMPORTANT:
# - DEBUG mode significantly increases log file sizes
# - Contains sensitive data (API keys are masked but queries/responses are logged)
# - Do NOT use in production
# - Useful for local debugging and understanding execution flow

# =============================================================================
# Webhook Settings (Only for production with BOT_MODE=webhook)
# =============================================================================

# Public URL where Telegram will send updates
# Example: https://yourdomain.com/webhook
# WEBHOOK_URL=

# Port for webhook server (usually 8443, 443, 80, or 88)
# WEBHOOK_PORT=8443

# Listen address (0.0.0.0 for all interfaces, 127.0.0.1 for localhost only)
# WEBHOOK_LISTEN=0.0.0.0

# Path to SSL certificate (required for custom cert)
# WEBHOOK_CERT_PATH=

# Path to SSL private key (required for custom cert)
# WEBHOOK_KEY_PATH=

# =============================================================================
# Hybrid Transcription Strategy (NEW - Phase 8)
# =============================================================================

# Enable hybrid strategy for fast feedback + quality improvement
# Strategy options: single, fallback, benchmark, hybrid
#
# Hybrid mode:
#   - Short audio (<threshold): Use quality model directly (medium)
#   - Long audio (>=threshold): Fast draft (small) → LLM refinement
#
# Benefits:
#   - 6x-9x faster for long audio (60s: 36s → 6s)
#   - Immediate feedback with draft
#   - Improved quality with LLM refinement
WHISPER_ROUTING_STRATEGY=single

# Duration threshold for hybrid strategy (seconds)
# Audio < threshold: quality model (medium)
# Audio >= threshold: draft model (small) + LLM refinement
# Recommended: 20 seconds
HYBRID_SHORT_THRESHOLD=20

# Draft provider for long audio
# Options: faster-whisper, openai
# Recommended: faster-whisper (free, fast)
HYBRID_DRAFT_PROVIDER=faster-whisper

# Draft model (for faster-whisper)
# Options: tiny, small, base
# Recommended: small (good balance of speed/quality, RTF ~0.2x)
# Alternative: tiny (faster RTF ~0.05x, lower quality)
HYBRID_DRAFT_MODEL=small

# Quality provider (for short audio)
# Usually same as main provider
HYBRID_QUALITY_PROVIDER=faster-whisper

# Quality model
# Recommended: medium (current production default)
HYBRID_QUALITY_MODEL=medium

# =============================================================================
# LLM Text Refinement (NEW - Phase 8)
# =============================================================================

# Enable LLM text refinement for draft transcriptions
# Corrects errors, adds punctuation, improves readability
# Only applies to long audio in hybrid mode
LLM_REFINEMENT_ENABLED=false

# LLM provider
# Options: deepseek (recommended - cheap, fast), openai, gigachat
# DeepSeek cost: ~$0.0002 per 60s audio (30x cheaper than OpenAI Whisper)
LLM_PROVIDER=deepseek

# DeepSeek API key
# Get from: https://platform.deepseek.com/api_keys
# Free tier available, pay-as-you-go after
# LLM_API_KEY=sk-your-deepseek-api-key-here

# LLM model name
# For DeepSeek: deepseek-chat (V3 model)
LLM_MODEL=deepseek-chat

# LLM API base URL
LLM_BASE_URL=https://api.deepseek.com

# System prompt for text refinement
# Note: Refinement prompt is stored in prompts/refinement.md
# Edit that file to customize refinement behavior

# LLM request timeout (seconds)
# Typical response time: 2-5 seconds
LLM_TIMEOUT=30

# LLM Debug Mode (for quality testing)
# When true, sends an additional message comparing draft and refined text
# Useful for evaluating LLM refinement quality
# Format: Shows both draft (from Whisper) and refined (from LLM) side-by-side
# WARNING: Only use for testing! Sends extra messages to users.
# Default: false (disabled)
LLM_DEBUG_MODE=false

# =============================================================================
# Audio Preprocessing (NEW - Phase 8)
# =============================================================================

# Convert audio to mono before transcription
# Pros: Faster processing, smaller files, ~5-10% speed improvement
# Cons: May reduce quality for stereo recordings
# Recommended: false (test first with your audio)
AUDIO_CONVERT_TO_MONO=false

# Target sample rate for mono conversion (Hz)
# 16000 Hz is sufficient for speech recognition
# Lower rate = faster processing, smaller files
AUDIO_TARGET_SAMPLE_RATE=16000

# Audio speed multiplier
# 1.0 = original speed (no change)
# 1.5 = 50% faster (recommended after testing)
# 2.0 = 2x faster (may reduce quality)
# Range: 0.5-2.0
#
# Note: This speeds up the audio for transcription, not the actual voice
# Whisper model processes sped-up audio faster
# Test quality impact before using in production
AUDIO_SPEED_MULTIPLIER=1.0

# =============================================================================
# Hybrid Mode Usage Examples
# =============================================================================

# Example 1: Conservative (Start Here)
# Test hybrid strategy without LLM refinement first
# WHISPER_ROUTING_STRATEGY=hybrid
# HYBRID_SHORT_THRESHOLD=20
# HYBRID_DRAFT_MODEL=small
# LLM_REFINEMENT_ENABLED=false
# AUDIO_SPEED_MULTIPLIER=1.0

# Example 2: Full Hybrid (After Testing)
# Fast draft + LLM refinement + speed boost
# WHISPER_ROUTING_STRATEGY=hybrid
# HYBRID_SHORT_THRESHOLD=20
# HYBRID_DRAFT_MODEL=small
# LLM_REFINEMENT_ENABLED=true
# LLM_API_KEY=sk-your-key
# LLM_DEBUG_MODE=true  # Enable to see draft vs refined comparison
# AUDIO_SPEED_MULTIPLIER=1.5

# Example 3: Aggressive Optimization
# Maximum speed with all optimizations
# WHISPER_ROUTING_STRATEGY=hybrid
# HYBRID_DRAFT_MODEL=small
# LLM_REFINEMENT_ENABLED=true
# AUDIO_SPEED_MULTIPLIER=1.5
# AUDIO_CONVERT_TO_MONO=true

# Example 4: Back to Original (Disable Hybrid)
# Return to single strategy if needed
# WHISPER_ROUTING_STRATEGY=single
# PRIMARY_PROVIDER=faster-whisper
# FASTER_WHISPER_MODEL_SIZE=medium
